{"content_list": [[{"type": "title", "raw_content": "<h1>Module<a class=\"headerlink\" href=\"#module\" title=\"Permalink to this heading\">\u00b6</a></h1>", "content": {"title_content": "Module \u00b6", "level": "1"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module\"><em class=\"property\"><span class=\"pre\">class</span><span class=\"w\"></span></em><span class=\"sig-prename descclassname\"><span class=\"pre\">torch.nn.</span></span><span class=\"sig-name descname\"><span class=\"pre\">Module</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span><span class=\"n\"><span class=\"pre\">args</span></span></em>, <em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">**</span></span><span class=\"n\"><span class=\"pre\">kwargs</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Base class for all neural network modules.</p><p>Your models should also subclass this class.</p><p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl>", "content": {"items": [[[{"c": "class", "t": "text"}, {"c": "torch.nn.", "t": "text"}, {"c": "Module", "t": "text"}, {"c": "(", "t": "text"}, {"c": "*", "t": "text"}, {"c": "args", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "**", "t": "text"}, {"c": "kwargs", "t": "text"}, {"c": ")", "t": "text"}, {"c": "[source]", "t": "text"}, {"c": "\u00b6", "t": "text"}]], [[{"c": "Base class for all neural network modules.", "t": "text"}, {"c": "Your models should also subclass this class.", "t": "text"}, {"c": "Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre><span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"nn\">nn</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn.functional</span> <span class=\"k\">as</span> <span class=\"nn\">F</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">Model</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv1</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv2</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n        <span class=\"k\">return</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n</pre>", "inline": false, "content": {"code_content": "import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div><p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <a class=\"reference internal\" href=\"#torch.nn.Module.to\" title=\"torch.nn.Module.to\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;to()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">to()</cccode-inline></a>, etc.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>As per the example above, an <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;__init__()&lt;/span&gt;&lt;/code&gt; call to the parent class\nmust be made before assignment on the child.' inline=\"true\">__init__()</cccode-inline> call to the parent class\nmust be made before assignment on the child.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Variables</dt><dd class=\"field-odd\"><p><strong>training</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 Boolean represents whether this module is in training or\nevaluation mode.</p></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.add_module\"><span class=\"sig-name descname\"><span class=\"pre\">add_module</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">name</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">module</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.add_module\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.add_module\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Add a child module to the current module.</p><p>The module can be accessed as an attribute using the given name.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 name of the child module. The child module can be\naccessed from this module using the given name</p></li><li><p><strong>module</strong> (<a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\"><em>Module</em></a>) \u2013 child module to be added to the module.</p></li></ul></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Apply <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;fn&lt;/span&gt;&lt;/code&gt; recursively to every submodule (as returned by ' inline=\"true\">fn</cccode-inline> recursively to every submodule (as returned by <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;.children()&lt;/span&gt;&lt;/code&gt;) as well as self.' inline=\"true\">.children()</cccode-inline>) as well as self.</p><p>Typical use includes initializing the parameters of a model\n(see also <a class=\"reference internal\" href=\"../nn.init.html#nn-init-doc\"><span class=\"std std-ref\">torch.nn.init</span></a>).</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>fn</strong> (<a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;Module&lt;/span&gt;&lt;/code&gt;' inline=\"true\">Module</cccode-inline></a> -&gt; None) \u2013 function to be applied to each submodule</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>self</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call ", "t": "text"}, {"c": "to()", "t": "code-inline"}, {"c": ", etc.", "t": "text"}, {"c": "Note", "t": "text"}, {"c": "As per the example above, an ", "t": "text"}, {"c": "__init__()", "t": "code-inline"}, {"c": " call to the parent class\nmust be made before assignment on the child.", "t": "text"}, {"c": "Variables", "t": "text"}, {"c": "training", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ") \u2013 Boolean represents whether this module is in training or\nevaluation mode.", "t": "text"}, {"c": "add_module", "t": "text"}, {"c": "(", "t": "text"}, {"c": "name", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "module", "t": "text"}, {"c": ")", "t": "text"}, {"c": "[source]", "t": "text"}, {"c": "\u00b6", "t": "text"}, {"c": "Add a child module to the current module.", "t": "text"}, {"c": "The module can be accessed as an attribute using the given name.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "name", "t": "text"}, {"c": " (", "t": "text"}, {"c": "str", "t": "text"}, {"c": ") \u2013 name of the child module. The child module can be\naccessed from this module using the given name", "t": "text"}, {"c": "module", "t": "text"}, {"c": " (", "t": "text"}, {"c": "Module", "t": "text"}, {"c": ") \u2013 child module to be added to the module.", "t": "text"}, {"c": "Apply ", "t": "text"}, {"c": "fn", "t": "code-inline"}, {"c": " recursively to every submodule (as returned by ", "t": "text"}, {"c": ".children()", "t": "code-inline"}, {"c": ") as well as self.", "t": "text"}, {"c": "Typical use includes initializing the parameters of a model\n(see also ", "t": "text"}, {"c": "torch.nn.init", "t": "text"}, {"c": ").", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "fn", "t": "text"}, {"c": " (", "t": "text"}, {"c": "Module", "t": "code-inline"}, {"c": " -> None) \u2013 function to be applied to each submodule", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "self", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Example:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"nd\">@torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">()</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">def</span> <span class=\"nf\">init_weights</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">):</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">if</span> <span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">:</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">weight</span><span class=\"o\">.</span><span class=\"n\">fill_</span><span class=\"p\">(</span><span class=\"mf\">1.0</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">weight</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"n\">init_weights</span><span class=\"p\">)</span>\n<span class=\"go\">Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[1., 1.],</span>\n<span class=\"go\">        [1., 1.]], requires_grad=True)</span>\n<span class=\"go\">Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[1., 1.],</span>\n<span class=\"go\">        [1., 1.]], requires_grad=True)</span>\n<span class=\"go\">Sequential(</span>\n<span class=\"go\">  (0): Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">  (1): Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">)</span>\n</pre>", "inline": false, "content": {"code_content": ">>> @torch.no_grad()\n>>> def init_weights(m):\n>>>     print(m)\n>>>     if type(m) == nn.Linear:\n>>>         m.weight.fill_(1.0)\n>>>         print(m.weight)\n>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n>>> net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dd><p>Casts all floating point parameters and buffers to <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;bfloat16&lt;/span&gt;&lt;/code&gt; datatype.' inline=\"true\">bfloat16</cccode-inline> datatype.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Returns</dt><dd class=\"field-odd\"><p>self</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Return an iterator over module buffers.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>recurse</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.</p></dd><dt class=\"field-even\">Yields</dt><dd class=\"field-even\"><p><em>torch.Tensor</em> \u2013 module buffer</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "Casts all floating point parameters and buffers to ", "t": "text"}, {"c": "bfloat16", "t": "code-inline"}, {"c": " datatype.", "t": "text"}, {"c": "Note", "t": "text"}, {"c": "This method modifies the module in-place.", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "self", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Return an iterator over module buffers.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "recurse", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ") \u2013 if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.", "t": "text"}, {"c": "Yields", "t": "text"}, {"c": "torch.Tensor", "t": "text"}, {"c": " \u2013 module buffer", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Example:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">buf</span> <span class=\"ow\">in</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">buffers</span><span class=\"p\">():</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">buf</span><span class=\"p\">),</span> <span class=\"n\">buf</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">())</span>\n<span class=\"go\">&lt;class 'torch.Tensor'&gt; (20L,)</span>\n<span class=\"go\">&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</span>\n</pre>", "inline": false, "content": {"code_content": ">>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dd><p>Return an iterator over immediate children modules.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Yields</dt><dd class=\"field-odd\"><p><em>Module</em> \u2013 a child module</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.compile\"><span class=\"sig-name descname\"><span class=\"pre\">compile</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span><span class=\"n\"><span class=\"pre\">args</span></span></em>, <em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">**</span></span><span class=\"n\"><span class=\"pre\">kwargs</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.compile\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.compile\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Compile this Module\u2019s forward using <a class=\"reference internal\" href=\"torch.compile.html#torch.compile\" title=\"torch.compile\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.compile()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">torch.compile()</cccode-inline></a>.</p><p>This Module\u2019s __call__ method is compiled and all arguments are passed as-is\nto <a class=\"reference internal\" href=\"torch.compile.html#torch.compile\" title=\"torch.compile\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.compile()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">torch.compile()</cccode-inline></a>.</p><p>See <a class=\"reference internal\" href=\"torch.compile.html#torch.compile\" title=\"torch.compile\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.compile()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">torch.compile()</cccode-inline></a> for details on the arguments for this function.</p></dd></dl><dl class=\"py method\"><dd><p>Move all model parameters and buffers to the CPU.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Returns</dt><dd class=\"field-odd\"><p>self</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Move all model parameters and buffers to the GPU.</p><p>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on GPU while being optimized.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>device</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.13)\"><em>int</em></a><em>, </em><em>optional</em>) \u2013 if specified, all parameters will be\ncopied to that device</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>self</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Casts all floating point parameters and buffers to <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;double&lt;/span&gt;&lt;/code&gt; datatype.' inline=\"true\">double</cccode-inline> datatype.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Returns</dt><dd class=\"field-odd\"><p>self</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Set the module in evaluation mode.</p><p>This has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. <a class=\"reference internal\" href=\"torch.nn.Dropout.html#torch.nn.Dropout\" title=\"torch.nn.Dropout\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;Dropout&lt;/span&gt;&lt;/code&gt;' inline=\"true\">Dropout</cccode-inline></a>, <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;BatchNorm&lt;/span&gt;&lt;/code&gt;,\netc.' inline=\"true\">BatchNorm</cccode-inline>,\netc.</p><p>This is equivalent with <a class=\"reference internal\" href=\"#torch.nn.Module.train\" title=\"torch.nn.Module.train\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;self.train(False)&lt;/span&gt;&lt;/code&gt;' inline=\"true\">self.train(False)</cccode-inline></a>.</p><p>See <a class=\"reference internal\" href=\"../notes/autograd.html#locally-disable-grad-doc\"><span class=\"std std-ref\">Locally disabling gradient computation</span></a> for a comparison between\n.eval() and several similar mechanisms that may be confused with it.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Returns</dt><dd class=\"field-odd\"><p>self</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Set the extra representation of the module.</p><p>To print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Casts all floating point parameters and buffers to <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;float&lt;/span&gt;&lt;/code&gt; datatype.' inline=\"true\">float</cccode-inline> datatype.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Returns</dt><dd class=\"field-odd\"><p>self</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.forward\"><span class=\"sig-name descname\"><span class=\"pre\">forward</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span><span class=\"n\"><span class=\"pre\">input</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#torch.nn.Module.forward\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Define the computation performed at every call.</p><p>Should be overridden by all subclasses.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;Module&lt;/span&gt;&lt;/code&gt;' inline=\"true\">Module</cccode-inline></a> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p></div><dl class=\"field-list simple\"></dl></dd></dl><dl class=\"py method\"><dd><p>Return the buffer given by <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt; if it exists, otherwise throw an error.' inline=\"true\">target</cccode-inline> if it exists, otherwise throw an error.</p><p>See the docstring for <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_submodule&lt;/span&gt;&lt;/code&gt; for a more detailed\nexplanation of this method\u2019s functionality as well as how to\ncorrectly specify ' inline=\"true\">get_submodule</cccode-inline> for a more detailed\nexplanation of this method\u2019s functionality as well as how to\ncorrectly specify <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">target</cccode-inline>.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>target</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 The fully-qualified string name of the buffer\nto look for. (See <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_submodule&lt;/span&gt;&lt;/code&gt; for how to specify a\nfully-qualified string.)' inline=\"true\">get_submodule</cccode-inline> for how to specify a\nfully-qualified string.)</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>The buffer referenced by <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt;' inline=\"true\">target</cccode-inline></p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd><dt class=\"field-even\">Raises</dt><dd class=\"field-even\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#AttributeError\" title=\"(in Python v3.13)\"><strong>AttributeError</strong></a> \u2013 If the target string references an invalid\n    path or resolves to something that is not a\n    buffer</p></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Return any extra state to include in the module\u2019s state_dict.</p><p>Implement this and a corresponding <a class=\"reference internal\" href=\"#torch.nn.Module.set_extra_state\" title=\"torch.nn.Module.set_extra_state\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;set_extra_state()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">set_extra_state()</cccode-inline></a> for your module\nif you need to store extra state. This function is called when building the\nmodule\u2019s state_dict().</p><p>Note that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Returns</dt><dd class=\"field-odd\"><p>Any extra state to store in the module\u2019s state_dict</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Return the parameter given by <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt; if it exists, otherwise throw an error.' inline=\"true\">target</cccode-inline> if it exists, otherwise throw an error.</p><p>See the docstring for <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_submodule&lt;/span&gt;&lt;/code&gt; for a more detailed\nexplanation of this method\u2019s functionality as well as how to\ncorrectly specify ' inline=\"true\">get_submodule</cccode-inline> for a more detailed\nexplanation of this method\u2019s functionality as well as how to\ncorrectly specify <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">target</cccode-inline>.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>target</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 The fully-qualified string name of the Parameter\nto look for. (See <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_submodule&lt;/span&gt;&lt;/code&gt; for how to specify a\nfully-qualified string.)' inline=\"true\">get_submodule</cccode-inline> for how to specify a\nfully-qualified string.)</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>The Parameter referenced by <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt;' inline=\"true\">target</cccode-inline></p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"><p>torch.nn.Parameter</p></dd><dt class=\"field-even\">Raises</dt><dd class=\"field-even\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#AttributeError\" title=\"(in Python v3.13)\"><strong>AttributeError</strong></a> \u2013 If the target string references an invalid\n    path or resolves to something that is not an\n    <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;nn.Parameter&lt;/span&gt;&lt;/code&gt;' inline=\"true\">nn.Parameter</cccode-inline></p></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Return the submodule given by <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt; if it exists, otherwise throw an error.' inline=\"true\">target</cccode-inline> if it exists, otherwise throw an error.</p><p>For example, let\u2019s say you have an <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;nn.Module&lt;/span&gt;&lt;/code&gt; ' inline=\"true\">nn.Module</cccode-inline><cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;A&lt;/span&gt;&lt;/code&gt; that\nlooks like this:' inline=\"true\">A</cccode-inline> that\nlooks like this:</p><div class=\"highlight-text notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "Return an iterator over immediate children modules.", "t": "text"}, {"c": "Yields", "t": "text"}, {"c": "Module", "t": "text"}, {"c": " \u2013 a child module", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "compile", "t": "text"}, {"c": "(", "t": "text"}, {"c": "*", "t": "text"}, {"c": "args", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "**", "t": "text"}, {"c": "kwargs", "t": "text"}, {"c": ")", "t": "text"}, {"c": "[source]", "t": "text"}, {"c": "\u00b6", "t": "text"}, {"c": "Compile this Module\u2019s forward using ", "t": "text"}, {"c": "torch.compile()", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "This Module\u2019s __call__ method is compiled and all arguments are passed as-is\nto ", "t": "text"}, {"c": "torch.compile()", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "See ", "t": "text"}, {"c": "torch.compile()", "t": "code-inline"}, {"c": " for details on the arguments for this function.", "t": "text"}, {"c": "Move all model parameters and buffers to the CPU.", "t": "text"}, {"c": "Note", "t": "text"}, {"c": "This method modifies the module in-place.", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "self", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Move all model parameters and buffers to the GPU.", "t": "text"}, {"c": "This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on GPU while being optimized.", "t": "text"}, {"c": "Note", "t": "text"}, {"c": "This method modifies the module in-place.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "device", "t": "text"}, {"c": " (", "t": "text"}, {"c": "int", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "optional", "t": "text"}, {"c": ") \u2013 if specified, all parameters will be\ncopied to that device", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "self", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Casts all floating point parameters and buffers to ", "t": "text"}, {"c": "double", "t": "code-inline"}, {"c": " datatype.", "t": "text"}, {"c": "Note", "t": "text"}, {"c": "This method modifies the module in-place.", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "self", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Set the module in evaluation mode.", "t": "text"}, {"c": "This has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. ", "t": "text"}, {"c": "Dropout", "t": "code-inline"}, {"c": ", ", "t": "text"}, {"c": "BatchNorm", "t": "code-inline"}, {"c": ",\netc.", "t": "text"}, {"c": "This is equivalent with ", "t": "text"}, {"c": "self.train(False)", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "See ", "t": "text"}, {"c": "Locally disabling gradient computation", "t": "text"}, {"c": " for a comparison between\n.eval() and several similar mechanisms that may be confused with it.", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "self", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Set the extra representation of the module.", "t": "text"}, {"c": "To print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Casts all floating point parameters and buffers to ", "t": "text"}, {"c": "float", "t": "code-inline"}, {"c": " datatype.", "t": "text"}, {"c": "Note", "t": "text"}, {"c": "This method modifies the module in-place.", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "self", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "forward", "t": "text"}, {"c": "(", "t": "text"}, {"c": "*", "t": "text"}, {"c": "input", "t": "text"}, {"c": ")", "t": "text"}, {"c": "\u00b6", "t": "text"}, {"c": "Define the computation performed at every call.", "t": "text"}, {"c": "Should be overridden by all subclasses.", "t": "text"}, {"c": "Note", "t": "text"}, {"c": "Although the recipe for forward pass needs to be defined within\nthis function, one should call the ", "t": "text"}, {"c": "Module", "t": "code-inline"}, {"c": " instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.", "t": "text"}, {"c": "Return the buffer given by ", "t": "text"}, {"c": "target", "t": "code-inline"}, {"c": " if it exists, otherwise throw an error.", "t": "text"}, {"c": "See the docstring for ", "t": "text"}, {"c": "get_submodule", "t": "code-inline"}, {"c": " for a more detailed\nexplanation of this method\u2019s functionality as well as how to\ncorrectly specify ", "t": "text"}, {"c": "target", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "target", "t": "text"}, {"c": " (", "t": "text"}, {"c": "str", "t": "text"}, {"c": ") \u2013 The fully-qualified string name of the buffer\nto look for. (See ", "t": "text"}, {"c": "get_submodule", "t": "code-inline"}, {"c": " for how to specify a\nfully-qualified string.)", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "The buffer referenced by ", "t": "text"}, {"c": "target", "t": "code-inline"}, {"c": "Return type", "t": "text"}, {"c": "Raises", "t": "text"}, {"c": "AttributeError", "t": "text"}, {"c": " \u2013 If the target string references an invalid\n    path or resolves to something that is not a\n    buffer", "t": "text"}, {"c": "Return any extra state to include in the module\u2019s state_dict.", "t": "text"}, {"c": "Implement this and a corresponding ", "t": "text"}, {"c": "set_extra_state()", "t": "code-inline"}, {"c": " for your module\nif you need to store extra state. This function is called when building the\nmodule\u2019s state_dict().", "t": "text"}, {"c": "Note that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "Any extra state to store in the module\u2019s state_dict", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Return the parameter given by ", "t": "text"}, {"c": "target", "t": "code-inline"}, {"c": " if it exists, otherwise throw an error.", "t": "text"}, {"c": "See the docstring for ", "t": "text"}, {"c": "get_submodule", "t": "code-inline"}, {"c": " for a more detailed\nexplanation of this method\u2019s functionality as well as how to\ncorrectly specify ", "t": "text"}, {"c": "target", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "target", "t": "text"}, {"c": " (", "t": "text"}, {"c": "str", "t": "text"}, {"c": ") \u2013 The fully-qualified string name of the Parameter\nto look for. (See ", "t": "text"}, {"c": "get_submodule", "t": "code-inline"}, {"c": " for how to specify a\nfully-qualified string.)", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "The Parameter referenced by ", "t": "text"}, {"c": "target", "t": "code-inline"}, {"c": "Return type", "t": "text"}, {"c": "torch.nn.Parameter", "t": "text"}, {"c": "Raises", "t": "text"}, {"c": "AttributeError", "t": "text"}, {"c": " \u2013 If the target string references an invalid\n    path or resolves to something that is not an\n    ", "t": "text"}, {"c": "nn.Parameter", "t": "code-inline"}, {"c": "Return the submodule given by ", "t": "text"}, {"c": "target", "t": "code-inline"}, {"c": " if it exists, otherwise throw an error.", "t": "text"}, {"c": "For example, let\u2019s say you have an ", "t": "text"}, {"c": "nn.Module", "t": "code-inline"}, {"c": "A", "t": "code-inline"}, {"c": " that\nlooks like this:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre>A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)\n</pre>", "inline": false, "content": {"code_content": "A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-text notranslate\"><div class=\"highlight\"></div></div><p>(The diagram shows an <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;nn.Module&lt;/span&gt;&lt;/code&gt; ' inline=\"true\">nn.Module</cccode-inline><cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;A&lt;/span&gt;&lt;/code&gt;. ' inline=\"true\">A</cccode-inline>. <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;A&lt;/span&gt;&lt;/code&gt; has a nested\nsubmodule ' inline=\"true\">A</cccode-inline> has a nested\nsubmodule <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;net_b&lt;/span&gt;&lt;/code&gt;, which itself has two submodules ' inline=\"true\">net_b</cccode-inline>, which itself has two submodules <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;net_c&lt;/span&gt;&lt;/code&gt;\nand ' inline=\"true\">net_c</cccode-inline>\nand <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;linear&lt;/span&gt;&lt;/code&gt;. ' inline=\"true\">linear</cccode-inline>. <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;net_c&lt;/span&gt;&lt;/code&gt; then has a submodule ' inline=\"true\">net_c</cccode-inline> then has a submodule <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;conv&lt;/span&gt;&lt;/code&gt;.)' inline=\"true\">conv</cccode-inline>.)</p><p>To check whether or not we have the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;linear&lt;/span&gt;&lt;/code&gt; submodule, we\nwould call ' inline=\"true\">linear</cccode-inline> submodule, we\nwould call <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_submodule(\"net_b.linear\")&lt;/span&gt;&lt;/code&gt;. To check whether\nwe have the ' inline=\"true\">get_submodule(\"net_b.linear\")</cccode-inline>. To check whether\nwe have the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;conv&lt;/span&gt;&lt;/code&gt; submodule, we would call\n' inline=\"true\">conv</cccode-inline> submodule, we would call\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_submodule(\"net_b.net_c.conv\")&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">get_submodule(\"net_b.net_c.conv\")</cccode-inline>.</p><p>The runtime of <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_submodule&lt;/span&gt;&lt;/code&gt; is bounded by the degree\nof module nesting in ' inline=\"true\">get_submodule</cccode-inline> is bounded by the degree\nof module nesting in <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt;. A query against\n' inline=\"true\">target</cccode-inline>. A query against\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;named_modules&lt;/span&gt;&lt;/code&gt; achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ' inline=\"true\">named_modules</cccode-inline> achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_submodule&lt;/span&gt;&lt;/code&gt; should always be\nused.' inline=\"true\">get_submodule</cccode-inline> should always be\nused.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>target</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>The submodule referenced by <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt;' inline=\"true\">target</cccode-inline></p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd><dt class=\"field-even\">Raises</dt><dd class=\"field-even\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#AttributeError\" title=\"(in Python v3.13)\"><strong>AttributeError</strong></a> \u2013 If the target string references an invalid\n    path or resolves to something that is not an\n    <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;nn.Module&lt;/span&gt;&lt;/code&gt;' inline=\"true\">nn.Module</cccode-inline></p></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Casts all floating point parameters and buffers to <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;half&lt;/span&gt;&lt;/code&gt; datatype.' inline=\"true\">half</cccode-inline> datatype.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Returns</dt><dd class=\"field-odd\"><p>self</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Move all model parameters and buffers to the IPU.</p><p>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on IPU while being optimized.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>device</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.13)\"><em>int</em></a><em>, </em><em>optional</em>) \u2013 if specified, all parameters will be\ncopied to that device</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>self</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.load_state_dict\"><span class=\"sig-name descname\"><span class=\"pre\">load_state_dict</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">state_dict</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">strict</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">assign</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.load_state_dict\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.load_state_dict\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Copy parameters and buffers from <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict</cccode-inline></a> into this module and its descendants.</p><p>If <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;strict&lt;/span&gt;&lt;/code&gt; is ' inline=\"true\">strict</cccode-inline> is <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;, then\nthe keys of ' inline=\"true\">True</cccode-inline>, then\nthe keys of <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict</cccode-inline></a> must exactly match the keys returned\nby this module\u2019s <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict()</cccode-inline></a> function.</p><div class=\"admonition warning\"><p class=\"admonition-title\">Warning</p><p>If <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;assign&lt;/span&gt;&lt;/code&gt; is ' inline=\"true\">assign</cccode-inline> is <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt; the optimizer must be created after\nthe call to ' inline=\"true\">True</cccode-inline> the optimizer must be created after\nthe call to <a class=\"reference internal\" href=\"#torch.nn.Module.load_state_dict\" title=\"torch.nn.Module.load_state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;load_state_dict&lt;/span&gt;&lt;/code&gt;' inline=\"true\">load_state_dict</cccode-inline></a> unless\n<a class=\"reference internal\" href=\"../future_mod.html#torch.__future__.get_swap_module_params_on_conversion\" title=\"torch.__future__.get_swap_module_params_on_conversion\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_swap_module_params_on_conversion()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">get_swap_module_params_on_conversion()</cccode-inline></a> is <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">True</cccode-inline>.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>state_dict</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.13)\"><em>dict</em></a>) \u2013 a dict containing parameters and\npersistent buffers.</p></li><li><p><strong>strict</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a><em>, </em><em>optional</em>) \u2013 whether to strictly enforce that the keys\nin <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict</cccode-inline></a> match the keys returned by this module\u2019s\n<a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict()</cccode-inline></a> function. Default: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;' inline=\"true\">True</cccode-inline></p></li><li><p><strong>assign</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a><em>, </em><em>optional</em>) \u2013 When <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt;, the properties of the tensors\nin the current module are preserved while when ' inline=\"true\">False</cccode-inline>, the properties of the tensors\nin the current module are preserved while when <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;, the\nproperties of the Tensors in the state dict are preserved. The only\nexception is the ' inline=\"true\">True</cccode-inline>, the\nproperties of the Tensors in the state dict are preserved. The only\nexception is the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;requires_grad&lt;/span&gt;&lt;/code&gt; field of ' inline=\"true\">requires_grad</cccode-inline> field of <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;\n&lt;span class=\"pre\"&gt;Default:&lt;/span&gt; &lt;span class=\"pre\"&gt;``False`&lt;/span&gt;&lt;/code&gt;' inline=\"true\">Default: ``False`</cccode-inline></p></li></ul></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><ul class=\"simple\"><li><dl class=\"simple\"><dt><strong>missing_keys</strong> is a list of str containing any keys that are expected</dt><dd><p>by this module but missing from the provided <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">state_dict</cccode-inline>.</p></dd></dl></li><li><dl class=\"simple\"><dt><strong>unexpected_keys</strong> is a list of str containing the keys that are not</dt><dd><p>expected by this module but present in the provided <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">state_dict</cccode-inline>.</p></dd></dl></li></ul></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"><p><cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;NamedTuple&lt;/span&gt;&lt;/code&gt; with ' inline=\"true\">NamedTuple</cccode-inline> with <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;missing_keys&lt;/span&gt;&lt;/code&gt; and ' inline=\"true\">missing_keys</cccode-inline> and <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;unexpected_keys&lt;/span&gt;&lt;/code&gt; fields' inline=\"true\">unexpected_keys</cccode-inline> fields</p></dd></dl><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>If a parameter or buffer is registered as <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;None&lt;/span&gt;&lt;/code&gt; and its corresponding key\nexists in ' inline=\"true\">None</cccode-inline> and its corresponding key\nexists in <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict</cccode-inline></a>, <a class=\"reference internal\" href=\"#torch.nn.Module.load_state_dict\" title=\"torch.nn.Module.load_state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;load_state_dict()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">load_state_dict()</cccode-inline></a> will raise a\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;RuntimeError&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">RuntimeError</cccode-inline>.</p></div></dd></dl><dl class=\"py method\"><dd><p>Return an iterator over all modules in the network.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Yields</dt><dd class=\"field-odd\"><p><em>Module</em> \u2013 a module in the network</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"></dd></dl><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>Duplicate modules are returned only once. In the following\nexample, <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;l&lt;/span&gt;&lt;/code&gt; will be returned only once.' inline=\"true\">l</cccode-inline> will be returned only once.</p></div><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "(The diagram shows an ", "t": "text"}, {"c": "nn.Module", "t": "code-inline"}, {"c": "A", "t": "code-inline"}, {"c": ". ", "t": "text"}, {"c": "A", "t": "code-inline"}, {"c": " has a nested\nsubmodule ", "t": "text"}, {"c": "net_b", "t": "code-inline"}, {"c": ", which itself has two submodules ", "t": "text"}, {"c": "net_c", "t": "code-inline"}, {"c": "\nand ", "t": "text"}, {"c": "linear", "t": "code-inline"}, {"c": ". ", "t": "text"}, {"c": "net_c", "t": "code-inline"}, {"c": " then has a submodule ", "t": "text"}, {"c": "conv", "t": "code-inline"}, {"c": ".)", "t": "text"}, {"c": "To check whether or not we have the ", "t": "text"}, {"c": "linear", "t": "code-inline"}, {"c": " submodule, we\nwould call ", "t": "text"}, {"c": "get_submodule(\"net_b.linear\")", "t": "code-inline"}, {"c": ". To check whether\nwe have the ", "t": "text"}, {"c": "conv", "t": "code-inline"}, {"c": " submodule, we would call\n", "t": "text"}, {"c": "get_submodule(\"net_b.net_c.conv\")", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "The runtime of ", "t": "text"}, {"c": "get_submodule", "t": "code-inline"}, {"c": " is bounded by the degree\nof module nesting in ", "t": "text"}, {"c": "target", "t": "code-inline"}, {"c": ". A query against\n", "t": "text"}, {"c": "named_modules", "t": "code-inline"}, {"c": " achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ", "t": "text"}, {"c": "get_submodule", "t": "code-inline"}, {"c": " should always be\nused.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "target", "t": "text"}, {"c": " (", "t": "text"}, {"c": "str", "t": "text"}, {"c": ") \u2013 The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "The submodule referenced by ", "t": "text"}, {"c": "target", "t": "code-inline"}, {"c": "Return type", "t": "text"}, {"c": "Raises", "t": "text"}, {"c": "AttributeError", "t": "text"}, {"c": " \u2013 If the target string references an invalid\n    path or resolves to something that is not an\n    ", "t": "text"}, {"c": "nn.Module", "t": "code-inline"}, {"c": "Casts all floating point parameters and buffers to ", "t": "text"}, {"c": "half", "t": "code-inline"}, {"c": " datatype.", "t": "text"}, {"c": "Note", "t": "text"}, {"c": "This method modifies the module in-place.", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "self", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Move all model parameters and buffers to the IPU.", "t": "text"}, {"c": "This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on IPU while being optimized.", "t": "text"}, {"c": "Note", "t": "text"}, {"c": "This method modifies the module in-place.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "device", "t": "text"}, {"c": " (", "t": "text"}, {"c": "int", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "optional", "t": "text"}, {"c": ") \u2013 if specified, all parameters will be\ncopied to that device", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "self", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "load_state_dict", "t": "text"}, {"c": "(", "t": "text"}, {"c": "state_dict", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "strict", "t": "text"}, {"c": "=", "t": "text"}, {"c": "True", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "assign", "t": "text"}, {"c": "=", "t": "text"}, {"c": "False", "t": "text"}, {"c": ")", "t": "text"}, {"c": "[source]", "t": "text"}, {"c": "\u00b6", "t": "text"}, {"c": "Copy parameters and buffers from ", "t": "text"}, {"c": "state_dict", "t": "code-inline"}, {"c": " into this module and its descendants.", "t": "text"}, {"c": "If ", "t": "text"}, {"c": "strict", "t": "code-inline"}, {"c": " is ", "t": "text"}, {"c": "True", "t": "code-inline"}, {"c": ", then\nthe keys of ", "t": "text"}, {"c": "state_dict", "t": "code-inline"}, {"c": " must exactly match the keys returned\nby this module\u2019s ", "t": "text"}, {"c": "state_dict()", "t": "code-inline"}, {"c": " function.", "t": "text"}, {"c": "Warning", "t": "text"}, {"c": "If ", "t": "text"}, {"c": "assign", "t": "code-inline"}, {"c": " is ", "t": "text"}, {"c": "True", "t": "code-inline"}, {"c": " the optimizer must be created after\nthe call to ", "t": "text"}, {"c": "load_state_dict", "t": "code-inline"}, {"c": " unless\n", "t": "text"}, {"c": "get_swap_module_params_on_conversion()", "t": "code-inline"}, {"c": " is ", "t": "text"}, {"c": "True", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "state_dict", "t": "text"}, {"c": " (", "t": "text"}, {"c": "dict", "t": "text"}, {"c": ") \u2013 a dict containing parameters and\npersistent buffers.", "t": "text"}, {"c": "strict", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "optional", "t": "text"}, {"c": ") \u2013 whether to strictly enforce that the keys\nin ", "t": "text"}, {"c": "state_dict", "t": "code-inline"}, {"c": " match the keys returned by this module\u2019s\n", "t": "text"}, {"c": "state_dict()", "t": "code-inline"}, {"c": " function. Default: ", "t": "text"}, {"c": "True", "t": "code-inline"}, {"c": "assign", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "optional", "t": "text"}, {"c": ") \u2013 When ", "t": "text"}, {"c": "False", "t": "code-inline"}, {"c": ", the properties of the tensors\nin the current module are preserved while when ", "t": "text"}, {"c": "True", "t": "code-inline"}, {"c": ", the\nproperties of the Tensors in the state dict are preserved. The only\nexception is the ", "t": "text"}, {"c": "requires_grad", "t": "code-inline"}, {"c": " field of ", "t": "text"}, {"c": "Default: ``False`", "t": "code-inline"}, {"c": "Returns", "t": "text"}, {"c": "missing_keys", "t": "text"}, {"c": " is a list of str containing any keys that are expected", "t": "text"}, {"c": "by this module but missing from the provided ", "t": "text"}, {"c": "state_dict", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "unexpected_keys", "t": "text"}, {"c": " is a list of str containing the keys that are not", "t": "text"}, {"c": "expected by this module but present in the provided ", "t": "text"}, {"c": "state_dict", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "NamedTuple", "t": "code-inline"}, {"c": " with ", "t": "text"}, {"c": "missing_keys", "t": "code-inline"}, {"c": " and ", "t": "text"}, {"c": "unexpected_keys", "t": "code-inline"}, {"c": " fields", "t": "text"}, {"c": "Note", "t": "text"}, {"c": "If a parameter or buffer is registered as ", "t": "text"}, {"c": "None", "t": "code-inline"}, {"c": " and its corresponding key\nexists in ", "t": "text"}, {"c": "state_dict", "t": "code-inline"}, {"c": ", ", "t": "text"}, {"c": "load_state_dict()", "t": "code-inline"}, {"c": " will raise a\n", "t": "text"}, {"c": "RuntimeError", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "Return an iterator over all modules in the network.", "t": "text"}, {"c": "Yields", "t": "text"}, {"c": "Module", "t": "text"}, {"c": " \u2013 a module in the network", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Note", "t": "text"}, {"c": "Duplicate modules are returned only once. In the following\nexample, ", "t": "text"}, {"c": "l", "t": "code-inline"}, {"c": " will be returned only once.", "t": "text"}, {"c": "Example:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">l</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">,</span> <span class=\"n\">l</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"n\">m</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"p\">()):</span>\n<span class=\"gp\">... </span>    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"s1\">'-&gt;'</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">)</span>\n\n<span class=\"go\">0 -&gt; Sequential(</span>\n<span class=\"go\">  (0): Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">  (1): Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">)</span>\n<span class=\"go\">1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>\n</pre>", "inline": false, "content": {"code_content": ">>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n...     print(idx, '->', m)\n\n0 -> Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dd><p>Move all model parameters and buffers to the MTIA.</p><p>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on MTIA while being optimized.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>device</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.13)\"><em>int</em></a><em>, </em><em>optional</em>) \u2013 if specified, all parameters will be\ncopied to that device</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>self</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.named_buffers\"><span class=\"sig-name descname\"><span class=\"pre\">named_buffers</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prefix</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">recurse</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">remove_duplicate</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.named_buffers\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.named_buffers\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>prefix</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 prefix to prepend to all buffer names.</p></li><li><p><strong>recurse</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a><em>, </em><em>optional</em>) \u2013 if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.</p></li><li><p><strong>remove_duplicate</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a><em>, </em><em>optional</em>) \u2013 whether to remove the duplicated buffers in the result. Defaults to True.</p></li></ul></dd><dt class=\"field-even\">Yields</dt><dd class=\"field-even\"><p><em>(str, torch.Tensor)</em> \u2013 Tuple containing the name and buffer</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Iterator\" title=\"(in Python v3.13)\"><em>Iterator</em></a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Tuple\" title=\"(in Python v3.13)\"><em>Tuple</em></a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\">str</a>, <a class=\"reference internal\" href=\"../tensors.html#torch.Tensor\" title=\"torch.Tensor\"><em>Tensor</em></a>]]</p></dd></dl><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "Move all model parameters and buffers to the MTIA.", "t": "text"}, {"c": "This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on MTIA while being optimized.", "t": "text"}, {"c": "Note", "t": "text"}, {"c": "This method modifies the module in-place.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "device", "t": "text"}, {"c": " (", "t": "text"}, {"c": "int", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "optional", "t": "text"}, {"c": ") \u2013 if specified, all parameters will be\ncopied to that device", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "self", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "named_buffers", "t": "text"}, {"c": "(", "t": "text"}, {"c": "prefix", "t": "text"}, {"c": "=", "t": "text"}, {"c": "''", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "recurse", "t": "text"}, {"c": "=", "t": "text"}, {"c": "True", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "remove_duplicate", "t": "text"}, {"c": "=", "t": "text"}, {"c": "True", "t": "text"}, {"c": ")", "t": "text"}, {"c": "[source]", "t": "text"}, {"c": "\u00b6", "t": "text"}, {"c": "Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "prefix", "t": "text"}, {"c": " (", "t": "text"}, {"c": "str", "t": "text"}, {"c": ") \u2013 prefix to prepend to all buffer names.", "t": "text"}, {"c": "recurse", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "optional", "t": "text"}, {"c": ") \u2013 if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.", "t": "text"}, {"c": "remove_duplicate", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "optional", "t": "text"}, {"c": ") \u2013 whether to remove the duplicated buffers in the result. Defaults to True.", "t": "text"}, {"c": "Yields", "t": "text"}, {"c": "(str, torch.Tensor)", "t": "text"}, {"c": " \u2013 Tuple containing the name and buffer", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Iterator", "t": "text"}, {"c": "[", "t": "text"}, {"c": "Tuple", "t": "text"}, {"c": "[", "t": "text"}, {"c": "str", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "Tensor", "t": "text"}, {"c": "]]", "t": "text"}, {"c": "Example:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">buf</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">named_buffers</span><span class=\"p\">():</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">if</span> <span class=\"n\">name</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">'running_var'</span><span class=\"p\">]:</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">buf</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">())</span>\n</pre>", "inline": false, "content": {"code_content": ">>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dd><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Yields</dt><dd class=\"field-odd\"><p><em>(str, Module)</em> \u2013 Tuple containing a name and child module</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Iterator\" title=\"(in Python v3.13)\"><em>Iterator</em></a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Tuple\" title=\"(in Python v3.13)\"><em>Tuple</em></a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\">str</a>, <a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.modules.module.Module\"><em>Module</em></a>]]</p></dd></dl><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.", "t": "text"}, {"c": "Yields", "t": "text"}, {"c": "(str, Module)", "t": "text"}, {"c": " \u2013 Tuple containing a name and child module", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Iterator", "t": "text"}, {"c": "[", "t": "text"}, {"c": "Tuple", "t": "text"}, {"c": "[", "t": "text"}, {"c": "str", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "Module", "t": "text"}, {"c": "]]", "t": "text"}, {"c": "Example:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">module</span> <span class=\"ow\">in</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">named_children</span><span class=\"p\">():</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">if</span> <span class=\"n\">name</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">'conv4'</span><span class=\"p\">,</span> <span class=\"s1\">'conv5'</span><span class=\"p\">]:</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">)</span>\n</pre>", "inline": false, "content": {"code_content": ">>> for name, module in model.named_children():\n>>>     if name in ['conv4', 'conv5']:\n>>>         print(module)", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.named_modules\"><span class=\"sig-name descname\"><span class=\"pre\">named_modules</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">memo</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prefix</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">remove_duplicate</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.named_modules\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.named_modules\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>memo</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Optional\" title=\"(in Python v3.13)\"><em>Optional</em></a><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Set\" title=\"(in Python v3.13)\"><em>Set</em></a><em>[</em><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.modules.module.Module\"><em>Module</em></a><em>]</em><em>]</em>) \u2013 a memo to store the set of modules already added to the result</p></li><li><p><strong>prefix</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 a prefix that will be added to the name of the module</p></li><li><p><strong>remove_duplicate</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 whether to remove the duplicated module instances in the result\nor not</p></li></ul></dd><dt class=\"field-even\">Yields</dt><dd class=\"field-even\"><p><em>(str, Module)</em> \u2013 Tuple of name and module</p></dd></dl><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>Duplicate modules are returned only once. In the following\nexample, <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;l&lt;/span&gt;&lt;/code&gt; will be returned only once.' inline=\"true\">l</cccode-inline> will be returned only once.</p></div><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "named_modules", "t": "text"}, {"c": "(", "t": "text"}, {"c": "memo", "t": "text"}, {"c": "=", "t": "text"}, {"c": "None", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "prefix", "t": "text"}, {"c": "=", "t": "text"}, {"c": "''", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "remove_duplicate", "t": "text"}, {"c": "=", "t": "text"}, {"c": "True", "t": "text"}, {"c": ")", "t": "text"}, {"c": "[source]", "t": "text"}, {"c": "\u00b6", "t": "text"}, {"c": "Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "memo", "t": "text"}, {"c": " (", "t": "text"}, {"c": "Optional", "t": "text"}, {"c": "[", "t": "text"}, {"c": "Set", "t": "text"}, {"c": "[", "t": "text"}, {"c": "Module", "t": "text"}, {"c": "]", "t": "text"}, {"c": "]", "t": "text"}, {"c": ") \u2013 a memo to store the set of modules already added to the result", "t": "text"}, {"c": "prefix", "t": "text"}, {"c": " (", "t": "text"}, {"c": "str", "t": "text"}, {"c": ") \u2013 a prefix that will be added to the name of the module", "t": "text"}, {"c": "remove_duplicate", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ") \u2013 whether to remove the duplicated module instances in the result\nor not", "t": "text"}, {"c": "Yields", "t": "text"}, {"c": "(str, Module)", "t": "text"}, {"c": " \u2013 Tuple of name and module", "t": "text"}, {"c": "Note", "t": "text"}, {"c": "Duplicate modules are returned only once. In the following\nexample, ", "t": "text"}, {"c": "l", "t": "code-inline"}, {"c": " will be returned only once.", "t": "text"}, {"c": "Example:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">l</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">,</span> <span class=\"n\">l</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"n\">m</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">named_modules</span><span class=\"p\">()):</span>\n<span class=\"gp\">... </span>    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"s1\">'-&gt;'</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">)</span>\n\n<span class=\"go\">0 -&gt; ('', Sequential(</span>\n<span class=\"go\">  (0): Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">  (1): Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">))</span>\n<span class=\"go\">1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</span>\n</pre>", "inline": false, "content": {"code_content": ">>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n...     print(idx, '->', m)\n\n0 -> ('', Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.named_parameters\"><span class=\"sig-name descname\"><span class=\"pre\">named_parameters</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prefix</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">recurse</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">remove_duplicate</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.named_parameters\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.named_parameters\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>prefix</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 prefix to prepend to all parameter names.</p></li><li><p><strong>recurse</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.</p></li><li><p><strong>remove_duplicate</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a><em>, </em><em>optional</em>) \u2013 whether to remove the duplicated\nparameters in the result. Defaults to True.</p></li></ul></dd><dt class=\"field-even\">Yields</dt><dd class=\"field-even\"><p><em>(str, Parameter)</em> \u2013 Tuple containing the name and parameter</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Iterator\" title=\"(in Python v3.13)\"><em>Iterator</em></a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Tuple\" title=\"(in Python v3.13)\"><em>Tuple</em></a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\">str</a>, <a class=\"reference internal\" href=\"torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter\" title=\"torch.nn.parameter.Parameter\"><em>Parameter</em></a>]]</p></dd></dl><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "named_parameters", "t": "text"}, {"c": "(", "t": "text"}, {"c": "prefix", "t": "text"}, {"c": "=", "t": "text"}, {"c": "''", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "recurse", "t": "text"}, {"c": "=", "t": "text"}, {"c": "True", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "remove_duplicate", "t": "text"}, {"c": "=", "t": "text"}, {"c": "True", "t": "text"}, {"c": ")", "t": "text"}, {"c": "[source]", "t": "text"}, {"c": "\u00b6", "t": "text"}, {"c": "Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "prefix", "t": "text"}, {"c": " (", "t": "text"}, {"c": "str", "t": "text"}, {"c": ") \u2013 prefix to prepend to all parameter names.", "t": "text"}, {"c": "recurse", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ") \u2013 if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.", "t": "text"}, {"c": "remove_duplicate", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "optional", "t": "text"}, {"c": ") \u2013 whether to remove the duplicated\nparameters in the result. Defaults to True.", "t": "text"}, {"c": "Yields", "t": "text"}, {"c": "(str, Parameter)", "t": "text"}, {"c": " \u2013 Tuple containing the name and parameter", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Iterator", "t": "text"}, {"c": "[", "t": "text"}, {"c": "Tuple", "t": "text"}, {"c": "[", "t": "text"}, {"c": "str", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "Parameter", "t": "text"}, {"c": "]]", "t": "text"}, {"c": "Example:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">param</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">named_parameters</span><span class=\"p\">():</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">if</span> <span class=\"n\">name</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">'bias'</span><span class=\"p\">]:</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">param</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">())</span>\n</pre>", "inline": false, "content": {"code_content": ">>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dd><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>recurse</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.</p></dd><dt class=\"field-even\">Yields</dt><dd class=\"field-even\"><p><em>Parameter</em> \u2013 module parameter</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "Return an iterator over module parameters.", "t": "text"}, {"c": "This is typically passed to an optimizer.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "recurse", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ") \u2013 if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.", "t": "text"}, {"c": "Yields", "t": "text"}, {"c": "Parameter", "t": "text"}, {"c": " \u2013 module parameter", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Example:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">param</span> <span class=\"ow\">in</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">():</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">param</span><span class=\"p\">),</span> <span class=\"n\">param</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">())</span>\n<span class=\"go\">&lt;class 'torch.Tensor'&gt; (20L,)</span>\n<span class=\"go\">&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</span>\n</pre>", "inline": false, "content": {"code_content": ">>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dd><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <a class=\"reference internal\" href=\"#torch.nn.Module.register_full_backward_hook\" title=\"torch.nn.Module.register_full_backward_hook\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;register_full_backward_hook()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">register_full_backward_hook()</cccode-inline></a> and\nthe behavior of this function will change in future versions.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Returns</dt><dd class=\"field-odd\"><p>a handle that can be used to remove the added hook by calling\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;handle.remove()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">handle.remove()</cccode-inline></p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"><p></p></dd></dl></dd></dl></dd></dl>", "content": {"items": [[[{"c": "Register a backward hook on the module.", "t": "text"}, {"c": "This function is deprecated in favor of ", "t": "text"}, {"c": "register_full_backward_hook()", "t": "code-inline"}, {"c": " and\nthe behavior of this function will change in future versions.", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "a handle that can be used to remove the added hook by calling\n", "t": "text"}, {"c": "handle.remove()", "t": "code-inline"}, {"c": "Return type", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.utils.hooks.RemovableHandle</span></code>", "inline": false, "content": {"code_content": "torch.utils.hooks.RemovableHandle", "by": "tag_code"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><dl class=\"field-list simple\"><dd class=\"field-even\"><p></p></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_buffer\"><span class=\"sig-name descname\"><span class=\"pre\">register_buffer</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">name</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">tensor</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">persistent</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_buffer\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_buffer\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm\u2019s <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;running_mean&lt;/span&gt;&lt;/code&gt;\nis not a parameter, but is part of the module\u2019s state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting ' inline=\"true\">running_mean</cccode-inline>\nis not a parameter, but is part of the module\u2019s state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;persistent&lt;/span&gt;&lt;/code&gt; to ' inline=\"true\">persistent</cccode-inline> to <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt;. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module\u2019s\n' inline=\"true\">False</cccode-inline>. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module\u2019s\n<a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict</cccode-inline></a>.</p><p>Buffers can be accessed as attributes using given names.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 name of the buffer. The buffer can be accessed\nfrom this module using the given name</p></li><li><p><strong>tensor</strong> (<a class=\"reference internal\" href=\"../tensors.html#torch.Tensor\" title=\"torch.Tensor\"><em>Tensor</em></a><em> or </em><em>None</em>) \u2013 buffer to be registered. If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;None&lt;/span&gt;&lt;/code&gt;, then operations\nthat run on buffers, such as ' inline=\"true\">None</cccode-inline>, then operations\nthat run on buffers, such as <a class=\"reference internal\" href=\"#torch.nn.Module.cuda\" title=\"torch.nn.Module.cuda\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;cuda&lt;/span&gt;&lt;/code&gt;' inline=\"true\">cuda</cccode-inline></a>, are ignored. If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;None&lt;/span&gt;&lt;/code&gt;,\nthe buffer is ' inline=\"true\">None</cccode-inline>,\nthe buffer is <strong>not</strong> included in the module\u2019s <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict</cccode-inline></a>.</p></li><li><p><strong>persistent</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 whether the buffer is part of this module\u2019s\n<a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict</cccode-inline></a>.</p></li></ul></dd></dl><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "register_buffer", "t": "text"}, {"c": "(", "t": "text"}, {"c": "name", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "tensor", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "persistent", "t": "text"}, {"c": "=", "t": "text"}, {"c": "True", "t": "text"}, {"c": ")", "t": "text"}, {"c": "[source]", "t": "text"}, {"c": "\u00b6", "t": "text"}, {"c": "Add a buffer to the module.", "t": "text"}, {"c": "This is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm\u2019s ", "t": "text"}, {"c": "running_mean", "t": "code-inline"}, {"c": "\nis not a parameter, but is part of the module\u2019s state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting ", "t": "text"}, {"c": "persistent", "t": "code-inline"}, {"c": " to ", "t": "text"}, {"c": "False", "t": "code-inline"}, {"c": ". The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module\u2019s\n", "t": "text"}, {"c": "state_dict", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "Buffers can be accessed as attributes using given names.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "name", "t": "text"}, {"c": " (", "t": "text"}, {"c": "str", "t": "text"}, {"c": ") \u2013 name of the buffer. The buffer can be accessed\nfrom this module using the given name", "t": "text"}, {"c": "tensor", "t": "text"}, {"c": " (", "t": "text"}, {"c": "Tensor", "t": "text"}, {"c": " or ", "t": "text"}, {"c": "None", "t": "text"}, {"c": ") \u2013 buffer to be registered. If ", "t": "text"}, {"c": "None", "t": "code-inline"}, {"c": ", then operations\nthat run on buffers, such as ", "t": "text"}, {"c": "cuda", "t": "code-inline"}, {"c": ", are ignored. If ", "t": "text"}, {"c": "None", "t": "code-inline"}, {"c": ",\nthe buffer is ", "t": "text"}, {"c": "not", "t": "text"}, {"c": " included in the module\u2019s ", "t": "text"}, {"c": "state_dict", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "persistent", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ") \u2013 whether the buffer is part of this module\u2019s\n", "t": "text"}, {"c": "state_dict", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "Example:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">register_buffer</span><span class=\"p\">(</span><span class=\"s1\">'running_mean'</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">num_features</span><span class=\"p\">))</span>\n</pre>", "inline": false, "content": {"code_content": ">>> self.register_buffer('running_mean', torch.zeros(num_features))", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_forward_hook\"><span class=\"sig-name descname\"><span class=\"pre\">register_forward_hook</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">hook</span></span></em>, <em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prepend</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">with_kwargs</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">always_call</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_forward_hook\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_forward_hook\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Register a forward hook on the module.</p><p>The hook will be called every time after <a class=\"reference internal\" href=\"#torch.nn.Module.forward\" title=\"torch.nn.Module.forward\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">forward()</cccode-inline></a> has computed an output.</p><p>If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;with_kwargs&lt;/span&gt;&lt;/code&gt; is ' inline=\"true\">with_kwargs</cccode-inline> is <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt; or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won\u2019t be\npassed to the hooks and only to the ' inline=\"true\">False</cccode-inline> or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won\u2019t be\npassed to the hooks and only to the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward&lt;/span&gt;&lt;/code&gt;. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after ' inline=\"true\">forward</cccode-inline>. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after <a class=\"reference internal\" href=\"#torch.nn.Module.forward\" title=\"torch.nn.Module.forward\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">forward()</cccode-inline></a> is called. The hook\nshould have the following signature:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "register_forward_hook", "t": "text"}, {"c": "(", "t": "text"}, {"c": "hook", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "*", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "prepend", "t": "text"}, {"c": "=", "t": "text"}, {"c": "False", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "with_kwargs", "t": "text"}, {"c": "=", "t": "text"}, {"c": "False", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "always_call", "t": "text"}, {"c": "=", "t": "text"}, {"c": "False", "t": "text"}, {"c": ")", "t": "text"}, {"c": "[source]", "t": "text"}, {"c": "\u00b6", "t": "text"}, {"c": "Register a forward hook on the module.", "t": "text"}, {"c": "The hook will be called every time after ", "t": "text"}, {"c": "forward()", "t": "code-inline"}, {"c": " has computed an output.", "t": "text"}, {"c": "If ", "t": "text"}, {"c": "with_kwargs", "t": "code-inline"}, {"c": " is ", "t": "text"}, {"c": "False", "t": "code-inline"}, {"c": " or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won\u2019t be\npassed to the hooks and only to the ", "t": "text"}, {"c": "forward", "t": "code-inline"}, {"c": ". The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after ", "t": "text"}, {"c": "forward()", "t": "code-inline"}, {"c": " is called. The hook\nshould have the following signature:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre><span class=\"n\">hook</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span> <span class=\"ow\">or</span> <span class=\"n\">modified</span> <span class=\"n\">output</span>\n</pre>", "inline": false, "content": {"code_content": "hook(module, args, output) -> None or modified output", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div><p>If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;with_kwargs&lt;/span&gt;&lt;/code&gt; is ' inline=\"true\">with_kwargs</cccode-inline> is <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;, the forward hook will be passed the\n' inline=\"true\">True</cccode-inline>, the forward hook will be passed the\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;kwargs&lt;/span&gt;&lt;/code&gt; given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature:' inline=\"true\">kwargs</cccode-inline> given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "If ", "t": "text"}, {"c": "with_kwargs", "t": "code-inline"}, {"c": " is ", "t": "text"}, {"c": "True", "t": "code-inline"}, {"c": ", the forward hook will be passed the\n", "t": "text"}, {"c": "kwargs", "t": "code-inline"}, {"c": " given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre><span class=\"n\">hook</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">kwargs</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span> <span class=\"ow\">or</span> <span class=\"n\">modified</span> <span class=\"n\">output</span>\n</pre>", "inline": false, "content": {"code_content": "hook(module, args, kwargs, output) -> None or modified output", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>hook</strong> (<em>Callable</em>) \u2013 The user defined hook to be registered.</p></li><li><p><strong>prepend</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;, the provided ' inline=\"true\">True</cccode-inline>, the provided <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be fired\nbefore all existing ' inline=\"true\">hook</cccode-inline> will be fired\nbefore all existing <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward&lt;/span&gt;&lt;/code&gt; hooks on this\n' inline=\"true\">forward</cccode-inline> hooks on this\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.nn.modules.Module&lt;/span&gt;&lt;/code&gt;. Otherwise, the provided\n' inline=\"true\">torch.nn.modules.Module</cccode-inline>. Otherwise, the provided\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be fired after all existing ' inline=\"true\">hook</cccode-inline> will be fired after all existing <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward&lt;/span&gt;&lt;/code&gt; hooks on\nthis ' inline=\"true\">forward</cccode-inline> hooks on\nthis <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.nn.modules.Module&lt;/span&gt;&lt;/code&gt;. Note that global\n' inline=\"true\">torch.nn.modules.Module</cccode-inline>. Note that global\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward&lt;/span&gt;&lt;/code&gt; hooks registered with\n' inline=\"true\">forward</cccode-inline> hooks registered with\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;register_module_forward_hook()&lt;/span&gt;&lt;/code&gt; will fire before all hooks\nregistered by this method.\nDefault: ' inline=\"true\">register_module_forward_hook()</cccode-inline> will fire before all hooks\nregistered by this method.\nDefault: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt;' inline=\"true\">False</cccode-inline></p></li><li><p><strong>with_kwargs</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;, the ' inline=\"true\">True</cccode-inline>, the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be passed the\nkwargs given to the forward function.\nDefault: ' inline=\"true\">hook</cccode-inline> will be passed the\nkwargs given to the forward function.\nDefault: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt;' inline=\"true\">False</cccode-inline></p></li><li><p><strong>always_call</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt; the ' inline=\"true\">True</cccode-inline> the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: ' inline=\"true\">hook</cccode-inline> will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt;' inline=\"true\">False</cccode-inline></p></li></ul></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>a handle that can be used to remove the added hook by calling\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;handle.remove()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">handle.remove()</cccode-inline></p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"><p></p></dd></dl></dd></dl></dd></dl>", "content": {"items": [[[{"c": "Parameters", "t": "text"}, {"c": "hook", "t": "text"}, {"c": " (", "t": "text"}, {"c": "Callable", "t": "text"}, {"c": ") \u2013 The user defined hook to be registered.", "t": "text"}, {"c": "prepend", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ") \u2013 If ", "t": "text"}, {"c": "True", "t": "code-inline"}, {"c": ", the provided ", "t": "text"}, {"c": "hook", "t": "code-inline"}, {"c": " will be fired\nbefore all existing ", "t": "text"}, {"c": "forward", "t": "code-inline"}, {"c": " hooks on this\n", "t": "text"}, {"c": "torch.nn.modules.Module", "t": "code-inline"}, {"c": ". Otherwise, the provided\n", "t": "text"}, {"c": "hook", "t": "code-inline"}, {"c": " will be fired after all existing ", "t": "text"}, {"c": "forward", "t": "code-inline"}, {"c": " hooks on\nthis ", "t": "text"}, {"c": "torch.nn.modules.Module", "t": "code-inline"}, {"c": ". Note that global\n", "t": "text"}, {"c": "forward", "t": "code-inline"}, {"c": " hooks registered with\n", "t": "text"}, {"c": "register_module_forward_hook()", "t": "code-inline"}, {"c": " will fire before all hooks\nregistered by this method.\nDefault: ", "t": "text"}, {"c": "False", "t": "code-inline"}, {"c": "with_kwargs", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ") \u2013 If ", "t": "text"}, {"c": "True", "t": "code-inline"}, {"c": ", the ", "t": "text"}, {"c": "hook", "t": "code-inline"}, {"c": " will be passed the\nkwargs given to the forward function.\nDefault: ", "t": "text"}, {"c": "False", "t": "code-inline"}, {"c": "always_call", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ") \u2013 If ", "t": "text"}, {"c": "True", "t": "code-inline"}, {"c": " the ", "t": "text"}, {"c": "hook", "t": "code-inline"}, {"c": " will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: ", "t": "text"}, {"c": "False", "t": "code-inline"}, {"c": "Returns", "t": "text"}, {"c": "a handle that can be used to remove the added hook by calling\n", "t": "text"}, {"c": "handle.remove()", "t": "code-inline"}, {"c": "Return type", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.utils.hooks.RemovableHandle</span></code>", "inline": false, "content": {"code_content": "torch.utils.hooks.RemovableHandle", "by": "tag_code"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><dl class=\"field-list simple\"><dd class=\"field-odd\"><p></p></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_forward_pre_hook\"><span class=\"sig-name descname\"><span class=\"pre\">register_forward_pre_hook</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">hook</span></span></em>, <em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prepend</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">with_kwargs</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_forward_pre_hook\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_forward_pre_hook\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <a class=\"reference internal\" href=\"#torch.nn.Module.forward\" title=\"torch.nn.Module.forward\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">forward()</cccode-inline></a> is invoked.</p><p>If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;with_kwargs&lt;/span&gt;&lt;/code&gt; is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won\u2019t be\npassed to the hooks and only to the ' inline=\"true\">with_kwargs</cccode-inline> is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won\u2019t be\npassed to the hooks and only to the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward&lt;/span&gt;&lt;/code&gt;. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature:' inline=\"true\">forward</cccode-inline>. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "register_forward_pre_hook", "t": "text"}, {"c": "(", "t": "text"}, {"c": "hook", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "*", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "prepend", "t": "text"}, {"c": "=", "t": "text"}, {"c": "False", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "with_kwargs", "t": "text"}, {"c": "=", "t": "text"}, {"c": "False", "t": "text"}, {"c": ")", "t": "text"}, {"c": "[source]", "t": "text"}, {"c": "\u00b6", "t": "text"}, {"c": "Register a forward pre-hook on the module.", "t": "text"}, {"c": "The hook will be called every time before ", "t": "text"}, {"c": "forward()", "t": "code-inline"}, {"c": " is invoked.", "t": "text"}, {"c": "If ", "t": "text"}, {"c": "with_kwargs", "t": "code-inline"}, {"c": " is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won\u2019t be\npassed to the hooks and only to the ", "t": "text"}, {"c": "forward", "t": "code-inline"}, {"c": ". The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre><span class=\"n\">hook</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span> <span class=\"ow\">or</span> <span class=\"n\">modified</span> <span class=\"nb\">input</span>\n</pre>", "inline": false, "content": {"code_content": "hook(module, args) -> None or modified input", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div><p>If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;with_kwargs&lt;/span&gt;&lt;/code&gt; is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature:' inline=\"true\">with_kwargs</cccode-inline> is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "If ", "t": "text"}, {"c": "with_kwargs", "t": "code-inline"}, {"c": " is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre><span class=\"n\">hook</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">kwargs</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span> <span class=\"ow\">or</span> <span class=\"n\">a</span> <span class=\"nb\">tuple</span> <span class=\"n\">of</span> <span class=\"n\">modified</span> <span class=\"nb\">input</span> <span class=\"ow\">and</span> <span class=\"n\">kwargs</span>\n</pre>", "inline": false, "content": {"code_content": "hook(module, args, kwargs) -> None or a tuple of modified input and kwargs", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>hook</strong> (<em>Callable</em>) \u2013 The user defined hook to be registered.</p></li><li><p><strong>prepend</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If true, the provided <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be fired before\nall existing ' inline=\"true\">hook</cccode-inline> will be fired before\nall existing <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward_pre&lt;/span&gt;&lt;/code&gt; hooks on this\n' inline=\"true\">forward_pre</cccode-inline> hooks on this\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.nn.modules.Module&lt;/span&gt;&lt;/code&gt;. Otherwise, the provided\n' inline=\"true\">torch.nn.modules.Module</cccode-inline>. Otherwise, the provided\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be fired after all existing ' inline=\"true\">hook</cccode-inline> will be fired after all existing <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward_pre&lt;/span&gt;&lt;/code&gt; hooks\non this ' inline=\"true\">forward_pre</cccode-inline> hooks\non this <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.nn.modules.Module&lt;/span&gt;&lt;/code&gt;. Note that global\n' inline=\"true\">torch.nn.modules.Module</cccode-inline>. Note that global\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward_pre&lt;/span&gt;&lt;/code&gt; hooks registered with\n' inline=\"true\">forward_pre</cccode-inline> hooks registered with\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;register_module_forward_pre_hook()&lt;/span&gt;&lt;/code&gt; will fire before all\nhooks registered by this method.\nDefault: ' inline=\"true\">register_module_forward_pre_hook()</cccode-inline> will fire before all\nhooks registered by this method.\nDefault: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt;' inline=\"true\">False</cccode-inline></p></li><li><p><strong>with_kwargs</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If true, the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be passed the kwargs\ngiven to the forward function.\nDefault: ' inline=\"true\">hook</cccode-inline> will be passed the kwargs\ngiven to the forward function.\nDefault: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt;' inline=\"true\">False</cccode-inline></p></li></ul></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>a handle that can be used to remove the added hook by calling\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;handle.remove()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">handle.remove()</cccode-inline></p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"><p></p></dd></dl></dd></dl></dd></dl>", "content": {"items": [[[{"c": "Parameters", "t": "text"}, {"c": "hook", "t": "text"}, {"c": " (", "t": "text"}, {"c": "Callable", "t": "text"}, {"c": ") \u2013 The user defined hook to be registered.", "t": "text"}, {"c": "prepend", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ") \u2013 If true, the provided ", "t": "text"}, {"c": "hook", "t": "code-inline"}, {"c": " will be fired before\nall existing ", "t": "text"}, {"c": "forward_pre", "t": "code-inline"}, {"c": " hooks on this\n", "t": "text"}, {"c": "torch.nn.modules.Module", "t": "code-inline"}, {"c": ". Otherwise, the provided\n", "t": "text"}, {"c": "hook", "t": "code-inline"}, {"c": " will be fired after all existing ", "t": "text"}, {"c": "forward_pre", "t": "code-inline"}, {"c": " hooks\non this ", "t": "text"}, {"c": "torch.nn.modules.Module", "t": "code-inline"}, {"c": ". Note that global\n", "t": "text"}, {"c": "forward_pre", "t": "code-inline"}, {"c": " hooks registered with\n", "t": "text"}, {"c": "register_module_forward_pre_hook()", "t": "code-inline"}, {"c": " will fire before all\nhooks registered by this method.\nDefault: ", "t": "text"}, {"c": "False", "t": "code-inline"}, {"c": "with_kwargs", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ") \u2013 If true, the ", "t": "text"}, {"c": "hook", "t": "code-inline"}, {"c": " will be passed the kwargs\ngiven to the forward function.\nDefault: ", "t": "text"}, {"c": "False", "t": "code-inline"}, {"c": "Returns", "t": "text"}, {"c": "a handle that can be used to remove the added hook by calling\n", "t": "text"}, {"c": "handle.remove()", "t": "code-inline"}, {"c": "Return type", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.utils.hooks.RemovableHandle</span></code>", "inline": false, "content": {"code_content": "torch.utils.hooks.RemovableHandle", "by": "tag_code"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><dl class=\"field-list simple\"><dd class=\"field-odd\"><p></p></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_full_backward_hook\"><span class=\"sig-name descname\"><span class=\"pre\">register_full_backward_hook</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">hook</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prepend</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_full_backward_hook\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_full_backward_hook\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "register_full_backward_hook", "t": "text"}, {"c": "(", "t": "text"}, {"c": "hook", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "prepend", "t": "text"}, {"c": "=", "t": "text"}, {"c": "False", "t": "text"}, {"c": ")", "t": "text"}, {"c": "[source]", "t": "text"}, {"c": "\u00b6", "t": "text"}, {"c": "Register a backward hook on the module.", "t": "text"}, {"c": "The hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre><span class=\"n\">hook</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">grad_input</span><span class=\"p\">,</span> <span class=\"n\">grad_output</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">tuple</span><span class=\"p\">(</span><span class=\"n\">Tensor</span><span class=\"p\">)</span> <span class=\"ow\">or</span> <span class=\"kc\">None</span>\n</pre>", "inline": false, "content": {"code_content": "hook(module, grad_input, grad_output) -> tuple(Tensor) or None", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div><p>The <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;grad_input&lt;/span&gt;&lt;/code&gt; and ' inline=\"true\">grad_input</cccode-inline> and <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;grad_output&lt;/span&gt;&lt;/code&gt; are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of ' inline=\"true\">grad_output</cccode-inline> are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;grad_input&lt;/span&gt;&lt;/code&gt; in\nsubsequent computations. ' inline=\"true\">grad_input</cccode-inline> in\nsubsequent computations. <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;grad_input&lt;/span&gt;&lt;/code&gt; will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin ' inline=\"true\">grad_input</cccode-inline> will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;grad_input&lt;/span&gt;&lt;/code&gt; and ' inline=\"true\">grad_input</cccode-inline> and <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;grad_output&lt;/span&gt;&lt;/code&gt; will be ' inline=\"true\">grad_output</cccode-inline> will be <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;None&lt;/span&gt;&lt;/code&gt; for all non-Tensor\narguments.' inline=\"true\">None</cccode-inline> for all non-Tensor\narguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module\u2019s forward function.</p><div class=\"admonition warning\"><p class=\"admonition-title\">Warning</p><p>Modifying inputs or outputs inplace is not allowed when using backward hooks and\nwill raise an error.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>hook</strong> (<em>Callable</em>) \u2013 The user-defined hook to be registered.</p></li><li><p><strong>prepend</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If true, the provided <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be fired before\nall existing ' inline=\"true\">hook</cccode-inline> will be fired before\nall existing <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;backward&lt;/span&gt;&lt;/code&gt; hooks on this\n' inline=\"true\">backward</cccode-inline> hooks on this\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.nn.modules.Module&lt;/span&gt;&lt;/code&gt;. Otherwise, the provided\n' inline=\"true\">torch.nn.modules.Module</cccode-inline>. Otherwise, the provided\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be fired after all existing ' inline=\"true\">hook</cccode-inline> will be fired after all existing <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;backward&lt;/span&gt;&lt;/code&gt; hooks on\nthis ' inline=\"true\">backward</cccode-inline> hooks on\nthis <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.nn.modules.Module&lt;/span&gt;&lt;/code&gt;. Note that global\n' inline=\"true\">torch.nn.modules.Module</cccode-inline>. Note that global\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;backward&lt;/span&gt;&lt;/code&gt; hooks registered with\n' inline=\"true\">backward</cccode-inline> hooks registered with\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;register_module_full_backward_hook()&lt;/span&gt;&lt;/code&gt; will fire before\nall hooks registered by this method.' inline=\"true\">register_module_full_backward_hook()</cccode-inline> will fire before\nall hooks registered by this method.</p></li></ul></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>a handle that can be used to remove the added hook by calling\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;handle.remove()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">handle.remove()</cccode-inline></p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"><p></p></dd></dl></dd></dl></dd></dl>", "content": {"items": [[[{"c": "The ", "t": "text"}, {"c": "grad_input", "t": "code-inline"}, {"c": " and ", "t": "text"}, {"c": "grad_output", "t": "code-inline"}, {"c": " are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of ", "t": "text"}, {"c": "grad_input", "t": "code-inline"}, {"c": " in\nsubsequent computations. ", "t": "text"}, {"c": "grad_input", "t": "code-inline"}, {"c": " will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin ", "t": "text"}, {"c": "grad_input", "t": "code-inline"}, {"c": " and ", "t": "text"}, {"c": "grad_output", "t": "code-inline"}, {"c": " will be ", "t": "text"}, {"c": "None", "t": "code-inline"}, {"c": " for all non-Tensor\narguments.", "t": "text"}, {"c": "For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module\u2019s forward function.", "t": "text"}, {"c": "Warning", "t": "text"}, {"c": "Modifying inputs or outputs inplace is not allowed when using backward hooks and\nwill raise an error.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "hook", "t": "text"}, {"c": " (", "t": "text"}, {"c": "Callable", "t": "text"}, {"c": ") \u2013 The user-defined hook to be registered.", "t": "text"}, {"c": "prepend", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ") \u2013 If true, the provided ", "t": "text"}, {"c": "hook", "t": "code-inline"}, {"c": " will be fired before\nall existing ", "t": "text"}, {"c": "backward", "t": "code-inline"}, {"c": " hooks on this\n", "t": "text"}, {"c": "torch.nn.modules.Module", "t": "code-inline"}, {"c": ". Otherwise, the provided\n", "t": "text"}, {"c": "hook", "t": "code-inline"}, {"c": " will be fired after all existing ", "t": "text"}, {"c": "backward", "t": "code-inline"}, {"c": " hooks on\nthis ", "t": "text"}, {"c": "torch.nn.modules.Module", "t": "code-inline"}, {"c": ". Note that global\n", "t": "text"}, {"c": "backward", "t": "code-inline"}, {"c": " hooks registered with\n", "t": "text"}, {"c": "register_module_full_backward_hook()", "t": "code-inline"}, {"c": " will fire before\nall hooks registered by this method.", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "a handle that can be used to remove the added hook by calling\n", "t": "text"}, {"c": "handle.remove()", "t": "code-inline"}, {"c": "Return type", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.utils.hooks.RemovableHandle</span></code>", "inline": false, "content": {"code_content": "torch.utils.hooks.RemovableHandle", "by": "tag_code"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><dl class=\"field-list simple\"><dd class=\"field-odd\"><p></p></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_full_backward_pre_hook\"><span class=\"sig-name descname\"><span class=\"pre\">register_full_backward_pre_hook</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">hook</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prepend</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_full_backward_pre_hook\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_full_backward_pre_hook\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "register_full_backward_pre_hook", "t": "text"}, {"c": "(", "t": "text"}, {"c": "hook", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "prepend", "t": "text"}, {"c": "=", "t": "text"}, {"c": "False", "t": "text"}, {"c": ")", "t": "text"}, {"c": "[source]", "t": "text"}, {"c": "\u00b6", "t": "text"}, {"c": "Register a backward pre-hook on the module.", "t": "text"}, {"c": "The hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre><span class=\"n\">hook</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">grad_output</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"ow\">or</span> <span class=\"kc\">None</span>\n</pre>", "inline": false, "content": {"code_content": "hook(module, grad_output) -> tuple[Tensor] or None", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div><p>The <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;grad_output&lt;/span&gt;&lt;/code&gt; is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of ' inline=\"true\">grad_output</cccode-inline> is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;grad_output&lt;/span&gt;&lt;/code&gt; in\nsubsequent computations. Entries in ' inline=\"true\">grad_output</cccode-inline> in\nsubsequent computations. Entries in <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;grad_output&lt;/span&gt;&lt;/code&gt; will be ' inline=\"true\">grad_output</cccode-inline> will be <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;None&lt;/span&gt;&lt;/code&gt; for\nall non-Tensor arguments.' inline=\"true\">None</cccode-inline> for\nall non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module\u2019s forward function.</p><div class=\"admonition warning\"><p class=\"admonition-title\">Warning</p><p>Modifying inputs inplace is not allowed when using backward hooks and\nwill raise an error.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>hook</strong> (<em>Callable</em>) \u2013 The user-defined hook to be registered.</p></li><li><p><strong>prepend</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If true, the provided <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be fired before\nall existing ' inline=\"true\">hook</cccode-inline> will be fired before\nall existing <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;backward_pre&lt;/span&gt;&lt;/code&gt; hooks on this\n' inline=\"true\">backward_pre</cccode-inline> hooks on this\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.nn.modules.Module&lt;/span&gt;&lt;/code&gt;. Otherwise, the provided\n' inline=\"true\">torch.nn.modules.Module</cccode-inline>. Otherwise, the provided\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be fired after all existing ' inline=\"true\">hook</cccode-inline> will be fired after all existing <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;backward_pre&lt;/span&gt;&lt;/code&gt; hooks\non this ' inline=\"true\">backward_pre</cccode-inline> hooks\non this <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.nn.modules.Module&lt;/span&gt;&lt;/code&gt;. Note that global\n' inline=\"true\">torch.nn.modules.Module</cccode-inline>. Note that global\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;backward_pre&lt;/span&gt;&lt;/code&gt; hooks registered with\n' inline=\"true\">backward_pre</cccode-inline> hooks registered with\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;register_module_full_backward_pre_hook()&lt;/span&gt;&lt;/code&gt; will fire before\nall hooks registered by this method.' inline=\"true\">register_module_full_backward_pre_hook()</cccode-inline> will fire before\nall hooks registered by this method.</p></li></ul></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>a handle that can be used to remove the added hook by calling\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;handle.remove()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">handle.remove()</cccode-inline></p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"><p></p></dd></dl></dd></dl></dd></dl>", "content": {"items": [[[{"c": "The ", "t": "text"}, {"c": "grad_output", "t": "code-inline"}, {"c": " is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of ", "t": "text"}, {"c": "grad_output", "t": "code-inline"}, {"c": " in\nsubsequent computations. Entries in ", "t": "text"}, {"c": "grad_output", "t": "code-inline"}, {"c": " will be ", "t": "text"}, {"c": "None", "t": "code-inline"}, {"c": " for\nall non-Tensor arguments.", "t": "text"}, {"c": "For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module\u2019s forward function.", "t": "text"}, {"c": "Warning", "t": "text"}, {"c": "Modifying inputs inplace is not allowed when using backward hooks and\nwill raise an error.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "hook", "t": "text"}, {"c": " (", "t": "text"}, {"c": "Callable", "t": "text"}, {"c": ") \u2013 The user-defined hook to be registered.", "t": "text"}, {"c": "prepend", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ") \u2013 If true, the provided ", "t": "text"}, {"c": "hook", "t": "code-inline"}, {"c": " will be fired before\nall existing ", "t": "text"}, {"c": "backward_pre", "t": "code-inline"}, {"c": " hooks on this\n", "t": "text"}, {"c": "torch.nn.modules.Module", "t": "code-inline"}, {"c": ". Otherwise, the provided\n", "t": "text"}, {"c": "hook", "t": "code-inline"}, {"c": " will be fired after all existing ", "t": "text"}, {"c": "backward_pre", "t": "code-inline"}, {"c": " hooks\non this ", "t": "text"}, {"c": "torch.nn.modules.Module", "t": "code-inline"}, {"c": ". Note that global\n", "t": "text"}, {"c": "backward_pre", "t": "code-inline"}, {"c": " hooks registered with\n", "t": "text"}, {"c": "register_module_full_backward_pre_hook()", "t": "code-inline"}, {"c": " will fire before\nall hooks registered by this method.", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "a handle that can be used to remove the added hook by calling\n", "t": "text"}, {"c": "handle.remove()", "t": "code-inline"}, {"c": "Return type", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.utils.hooks.RemovableHandle</span></code>", "inline": false, "content": {"code_content": "torch.utils.hooks.RemovableHandle", "by": "tag_code"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><dl class=\"field-list simple\"><dd class=\"field-odd\"><p></p></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Register a post-hook to be run after module\u2019s <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;load_state_dict()&lt;/span&gt;&lt;/code&gt; is called.' inline=\"true\">load_state_dict()</cccode-inline> is called.</p><dl class=\"simple\"><dt>It should have the following signature::</dt><dd><p>hook(module, incompatible_keys) -&gt; None</p></dd></dl><p>The <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;module&lt;/span&gt;&lt;/code&gt; argument is the current module that this hook is registered\non, and the ' inline=\"true\">module</cccode-inline> argument is the current module that this hook is registered\non, and the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;incompatible_keys&lt;/span&gt;&lt;/code&gt; argument is a ' inline=\"true\">incompatible_keys</cccode-inline> argument is a <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;NamedTuple&lt;/span&gt;&lt;/code&gt; consisting\nof attributes ' inline=\"true\">NamedTuple</cccode-inline> consisting\nof attributes <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;missing_keys&lt;/span&gt;&lt;/code&gt; and ' inline=\"true\">missing_keys</cccode-inline> and <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;unexpected_keys&lt;/span&gt;&lt;/code&gt;. ' inline=\"true\">unexpected_keys</cccode-inline>. <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;missing_keys&lt;/span&gt;&lt;/code&gt;\nis a ' inline=\"true\">missing_keys</cccode-inline>\nis a <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;list&lt;/span&gt;&lt;/code&gt; of ' inline=\"true\">list</cccode-inline> of <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;str&lt;/span&gt;&lt;/code&gt; containing the missing keys and\n' inline=\"true\">str</cccode-inline> containing the missing keys and\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;unexpected_keys&lt;/span&gt;&lt;/code&gt; is a ' inline=\"true\">unexpected_keys</cccode-inline> is a <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;list&lt;/span&gt;&lt;/code&gt; of ' inline=\"true\">list</cccode-inline> of <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;str&lt;/span&gt;&lt;/code&gt; containing the unexpected keys.' inline=\"true\">str</cccode-inline> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <a class=\"reference internal\" href=\"#torch.nn.Module.load_state_dict\" title=\"torch.nn.Module.load_state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;load_state_dict()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">load_state_dict()</cccode-inline></a> with\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;strict=True&lt;/span&gt;&lt;/code&gt; are affected by modifications the hook makes to\n' inline=\"true\">strict=True</cccode-inline> are affected by modifications the hook makes to\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;missing_keys&lt;/span&gt;&lt;/code&gt; or ' inline=\"true\">missing_keys</cccode-inline> or <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;unexpected_keys&lt;/span&gt;&lt;/code&gt;, as expected. Additions to either\nset of keys will result in an error being thrown when ' inline=\"true\">unexpected_keys</cccode-inline>, as expected. Additions to either\nset of keys will result in an error being thrown when <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;strict=True&lt;/span&gt;&lt;/code&gt;, and\nclearing out both missing and unexpected keys will avoid an error.' inline=\"true\">strict=True</cccode-inline>, and\nclearing out both missing and unexpected keys will avoid an error.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Returns</dt><dd class=\"field-odd\"><p>a handle that can be used to remove the added hook by calling\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;handle.remove()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">handle.remove()</cccode-inline></p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"><p></p></dd></dl></dd></dl></dd></dl>", "content": {"items": [[[{"c": "Register a post-hook to be run after module\u2019s ", "t": "text"}, {"c": "load_state_dict()", "t": "code-inline"}, {"c": " is called.", "t": "text"}, {"c": "It should have the following signature::", "t": "text"}, {"c": "hook(module, incompatible_keys) -> None", "t": "text"}, {"c": "The ", "t": "text"}, {"c": "module", "t": "code-inline"}, {"c": " argument is the current module that this hook is registered\non, and the ", "t": "text"}, {"c": "incompatible_keys", "t": "code-inline"}, {"c": " argument is a ", "t": "text"}, {"c": "NamedTuple", "t": "code-inline"}, {"c": " consisting\nof attributes ", "t": "text"}, {"c": "missing_keys", "t": "code-inline"}, {"c": " and ", "t": "text"}, {"c": "unexpected_keys", "t": "code-inline"}, {"c": ". ", "t": "text"}, {"c": "missing_keys", "t": "code-inline"}, {"c": "\nis a ", "t": "text"}, {"c": "list", "t": "code-inline"}, {"c": " of ", "t": "text"}, {"c": "str", "t": "code-inline"}, {"c": " containing the missing keys and\n", "t": "text"}, {"c": "unexpected_keys", "t": "code-inline"}, {"c": " is a ", "t": "text"}, {"c": "list", "t": "code-inline"}, {"c": " of ", "t": "text"}, {"c": "str", "t": "code-inline"}, {"c": " containing the unexpected keys.", "t": "text"}, {"c": "The given incompatible_keys can be modified inplace if needed.", "t": "text"}, {"c": "Note that the checks performed when calling ", "t": "text"}, {"c": "load_state_dict()", "t": "code-inline"}, {"c": " with\n", "t": "text"}, {"c": "strict=True", "t": "code-inline"}, {"c": " are affected by modifications the hook makes to\n", "t": "text"}, {"c": "missing_keys", "t": "code-inline"}, {"c": " or ", "t": "text"}, {"c": "unexpected_keys", "t": "code-inline"}, {"c": ", as expected. Additions to either\nset of keys will result in an error being thrown when ", "t": "text"}, {"c": "strict=True", "t": "code-inline"}, {"c": ", and\nclearing out both missing and unexpected keys will avoid an error.", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "a handle that can be used to remove the added hook by calling\n", "t": "text"}, {"c": "handle.remove()", "t": "code-inline"}, {"c": "Return type", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.utils.hooks.RemovableHandle</span></code>", "inline": false, "content": {"code_content": "torch.utils.hooks.RemovableHandle", "by": "tag_code"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><dl class=\"field-list simple\"><dd class=\"field-even\"><p></p></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Register a pre-hook to be run before module\u2019s <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;load_state_dict()&lt;/span&gt;&lt;/code&gt; is called.' inline=\"true\">load_state_dict()</cccode-inline> is called.</p><dl class=\"simple\"><dt>It should have the following signature::</dt><dd><p>hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950</p></dd></dl><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>hook</strong> (<em>Callable</em>) \u2013 Callable hook that will be invoked before\nloading the state dict.</p></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_module\"><span class=\"sig-name descname\"><span class=\"pre\">register_module</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">name</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">module</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_module\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_module\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Alias for <a class=\"reference internal\" href=\"#torch.nn.Module.add_module\" title=\"torch.nn.Module.add_module\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;add_module()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">add_module()</cccode-inline></a>.</p><dl class=\"field-list simple\"></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_parameter\"><span class=\"sig-name descname\"><span class=\"pre\">register_parameter</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">name</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">param</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_parameter\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_parameter\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 name of the parameter. The parameter can be accessed\nfrom this module using the given name</p></li><li><p><strong>param</strong> (<a class=\"reference internal\" href=\"torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter\" title=\"torch.nn.parameter.Parameter\"><em>Parameter</em></a><em> or </em><em>None</em>) \u2013 parameter to be added to the module. If\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;None&lt;/span&gt;&lt;/code&gt;, then operations that run on parameters, such as ' inline=\"true\">None</cccode-inline>, then operations that run on parameters, such as <a class=\"reference internal\" href=\"#torch.nn.Module.cuda\" title=\"torch.nn.Module.cuda\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;cuda&lt;/span&gt;&lt;/code&gt;' inline=\"true\">cuda</cccode-inline></a>,\nare ignored. If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;None&lt;/span&gt;&lt;/code&gt;, the parameter is ' inline=\"true\">None</cccode-inline>, the parameter is <strong>not</strong> included in the\nmodule\u2019s <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict</cccode-inline></a>.</p></li></ul></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Register a post-hook for the <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict()</cccode-inline></a> method.</p><dl class=\"simple\"><dt>It should have the following signature::</dt><dd><p>hook(module, state_dict, prefix, local_metadata) -&gt; None</p></dd></dl><p>The registered hooks can modify the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt; inplace.' inline=\"true\">state_dict</cccode-inline> inplace.</p></dd></dl><dl class=\"py method\"><dd><p>Register a pre-hook for the <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict()</cccode-inline></a> method.</p><dl class=\"simple\"><dt>It should have the following signature::</dt><dd><p>hook(module, prefix, keep_vars) -&gt; None</p></dd></dl><p>The registered hooks can be used to perform pre-processing before the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;\ncall is made.' inline=\"true\">state_dict</cccode-inline>\ncall is made.</p></dd></dl><dl class=\"py method\"><dd><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters\u2019 <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;requires_grad&lt;/span&gt;&lt;/code&gt; attributes\nin-place.' inline=\"true\">requires_grad</cccode-inline> attributes\nin-place.</p><p>This method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).</p><p>See <a class=\"reference internal\" href=\"../notes/autograd.html#locally-disable-grad-doc\"><span class=\"std std-ref\">Locally disabling gradient computation</span></a> for a comparison between\n.requires_grad_() and several similar mechanisms that may be confused with it.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>requires_grad</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 whether autograd should record operations on\nparameters in this module. Default: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">True</cccode-inline>.</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>self</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Set extra state contained in the loaded state_dict.</p><p>This function is called from <a class=\"reference internal\" href=\"#torch.nn.Module.load_state_dict\" title=\"torch.nn.Module.load_state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;load_state_dict()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">load_state_dict()</cccode-inline></a> to handle any extra state\nfound within the state_dict. Implement this function and a corresponding\n<a class=\"reference internal\" href=\"#torch.nn.Module.get_extra_state\" title=\"torch.nn.Module.get_extra_state\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_extra_state()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">get_extra_state()</cccode-inline></a> for your module if you need to store extra state within its\nstate_dict.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>state</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.13)\"><em>dict</em></a>) \u2013 Extra state from the state_dict</p></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.set_submodule\"><span class=\"sig-name descname\"><span class=\"pre\">set_submodule</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">target</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">module</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.set_submodule\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.set_submodule\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Set the submodule given by <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt; if it exists, otherwise throw an error.' inline=\"true\">target</cccode-inline> if it exists, otherwise throw an error.</p><p>For example, let\u2019s say you have an <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;nn.Module&lt;/span&gt;&lt;/code&gt; ' inline=\"true\">nn.Module</cccode-inline><cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;A&lt;/span&gt;&lt;/code&gt; that\nlooks like this:' inline=\"true\">A</cccode-inline> that\nlooks like this:</p><div class=\"highlight-text notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "Register a pre-hook to be run before module\u2019s ", "t": "text"}, {"c": "load_state_dict()", "t": "code-inline"}, {"c": " is called.", "t": "text"}, {"c": "It should have the following signature::", "t": "text"}, {"c": "hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "hook", "t": "text"}, {"c": " (", "t": "text"}, {"c": "Callable", "t": "text"}, {"c": ") \u2013 Callable hook that will be invoked before\nloading the state dict.", "t": "text"}, {"c": "register_module", "t": "text"}, {"c": "(", "t": "text"}, {"c": "name", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "module", "t": "text"}, {"c": ")", "t": "text"}, {"c": "[source]", "t": "text"}, {"c": "\u00b6", "t": "text"}, {"c": "Alias for ", "t": "text"}, {"c": "add_module()", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "register_parameter", "t": "text"}, {"c": "(", "t": "text"}, {"c": "name", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "param", "t": "text"}, {"c": ")", "t": "text"}, {"c": "[source]", "t": "text"}, {"c": "\u00b6", "t": "text"}, {"c": "Add a parameter to the module.", "t": "text"}, {"c": "The parameter can be accessed as an attribute using given name.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "name", "t": "text"}, {"c": " (", "t": "text"}, {"c": "str", "t": "text"}, {"c": ") \u2013 name of the parameter. The parameter can be accessed\nfrom this module using the given name", "t": "text"}, {"c": "param", "t": "text"}, {"c": " (", "t": "text"}, {"c": "Parameter", "t": "text"}, {"c": " or ", "t": "text"}, {"c": "None", "t": "text"}, {"c": ") \u2013 parameter to be added to the module. If\n", "t": "text"}, {"c": "None", "t": "code-inline"}, {"c": ", then operations that run on parameters, such as ", "t": "text"}, {"c": "cuda", "t": "code-inline"}, {"c": ",\nare ignored. If ", "t": "text"}, {"c": "None", "t": "code-inline"}, {"c": ", the parameter is ", "t": "text"}, {"c": "not", "t": "text"}, {"c": " included in the\nmodule\u2019s ", "t": "text"}, {"c": "state_dict", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "Register a post-hook for the ", "t": "text"}, {"c": "state_dict()", "t": "code-inline"}, {"c": " method.", "t": "text"}, {"c": "It should have the following signature::", "t": "text"}, {"c": "hook(module, state_dict, prefix, local_metadata) -> None", "t": "text"}, {"c": "The registered hooks can modify the ", "t": "text"}, {"c": "state_dict", "t": "code-inline"}, {"c": " inplace.", "t": "text"}, {"c": "Register a pre-hook for the ", "t": "text"}, {"c": "state_dict()", "t": "code-inline"}, {"c": " method.", "t": "text"}, {"c": "It should have the following signature::", "t": "text"}, {"c": "hook(module, prefix, keep_vars) -> None", "t": "text"}, {"c": "The registered hooks can be used to perform pre-processing before the ", "t": "text"}, {"c": "state_dict", "t": "code-inline"}, {"c": "\ncall is made.", "t": "text"}, {"c": "Change if autograd should record operations on parameters in this module.", "t": "text"}, {"c": "This method sets the parameters\u2019 ", "t": "text"}, {"c": "requires_grad", "t": "code-inline"}, {"c": " attributes\nin-place.", "t": "text"}, {"c": "This method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).", "t": "text"}, {"c": "See ", "t": "text"}, {"c": "Locally disabling gradient computation", "t": "text"}, {"c": " for a comparison between\n.requires_grad_() and several similar mechanisms that may be confused with it.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "requires_grad", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ") \u2013 whether autograd should record operations on\nparameters in this module. Default: ", "t": "text"}, {"c": "True", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "self", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Set extra state contained in the loaded state_dict.", "t": "text"}, {"c": "This function is called from ", "t": "text"}, {"c": "load_state_dict()", "t": "code-inline"}, {"c": " to handle any extra state\nfound within the state_dict. Implement this function and a corresponding\n", "t": "text"}, {"c": "get_extra_state()", "t": "code-inline"}, {"c": " for your module if you need to store extra state within its\nstate_dict.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "state", "t": "text"}, {"c": " (", "t": "text"}, {"c": "dict", "t": "text"}, {"c": ") \u2013 Extra state from the state_dict", "t": "text"}, {"c": "set_submodule", "t": "text"}, {"c": "(", "t": "text"}, {"c": "target", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "module", "t": "text"}, {"c": ")", "t": "text"}, {"c": "[source]", "t": "text"}, {"c": "\u00b6", "t": "text"}, {"c": "Set the submodule given by ", "t": "text"}, {"c": "target", "t": "code-inline"}, {"c": " if it exists, otherwise throw an error.", "t": "text"}, {"c": "For example, let\u2019s say you have an ", "t": "text"}, {"c": "nn.Module", "t": "code-inline"}, {"c": "A", "t": "code-inline"}, {"c": " that\nlooks like this:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre>A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)\n</pre>", "inline": false, "content": {"code_content": "A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-text notranslate\"><div class=\"highlight\"></div></div><p>(The diagram shows an <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;nn.Module&lt;/span&gt;&lt;/code&gt; ' inline=\"true\">nn.Module</cccode-inline><cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;A&lt;/span&gt;&lt;/code&gt;. ' inline=\"true\">A</cccode-inline>. <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;A&lt;/span&gt;&lt;/code&gt; has a nested\nsubmodule ' inline=\"true\">A</cccode-inline> has a nested\nsubmodule <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;net_b&lt;/span&gt;&lt;/code&gt;, which itself has two submodules ' inline=\"true\">net_b</cccode-inline>, which itself has two submodules <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;net_c&lt;/span&gt;&lt;/code&gt;\nand ' inline=\"true\">net_c</cccode-inline>\nand <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;linear&lt;/span&gt;&lt;/code&gt;. ' inline=\"true\">linear</cccode-inline>. <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;net_c&lt;/span&gt;&lt;/code&gt; then has a submodule ' inline=\"true\">net_c</cccode-inline> then has a submodule <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;conv&lt;/span&gt;&lt;/code&gt;.)' inline=\"true\">conv</cccode-inline>.)</p><p>To overide the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;Conv2d&lt;/span&gt;&lt;/code&gt; with a new submodule ' inline=\"true\">Conv2d</cccode-inline> with a new submodule <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;Linear&lt;/span&gt;&lt;/code&gt;, you\nwould call\n' inline=\"true\">Linear</cccode-inline>, you\nwould call\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;set_submodule(\"net_b.net_c.conv\",&lt;/span&gt; &lt;span class=\"pre\"&gt;nn.Linear(33,&lt;/span&gt; &lt;span class=\"pre\"&gt;16))&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))</cccode-inline>.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>target</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)</p></li><li><p><strong>module</strong> (<a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.modules.module.Module\"><em>Module</em></a>) \u2013 The module to set the submodule to.</p></li></ul></dd><dt class=\"field-even\">Raises</dt><dd class=\"field-even\"><ul class=\"simple\"><li><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#ValueError\" title=\"(in Python v3.13)\"><strong>ValueError</strong></a> \u2013 If the target string is empty</p></li><li><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#AttributeError\" title=\"(in Python v3.13)\"><strong>AttributeError</strong></a> \u2013 If the target string references an invalid\n    path or resolves to something that is not an\n    <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;nn.Module&lt;/span&gt;&lt;/code&gt;' inline=\"true\">nn.Module</cccode-inline></p></li></ul></dd></dl></dd></dl><dl class=\"py method\"><dd><p>See <a class=\"reference internal\" href=\"torch.Tensor.share_memory_.html#torch.Tensor.share_memory_\" title=\"torch.Tensor.share_memory_\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.Tensor.share_memory_()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">torch.Tensor.share_memory_()</cccode-inline></a>.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"><p><em>T</em></p></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.state_dict\"><span class=\"sig-name descname\"><span class=\"pre\">state_dict</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">destination</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><span class=\"pre\">T_destination</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prefix</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><span class=\"pre\">str</span></a></span><span class=\"w\"></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">keep_vars</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><span class=\"pre\">bool</span></a></span><span class=\"w\"></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><span class=\"sig-return\"><span class=\"sig-return-typehint\"><span class=\"pre\">T_destination</span></span></span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.state_dict\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.state_dict\" title=\"Permalink to this definition\">\u00b6</a></dt><dt class=\"sig sig-object py\"><span class=\"sig-name descname\"><span class=\"pre\">state_dict</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prefix</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><span class=\"pre\">str</span></a></span><span class=\"w\"></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">keep_vars</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><span class=\"pre\">bool</span></a></span><span class=\"w\"></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><span class=\"sig-return\"><span class=\"sig-return-typehint\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Dict\" title=\"(in Python v3.13)\"><span class=\"pre\">Dict</span></a><span class=\"p\"><span class=\"pre\">[</span></span><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><span class=\"pre\">str</span></a><span class=\"p\"><span class=\"pre\">,</span></span><span class=\"w\"></span><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Any\" title=\"(in Python v3.13)\"><span class=\"pre\">Any</span></a><span class=\"p\"><span class=\"pre\">]</span></span></span></span></dt><dd><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;None&lt;/span&gt;&lt;/code&gt; are not included.' inline=\"true\">None</cccode-inline> are not included.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>The returned object is a shallow copy. It contains references\nto the module\u2019s parameters and buffers.</p></div><div class=\"admonition warning\"><p class=\"admonition-title\">Warning</p><p>Currently <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict()&lt;/span&gt;&lt;/code&gt; also accepts positional arguments for\n' inline=\"true\">state_dict()</cccode-inline> also accepts positional arguments for\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;destination&lt;/span&gt;&lt;/code&gt;, ' inline=\"true\">destination</cccode-inline>, <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;prefix&lt;/span&gt;&lt;/code&gt; and ' inline=\"true\">prefix</cccode-inline> and <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;keep_vars&lt;/span&gt;&lt;/code&gt; in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.' inline=\"true\">keep_vars</cccode-inline> in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.</p></div><div class=\"admonition warning\"><p class=\"admonition-title\">Warning</p><p>Please avoid the use of argument <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;destination&lt;/span&gt;&lt;/code&gt; as it is not\ndesigned for end-users.' inline=\"true\">destination</cccode-inline> as it is not\ndesigned for end-users.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>destination</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.13)\"><em>dict</em></a><em>, </em><em>optional</em>) \u2013 If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;OrderedDict&lt;/span&gt;&lt;/code&gt; will be created and returned.\nDefault: ' inline=\"true\">OrderedDict</cccode-inline> will be created and returned.\nDefault: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;None&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">None</cccode-inline>.</p></li><li><p><strong>prefix</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a><em>, </em><em>optional</em>) \u2013 a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: <cccode-inline by=\"tag_code\" html=\"&lt;code class=&quot;docutils literal notranslate&quot;&gt;&lt;span class=&quot;pre&quot;&gt;''&lt;/span&gt;&lt;/code&gt;.\" inline=\"true\">''</cccode-inline>.</p></li><li><p><strong>keep_vars</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a><em>, </em><em>optional</em>) \u2013 by default the <a class=\"reference internal\" href=\"../tensors.html#torch.Tensor\" title=\"torch.Tensor\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;Tensor&lt;/span&gt;&lt;/code&gt;' inline=\"true\">Tensor</cccode-inline></a> s\nreturned in the state dict are detached from autograd. If it\u2019s\nset to <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;, detaching will not be performed.\nDefault: ' inline=\"true\">True</cccode-inline>, detaching will not be performed.\nDefault: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">False</cccode-inline>.</p></li></ul></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>a dictionary containing a whole state of the module</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "(The diagram shows an ", "t": "text"}, {"c": "nn.Module", "t": "code-inline"}, {"c": "A", "t": "code-inline"}, {"c": ". ", "t": "text"}, {"c": "A", "t": "code-inline"}, {"c": " has a nested\nsubmodule ", "t": "text"}, {"c": "net_b", "t": "code-inline"}, {"c": ", which itself has two submodules ", "t": "text"}, {"c": "net_c", "t": "code-inline"}, {"c": "\nand ", "t": "text"}, {"c": "linear", "t": "code-inline"}, {"c": ". ", "t": "text"}, {"c": "net_c", "t": "code-inline"}, {"c": " then has a submodule ", "t": "text"}, {"c": "conv", "t": "code-inline"}, {"c": ".)", "t": "text"}, {"c": "To overide the ", "t": "text"}, {"c": "Conv2d", "t": "code-inline"}, {"c": " with a new submodule ", "t": "text"}, {"c": "Linear", "t": "code-inline"}, {"c": ", you\nwould call\n", "t": "text"}, {"c": "set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "target", "t": "text"}, {"c": " (", "t": "text"}, {"c": "str", "t": "text"}, {"c": ") \u2013 The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)", "t": "text"}, {"c": "module", "t": "text"}, {"c": " (", "t": "text"}, {"c": "Module", "t": "text"}, {"c": ") \u2013 The module to set the submodule to.", "t": "text"}, {"c": "Raises", "t": "text"}, {"c": "ValueError", "t": "text"}, {"c": " \u2013 If the target string is empty", "t": "text"}, {"c": "AttributeError", "t": "text"}, {"c": " \u2013 If the target string references an invalid\n    path or resolves to something that is not an\n    ", "t": "text"}, {"c": "nn.Module", "t": "code-inline"}, {"c": "See ", "t": "text"}, {"c": "torch.Tensor.share_memory_()", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "T", "t": "text"}, {"c": "state_dict", "t": "text"}, {"c": "(", "t": "text"}, {"c": "*", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "destination", "t": "text"}, {"c": ":", "t": "text"}, {"c": "T_destination", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "prefix", "t": "text"}, {"c": ":", "t": "text"}, {"c": "str", "t": "text"}, {"c": "=", "t": "text"}, {"c": "''", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "keep_vars", "t": "text"}, {"c": ":", "t": "text"}, {"c": "bool", "t": "text"}, {"c": "=", "t": "text"}, {"c": "False", "t": "text"}, {"c": ")", "t": "text"}, {"c": "T_destination", "t": "text"}, {"c": "[source]", "t": "text"}, {"c": "\u00b6", "t": "text"}, {"c": "state_dict", "t": "text"}, {"c": "(", "t": "text"}, {"c": "*", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "prefix", "t": "text"}, {"c": ":", "t": "text"}, {"c": "str", "t": "text"}, {"c": "=", "t": "text"}, {"c": "''", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "keep_vars", "t": "text"}, {"c": ":", "t": "text"}, {"c": "bool", "t": "text"}, {"c": "=", "t": "text"}, {"c": "False", "t": "text"}, {"c": ")", "t": "text"}, {"c": "Dict", "t": "text"}, {"c": "[", "t": "text"}, {"c": "str", "t": "text"}, {"c": ",", "t": "text"}, {"c": "Any", "t": "text"}, {"c": "]", "t": "text"}, {"c": "Return a dictionary containing references to the whole state of the module.", "t": "text"}, {"c": "Both parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ", "t": "text"}, {"c": "None", "t": "code-inline"}, {"c": " are not included.", "t": "text"}, {"c": "Note", "t": "text"}, {"c": "The returned object is a shallow copy. It contains references\nto the module\u2019s parameters and buffers.", "t": "text"}, {"c": "Warning", "t": "text"}, {"c": "Currently ", "t": "text"}, {"c": "state_dict()", "t": "code-inline"}, {"c": " also accepts positional arguments for\n", "t": "text"}, {"c": "destination", "t": "code-inline"}, {"c": ", ", "t": "text"}, {"c": "prefix", "t": "code-inline"}, {"c": " and ", "t": "text"}, {"c": "keep_vars", "t": "code-inline"}, {"c": " in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.", "t": "text"}, {"c": "Warning", "t": "text"}, {"c": "Please avoid the use of argument ", "t": "text"}, {"c": "destination", "t": "code-inline"}, {"c": " as it is not\ndesigned for end-users.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "destination", "t": "text"}, {"c": " (", "t": "text"}, {"c": "dict", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "optional", "t": "text"}, {"c": ") \u2013 If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an ", "t": "text"}, {"c": "OrderedDict", "t": "code-inline"}, {"c": " will be created and returned.\nDefault: ", "t": "text"}, {"c": "None", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "prefix", "t": "text"}, {"c": " (", "t": "text"}, {"c": "str", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "optional", "t": "text"}, {"c": ") \u2013 a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: ", "t": "text"}, {"c": "''", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "keep_vars", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "optional", "t": "text"}, {"c": ") \u2013 by default the ", "t": "text"}, {"c": "Tensor", "t": "code-inline"}, {"c": " s\nreturned in the state dict are detached from autograd. If it\u2019s\nset to ", "t": "text"}, {"c": "True", "t": "code-inline"}, {"c": ", detaching will not be performed.\nDefault: ", "t": "text"}, {"c": "False", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "a dictionary containing a whole state of the module", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Example:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">state_dict</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">()</span>\n<span class=\"go\">['bias', 'weight']</span>\n</pre>", "inline": false, "content": {"code_content": ">>> module.state_dict().keys()\n['bias', 'weight']", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.to\"><span class=\"sig-name descname\"><span class=\"pre\">to</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">device</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Optional\" title=\"(in Python v3.13)\"><span class=\"pre\">Optional</span></a><span class=\"p\"><span class=\"pre\">[</span></span><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Union\" title=\"(in Python v3.13)\"><span class=\"pre\">Union</span></a><span class=\"p\"><span class=\"pre\">[</span></span><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><span class=\"pre\">str</span></a><span class=\"p\"><span class=\"pre\">,</span></span><span class=\"w\"></span><a class=\"reference internal\" href=\"../tensor_attributes.html#torch.device\" title=\"torch.device\"><span class=\"pre\">device</span></a><span class=\"p\"><span class=\"pre\">,</span></span><span class=\"w\"></span><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.13)\"><span class=\"pre\">int</span></a><span class=\"p\"><span class=\"pre\">]</span></span><span class=\"p\"><span class=\"pre\">]</span></span></span><span class=\"w\"></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"></span><span class=\"default_value\"><span class=\"pre\">...</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">dtype</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Optional\" title=\"(in Python v3.13)\"><span class=\"pre\">Optional</span></a><span class=\"p\"><span class=\"pre\">[</span></span><a class=\"reference internal\" href=\"../tensor_attributes.html#torch.dtype\" title=\"torch.dtype\"><span class=\"pre\">dtype</span></a><span class=\"p\"><span class=\"pre\">]</span></span></span><span class=\"w\"></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"></span><span class=\"default_value\"><span class=\"pre\">...</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">non_blocking</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><span class=\"pre\">bool</span></a></span><span class=\"w\"></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"></span><span class=\"default_value\"><span class=\"pre\">...</span></span></em><span class=\"sig-paren\">)</span><span class=\"sig-return\"><span class=\"sig-return-typehint\"><span class=\"pre\">Self</span></span></span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.to\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.to\" title=\"Permalink to this definition\">\u00b6</a></dt><dt class=\"sig sig-object py\"><span class=\"sig-name descname\"><span class=\"pre\">to</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">dtype</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference internal\" href=\"../tensor_attributes.html#torch.dtype\" title=\"torch.dtype\"><span class=\"pre\">dtype</span></a></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">non_blocking</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><span class=\"pre\">bool</span></a></span><span class=\"w\"></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"></span><span class=\"default_value\"><span class=\"pre\">...</span></span></em><span class=\"sig-paren\">)</span><span class=\"sig-return\"><span class=\"sig-return-typehint\"><span class=\"pre\">Self</span></span></span></dt><dt class=\"sig sig-object py\"><span class=\"sig-name descname\"><span class=\"pre\">to</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">tensor</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference internal\" href=\"../tensors.html#torch.Tensor\" title=\"torch.Tensor\"><span class=\"pre\">Tensor</span></a></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">non_blocking</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><span class=\"pre\">bool</span></a></span><span class=\"w\"></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"></span><span class=\"default_value\"><span class=\"pre\">...</span></span></em><span class=\"sig-paren\">)</span><span class=\"sig-return\"><span class=\"sig-return-typehint\"><span class=\"pre\">Self</span></span></span></dt><dd><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>Its signature is similar to <a class=\"reference internal\" href=\"torch.Tensor.to.html#torch.Tensor.to\" title=\"torch.Tensor.to\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.Tensor.to()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">torch.Tensor.to()</cccode-inline></a>, but only accepts\nfloating point or complex <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;dtype&lt;/span&gt;&lt;/code&gt;s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to ' inline=\"true\">dtype</cccode-inline>s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;dtype&lt;/span&gt;&lt;/code&gt;\n(if given). The integral parameters and buffers will be moved\n' inline=\"true\">dtype</cccode-inline>\n(if given). The integral parameters and buffers will be moved\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;device&lt;/span&gt;&lt;/code&gt;, if that is given, but with dtypes unchanged. When\n' inline=\"true\">device</cccode-inline>, if that is given, but with dtypes unchanged. When\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;non_blocking&lt;/span&gt;&lt;/code&gt; is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.' inline=\"true\">non_blocking</cccode-inline> is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.</p><p>See below for examples.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><p>Examples:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl>", "content": {"items": [[[{"c": "to", "t": "text"}, {"c": "(", "t": "text"}, {"c": "device", "t": "text"}, {"c": ":", "t": "text"}, {"c": "Optional", "t": "text"}, {"c": "[", "t": "text"}, {"c": "Union", "t": "text"}, {"c": "[", "t": "text"}, {"c": "str", "t": "text"}, {"c": ",", "t": "text"}, {"c": "device", "t": "text"}, {"c": ",", "t": "text"}, {"c": "int", "t": "text"}, {"c": "]", "t": "text"}, {"c": "]", "t": "text"}, {"c": "=", "t": "text"}, {"c": "...", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "dtype", "t": "text"}, {"c": ":", "t": "text"}, {"c": "Optional", "t": "text"}, {"c": "[", "t": "text"}, {"c": "dtype", "t": "text"}, {"c": "]", "t": "text"}, {"c": "=", "t": "text"}, {"c": "...", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "non_blocking", "t": "text"}, {"c": ":", "t": "text"}, {"c": "bool", "t": "text"}, {"c": "=", "t": "text"}, {"c": "...", "t": "text"}, {"c": ")", "t": "text"}, {"c": "Self", "t": "text"}, {"c": "[source]", "t": "text"}, {"c": "\u00b6", "t": "text"}, {"c": "to", "t": "text"}, {"c": "(", "t": "text"}, {"c": "dtype", "t": "text"}, {"c": ":", "t": "text"}, {"c": "dtype", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "non_blocking", "t": "text"}, {"c": ":", "t": "text"}, {"c": "bool", "t": "text"}, {"c": "=", "t": "text"}, {"c": "...", "t": "text"}, {"c": ")", "t": "text"}, {"c": "Self", "t": "text"}, {"c": "to", "t": "text"}, {"c": "(", "t": "text"}, {"c": "tensor", "t": "text"}, {"c": ":", "t": "text"}, {"c": "Tensor", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "non_blocking", "t": "text"}, {"c": ":", "t": "text"}, {"c": "bool", "t": "text"}, {"c": "=", "t": "text"}, {"c": "...", "t": "text"}, {"c": ")", "t": "text"}, {"c": "Self", "t": "text"}, {"c": "Move and/or cast the parameters and buffers.", "t": "text"}, {"c": "This can be called as", "t": "text"}, {"c": "Its signature is similar to ", "t": "text"}, {"c": "torch.Tensor.to()", "t": "code-inline"}, {"c": ", but only accepts\nfloating point or complex ", "t": "text"}, {"c": "dtype", "t": "code-inline"}, {"c": "s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to ", "t": "text"}, {"c": "dtype", "t": "code-inline"}, {"c": "\n(if given). The integral parameters and buffers will be moved\n", "t": "text"}, {"c": "device", "t": "code-inline"}, {"c": ", if that is given, but with dtypes unchanged. When\n", "t": "text"}, {"c": "non_blocking", "t": "code-inline"}, {"c": " is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.", "t": "text"}, {"c": "See below for examples.", "t": "text"}, {"c": "Note", "t": "text"}, {"c": "This method modifies the module in-place.", "t": "text"}, {"c": "Examples:", "t": "text"}]]], "ordered": true}}, {"type": "code", "raw_content": "<pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">weight</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[ 0.1913, -0.3420],</span>\n<span class=\"go\">        [-0.5113, -0.2325]])</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">double</span><span class=\"p\">)</span>\n<span class=\"go\">Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">weight</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[ 0.1913, -0.3420],</span>\n<span class=\"go\">        [-0.5113, -0.2325]], dtype=torch.float64)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">gpu1</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">\"cuda:1\"</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">gpu1</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">half</span><span class=\"p\">,</span> <span class=\"n\">non_blocking</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"go\">Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">weight</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[ 0.1914, -0.3420],</span>\n<span class=\"go\">        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">cpu</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">\"cpu\"</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">cpu</span><span class=\"p\">)</span>\n<span class=\"go\">Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">weight</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[ 0.1914, -0.3420],</span>\n<span class=\"go\">        [-0.5112, -0.2324]], dtype=torch.float16)</span>\n\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cdouble</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">weight</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[ 0.3741+0.j,  0.2382+0.j],</span>\n<span class=\"go\">        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cdouble</span><span class=\"p\">))</span>\n<span class=\"go\">tensor([[0.6122+0.j, 0.1150+0.j],</span>\n<span class=\"go\">        [0.6122+0.j, 0.1150+0.j],</span>\n<span class=\"go\">        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</span>\n</pre>", "inline": false, "content": {"code_content": ">>> linear = nn.Linear(2, 2)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n>>> linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n>>> gpu1 = torch.device(\"cuda:1\")\n>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n>>> cpu = torch.device(\"cpu\")\n>>> linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n>>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n>>> linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)", "by": "tag_pre"}}, {"type": "list", "raw_content": "<dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.to_empty\"><span class=\"sig-name descname\"><span class=\"pre\">to_empty</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">device</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">recurse</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.to_empty\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.to_empty\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Move the parameters and buffers to the specified device without copying storage.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>device</strong> (<a class=\"reference internal\" href=\"../tensor_attributes.html#torch.device\" title=\"torch.device\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.device&lt;/span&gt;&lt;/code&gt;' inline=\"true\">torch.device</cccode-inline></a>) \u2013 The desired device of the parameters\nand buffers in this module.</p></li><li><p><strong>recurse</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 Whether parameters and buffers of submodules should\nbe recursively moved to the specified device.</p></li></ul></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>self</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Set the module in training mode.</p><p>This has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. <a class=\"reference internal\" href=\"torch.nn.Dropout.html#torch.nn.Dropout\" title=\"torch.nn.Dropout\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;Dropout&lt;/span&gt;&lt;/code&gt;' inline=\"true\">Dropout</cccode-inline></a>, <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;BatchNorm&lt;/span&gt;&lt;/code&gt;,\netc.' inline=\"true\">BatchNorm</cccode-inline>,\netc.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>mode</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 whether to set training mode (<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;) or evaluation\nmode (' inline=\"true\">True</cccode-inline>) or evaluation\nmode (<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt;). Default: ' inline=\"true\">False</cccode-inline>). Default: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">True</cccode-inline>.</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>self</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Casts all parameters and buffers to <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;dst_type&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">dst_type</cccode-inline>.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>dst_type</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#type\" title=\"(in Python v3.13)\"><em>type</em></a><em> or </em><em>string</em>) \u2013 the desired type</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>self</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Move all model parameters and buffers to the XPU.</p><p>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>device</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.13)\"><em>int</em></a><em>, </em><em>optional</em>) \u2013 if specified, all parameters will be\ncopied to that device</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>self</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Reset gradients of all model parameters.</p><p>See similar function under <a class=\"reference internal\" href=\"../optim.html#torch.optim.Optimizer\" title=\"torch.optim.Optimizer\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.optim.Optimizer&lt;/span&gt;&lt;/code&gt;' inline=\"true\">torch.optim.Optimizer</cccode-inline></a> for more context.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>set_to_none</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 instead of setting to zero, set the grads to None.\nSee <a class=\"reference internal\" href=\"torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad\" title=\"torch.optim.Optimizer.zero_grad\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.optim.Optimizer.zero_grad()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">torch.optim.Optimizer.zero_grad()</cccode-inline></a> for details.</p></dd></dl></dd></dl></dd></dl>", "content": {"items": [[[{"c": "to_empty", "t": "text"}, {"c": "(", "t": "text"}, {"c": "*", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "device", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "recurse", "t": "text"}, {"c": "=", "t": "text"}, {"c": "True", "t": "text"}, {"c": ")", "t": "text"}, {"c": "[source]", "t": "text"}, {"c": "\u00b6", "t": "text"}, {"c": "Move the parameters and buffers to the specified device without copying storage.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "device", "t": "text"}, {"c": " (", "t": "text"}, {"c": "torch.device", "t": "code-inline"}, {"c": ") \u2013 The desired device of the parameters\nand buffers in this module.", "t": "text"}, {"c": "recurse", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ") \u2013 Whether parameters and buffers of submodules should\nbe recursively moved to the specified device.", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "self", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Set the module in training mode.", "t": "text"}, {"c": "This has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. ", "t": "text"}, {"c": "Dropout", "t": "code-inline"}, {"c": ", ", "t": "text"}, {"c": "BatchNorm", "t": "code-inline"}, {"c": ",\netc.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "mode", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ") \u2013 whether to set training mode (", "t": "text"}, {"c": "True", "t": "code-inline"}, {"c": ") or evaluation\nmode (", "t": "text"}, {"c": "False", "t": "code-inline"}, {"c": "). Default: ", "t": "text"}, {"c": "True", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "self", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Casts all parameters and buffers to ", "t": "text"}, {"c": "dst_type", "t": "code-inline"}, {"c": ".", "t": "text"}, {"c": "Note", "t": "text"}, {"c": "This method modifies the module in-place.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "dst_type", "t": "text"}, {"c": " (", "t": "text"}, {"c": "type", "t": "text"}, {"c": " or ", "t": "text"}, {"c": "string", "t": "text"}, {"c": ") \u2013 the desired type", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "self", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Move all model parameters and buffers to the XPU.", "t": "text"}, {"c": "This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.", "t": "text"}, {"c": "Note", "t": "text"}, {"c": "This method modifies the module in-place.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "device", "t": "text"}, {"c": " (", "t": "text"}, {"c": "int", "t": "text"}, {"c": ", ", "t": "text"}, {"c": "optional", "t": "text"}, {"c": ") \u2013 if specified, all parameters will be\ncopied to that device", "t": "text"}, {"c": "Returns", "t": "text"}, {"c": "self", "t": "text"}, {"c": "Return type", "t": "text"}, {"c": "Reset gradients of all model parameters.", "t": "text"}, {"c": "See similar function under ", "t": "text"}, {"c": "torch.optim.Optimizer", "t": "code-inline"}, {"c": " for more context.", "t": "text"}, {"c": "Parameters", "t": "text"}, {"c": "set_to_none", "t": "text"}, {"c": " (", "t": "text"}, {"c": "bool", "t": "text"}, {"c": ") \u2013 instead of setting to zero, set the grads to None.\nSee ", "t": "text"}, {"c": "torch.optim.Optimizer.zero_grad()", "t": "code-inline"}, {"c": " for details.", "t": "text"}]]], "ordered": true}}]], "main_html": "<h1>Module<a class=\"headerlink\" href=\"#module\" title=\"Permalink to this heading\">\u00b6</a></h1><dl class=\"py class\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module\"><em class=\"property\"><span class=\"pre\">class</span><span class=\"w\"></span></em><span class=\"sig-prename descclassname\"><span class=\"pre\">torch.nn.</span></span><span class=\"sig-name descname\"><span class=\"pre\">Module</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span><span class=\"n\"><span class=\"pre\">args</span></span></em>, <em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">**</span></span><span class=\"n\"><span class=\"pre\">kwargs</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Base class for all neural network modules.</p><p>Your models should also subclass this class.</p><p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><pre><span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"nn\">nn</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn.functional</span> <span class=\"k\">as</span> <span class=\"nn\">F</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">Model</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv1</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv2</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n        <span class=\"k\">return</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n</pre><dl class=\"py class\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div><p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <a class=\"reference internal\" href=\"#torch.nn.Module.to\" title=\"torch.nn.Module.to\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;to()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">to()</cccode-inline></a>, etc.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>As per the example above, an <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;__init__()&lt;/span&gt;&lt;/code&gt; call to the parent class\nmust be made before assignment on the child.' inline=\"true\">__init__()</cccode-inline> call to the parent class\nmust be made before assignment on the child.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Variables</dt><dd class=\"field-odd\"><p><strong>training</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 Boolean represents whether this module is in training or\nevaluation mode.</p></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.add_module\"><span class=\"sig-name descname\"><span class=\"pre\">add_module</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">name</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">module</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.add_module\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.add_module\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Add a child module to the current module.</p><p>The module can be accessed as an attribute using the given name.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 name of the child module. The child module can be\naccessed from this module using the given name</p></li><li><p><strong>module</strong> (<a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\"><em>Module</em></a>) \u2013 child module to be added to the module.</p></li></ul></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Apply <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;fn&lt;/span&gt;&lt;/code&gt; recursively to every submodule (as returned by ' inline=\"true\">fn</cccode-inline> recursively to every submodule (as returned by <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;.children()&lt;/span&gt;&lt;/code&gt;) as well as self.' inline=\"true\">.children()</cccode-inline>) as well as self.</p><p>Typical use includes initializing the parameters of a model\n(see also <a class=\"reference internal\" href=\"../nn.init.html#nn-init-doc\"><span class=\"std std-ref\">torch.nn.init</span></a>).</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>fn</strong> (<a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;Module&lt;/span&gt;&lt;/code&gt;' inline=\"true\">Module</cccode-inline></a> -&gt; None) \u2013 function to be applied to each submodule</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>self</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"nd\">@torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">()</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">def</span> <span class=\"nf\">init_weights</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">):</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">if</span> <span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">:</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">weight</span><span class=\"o\">.</span><span class=\"n\">fill_</span><span class=\"p\">(</span><span class=\"mf\">1.0</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">weight</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"n\">init_weights</span><span class=\"p\">)</span>\n<span class=\"go\">Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[1., 1.],</span>\n<span class=\"go\">        [1., 1.]], requires_grad=True)</span>\n<span class=\"go\">Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[1., 1.],</span>\n<span class=\"go\">        [1., 1.]], requires_grad=True)</span>\n<span class=\"go\">Sequential(</span>\n<span class=\"go\">  (0): Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">  (1): Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">)</span>\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dd><p>Casts all floating point parameters and buffers to <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;bfloat16&lt;/span&gt;&lt;/code&gt; datatype.' inline=\"true\">bfloat16</cccode-inline> datatype.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Returns</dt><dd class=\"field-odd\"><p>self</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Return an iterator over module buffers.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>recurse</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.</p></dd><dt class=\"field-even\">Yields</dt><dd class=\"field-even\"><p><em>torch.Tensor</em> \u2013 module buffer</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">buf</span> <span class=\"ow\">in</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">buffers</span><span class=\"p\">():</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">buf</span><span class=\"p\">),</span> <span class=\"n\">buf</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">())</span>\n<span class=\"go\">&lt;class 'torch.Tensor'&gt; (20L,)</span>\n<span class=\"go\">&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</span>\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dd><p>Return an iterator over immediate children modules.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Yields</dt><dd class=\"field-odd\"><p><em>Module</em> \u2013 a child module</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.compile\"><span class=\"sig-name descname\"><span class=\"pre\">compile</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span><span class=\"n\"><span class=\"pre\">args</span></span></em>, <em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">**</span></span><span class=\"n\"><span class=\"pre\">kwargs</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.compile\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.compile\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Compile this Module\u2019s forward using <a class=\"reference internal\" href=\"torch.compile.html#torch.compile\" title=\"torch.compile\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.compile()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">torch.compile()</cccode-inline></a>.</p><p>This Module\u2019s __call__ method is compiled and all arguments are passed as-is\nto <a class=\"reference internal\" href=\"torch.compile.html#torch.compile\" title=\"torch.compile\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.compile()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">torch.compile()</cccode-inline></a>.</p><p>See <a class=\"reference internal\" href=\"torch.compile.html#torch.compile\" title=\"torch.compile\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.compile()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">torch.compile()</cccode-inline></a> for details on the arguments for this function.</p></dd></dl><dl class=\"py method\"><dd><p>Move all model parameters and buffers to the CPU.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Returns</dt><dd class=\"field-odd\"><p>self</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Move all model parameters and buffers to the GPU.</p><p>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on GPU while being optimized.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>device</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.13)\"><em>int</em></a><em>, </em><em>optional</em>) \u2013 if specified, all parameters will be\ncopied to that device</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>self</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Casts all floating point parameters and buffers to <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;double&lt;/span&gt;&lt;/code&gt; datatype.' inline=\"true\">double</cccode-inline> datatype.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Returns</dt><dd class=\"field-odd\"><p>self</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Set the module in evaluation mode.</p><p>This has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. <a class=\"reference internal\" href=\"torch.nn.Dropout.html#torch.nn.Dropout\" title=\"torch.nn.Dropout\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;Dropout&lt;/span&gt;&lt;/code&gt;' inline=\"true\">Dropout</cccode-inline></a>, <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;BatchNorm&lt;/span&gt;&lt;/code&gt;,\netc.' inline=\"true\">BatchNorm</cccode-inline>,\netc.</p><p>This is equivalent with <a class=\"reference internal\" href=\"#torch.nn.Module.train\" title=\"torch.nn.Module.train\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;self.train(False)&lt;/span&gt;&lt;/code&gt;' inline=\"true\">self.train(False)</cccode-inline></a>.</p><p>See <a class=\"reference internal\" href=\"../notes/autograd.html#locally-disable-grad-doc\"><span class=\"std std-ref\">Locally disabling gradient computation</span></a> for a comparison between\n.eval() and several similar mechanisms that may be confused with it.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Returns</dt><dd class=\"field-odd\"><p>self</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Set the extra representation of the module.</p><p>To print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Casts all floating point parameters and buffers to <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;float&lt;/span&gt;&lt;/code&gt; datatype.' inline=\"true\">float</cccode-inline> datatype.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Returns</dt><dd class=\"field-odd\"><p>self</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.forward\"><span class=\"sig-name descname\"><span class=\"pre\">forward</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span><span class=\"n\"><span class=\"pre\">input</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#torch.nn.Module.forward\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Define the computation performed at every call.</p><p>Should be overridden by all subclasses.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;Module&lt;/span&gt;&lt;/code&gt;' inline=\"true\">Module</cccode-inline></a> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p></div><dl class=\"field-list simple\"></dl></dd></dl><dl class=\"py method\"><dd><p>Return the buffer given by <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt; if it exists, otherwise throw an error.' inline=\"true\">target</cccode-inline> if it exists, otherwise throw an error.</p><p>See the docstring for <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_submodule&lt;/span&gt;&lt;/code&gt; for a more detailed\nexplanation of this method\u2019s functionality as well as how to\ncorrectly specify ' inline=\"true\">get_submodule</cccode-inline> for a more detailed\nexplanation of this method\u2019s functionality as well as how to\ncorrectly specify <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">target</cccode-inline>.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>target</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 The fully-qualified string name of the buffer\nto look for. (See <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_submodule&lt;/span&gt;&lt;/code&gt; for how to specify a\nfully-qualified string.)' inline=\"true\">get_submodule</cccode-inline> for how to specify a\nfully-qualified string.)</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>The buffer referenced by <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt;' inline=\"true\">target</cccode-inline></p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd><dt class=\"field-even\">Raises</dt><dd class=\"field-even\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#AttributeError\" title=\"(in Python v3.13)\"><strong>AttributeError</strong></a> \u2013 If the target string references an invalid\n    path or resolves to something that is not a\n    buffer</p></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Return any extra state to include in the module\u2019s state_dict.</p><p>Implement this and a corresponding <a class=\"reference internal\" href=\"#torch.nn.Module.set_extra_state\" title=\"torch.nn.Module.set_extra_state\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;set_extra_state()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">set_extra_state()</cccode-inline></a> for your module\nif you need to store extra state. This function is called when building the\nmodule\u2019s state_dict().</p><p>Note that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Returns</dt><dd class=\"field-odd\"><p>Any extra state to store in the module\u2019s state_dict</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Return the parameter given by <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt; if it exists, otherwise throw an error.' inline=\"true\">target</cccode-inline> if it exists, otherwise throw an error.</p><p>See the docstring for <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_submodule&lt;/span&gt;&lt;/code&gt; for a more detailed\nexplanation of this method\u2019s functionality as well as how to\ncorrectly specify ' inline=\"true\">get_submodule</cccode-inline> for a more detailed\nexplanation of this method\u2019s functionality as well as how to\ncorrectly specify <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">target</cccode-inline>.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>target</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 The fully-qualified string name of the Parameter\nto look for. (See <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_submodule&lt;/span&gt;&lt;/code&gt; for how to specify a\nfully-qualified string.)' inline=\"true\">get_submodule</cccode-inline> for how to specify a\nfully-qualified string.)</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>The Parameter referenced by <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt;' inline=\"true\">target</cccode-inline></p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"><p>torch.nn.Parameter</p></dd><dt class=\"field-even\">Raises</dt><dd class=\"field-even\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#AttributeError\" title=\"(in Python v3.13)\"><strong>AttributeError</strong></a> \u2013 If the target string references an invalid\n    path or resolves to something that is not an\n    <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;nn.Parameter&lt;/span&gt;&lt;/code&gt;' inline=\"true\">nn.Parameter</cccode-inline></p></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Return the submodule given by <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt; if it exists, otherwise throw an error.' inline=\"true\">target</cccode-inline> if it exists, otherwise throw an error.</p><p>For example, let\u2019s say you have an <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;nn.Module&lt;/span&gt;&lt;/code&gt; ' inline=\"true\">nn.Module</cccode-inline><cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;A&lt;/span&gt;&lt;/code&gt; that\nlooks like this:' inline=\"true\">A</cccode-inline> that\nlooks like this:</p><div class=\"highlight-text notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre>A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-text notranslate\"><div class=\"highlight\"></div></div><p>(The diagram shows an <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;nn.Module&lt;/span&gt;&lt;/code&gt; ' inline=\"true\">nn.Module</cccode-inline><cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;A&lt;/span&gt;&lt;/code&gt;. ' inline=\"true\">A</cccode-inline>. <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;A&lt;/span&gt;&lt;/code&gt; has a nested\nsubmodule ' inline=\"true\">A</cccode-inline> has a nested\nsubmodule <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;net_b&lt;/span&gt;&lt;/code&gt;, which itself has two submodules ' inline=\"true\">net_b</cccode-inline>, which itself has two submodules <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;net_c&lt;/span&gt;&lt;/code&gt;\nand ' inline=\"true\">net_c</cccode-inline>\nand <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;linear&lt;/span&gt;&lt;/code&gt;. ' inline=\"true\">linear</cccode-inline>. <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;net_c&lt;/span&gt;&lt;/code&gt; then has a submodule ' inline=\"true\">net_c</cccode-inline> then has a submodule <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;conv&lt;/span&gt;&lt;/code&gt;.)' inline=\"true\">conv</cccode-inline>.)</p><p>To check whether or not we have the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;linear&lt;/span&gt;&lt;/code&gt; submodule, we\nwould call ' inline=\"true\">linear</cccode-inline> submodule, we\nwould call <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_submodule(\"net_b.linear\")&lt;/span&gt;&lt;/code&gt;. To check whether\nwe have the ' inline=\"true\">get_submodule(\"net_b.linear\")</cccode-inline>. To check whether\nwe have the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;conv&lt;/span&gt;&lt;/code&gt; submodule, we would call\n' inline=\"true\">conv</cccode-inline> submodule, we would call\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_submodule(\"net_b.net_c.conv\")&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">get_submodule(\"net_b.net_c.conv\")</cccode-inline>.</p><p>The runtime of <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_submodule&lt;/span&gt;&lt;/code&gt; is bounded by the degree\nof module nesting in ' inline=\"true\">get_submodule</cccode-inline> is bounded by the degree\nof module nesting in <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt;. A query against\n' inline=\"true\">target</cccode-inline>. A query against\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;named_modules&lt;/span&gt;&lt;/code&gt; achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ' inline=\"true\">named_modules</cccode-inline> achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_submodule&lt;/span&gt;&lt;/code&gt; should always be\nused.' inline=\"true\">get_submodule</cccode-inline> should always be\nused.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>target</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>The submodule referenced by <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt;' inline=\"true\">target</cccode-inline></p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd><dt class=\"field-even\">Raises</dt><dd class=\"field-even\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#AttributeError\" title=\"(in Python v3.13)\"><strong>AttributeError</strong></a> \u2013 If the target string references an invalid\n    path or resolves to something that is not an\n    <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;nn.Module&lt;/span&gt;&lt;/code&gt;' inline=\"true\">nn.Module</cccode-inline></p></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Casts all floating point parameters and buffers to <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;half&lt;/span&gt;&lt;/code&gt; datatype.' inline=\"true\">half</cccode-inline> datatype.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Returns</dt><dd class=\"field-odd\"><p>self</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Move all model parameters and buffers to the IPU.</p><p>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on IPU while being optimized.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>device</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.13)\"><em>int</em></a><em>, </em><em>optional</em>) \u2013 if specified, all parameters will be\ncopied to that device</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>self</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.load_state_dict\"><span class=\"sig-name descname\"><span class=\"pre\">load_state_dict</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">state_dict</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">strict</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">assign</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.load_state_dict\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.load_state_dict\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Copy parameters and buffers from <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict</cccode-inline></a> into this module and its descendants.</p><p>If <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;strict&lt;/span&gt;&lt;/code&gt; is ' inline=\"true\">strict</cccode-inline> is <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;, then\nthe keys of ' inline=\"true\">True</cccode-inline>, then\nthe keys of <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict</cccode-inline></a> must exactly match the keys returned\nby this module\u2019s <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict()</cccode-inline></a> function.</p><div class=\"admonition warning\"><p class=\"admonition-title\">Warning</p><p>If <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;assign&lt;/span&gt;&lt;/code&gt; is ' inline=\"true\">assign</cccode-inline> is <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt; the optimizer must be created after\nthe call to ' inline=\"true\">True</cccode-inline> the optimizer must be created after\nthe call to <a class=\"reference internal\" href=\"#torch.nn.Module.load_state_dict\" title=\"torch.nn.Module.load_state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;load_state_dict&lt;/span&gt;&lt;/code&gt;' inline=\"true\">load_state_dict</cccode-inline></a> unless\n<a class=\"reference internal\" href=\"../future_mod.html#torch.__future__.get_swap_module_params_on_conversion\" title=\"torch.__future__.get_swap_module_params_on_conversion\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_swap_module_params_on_conversion()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">get_swap_module_params_on_conversion()</cccode-inline></a> is <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">True</cccode-inline>.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>state_dict</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.13)\"><em>dict</em></a>) \u2013 a dict containing parameters and\npersistent buffers.</p></li><li><p><strong>strict</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a><em>, </em><em>optional</em>) \u2013 whether to strictly enforce that the keys\nin <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict</cccode-inline></a> match the keys returned by this module\u2019s\n<a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict()</cccode-inline></a> function. Default: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;' inline=\"true\">True</cccode-inline></p></li><li><p><strong>assign</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a><em>, </em><em>optional</em>) \u2013 When <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt;, the properties of the tensors\nin the current module are preserved while when ' inline=\"true\">False</cccode-inline>, the properties of the tensors\nin the current module are preserved while when <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;, the\nproperties of the Tensors in the state dict are preserved. The only\nexception is the ' inline=\"true\">True</cccode-inline>, the\nproperties of the Tensors in the state dict are preserved. The only\nexception is the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;requires_grad&lt;/span&gt;&lt;/code&gt; field of ' inline=\"true\">requires_grad</cccode-inline> field of <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;\n&lt;span class=\"pre\"&gt;Default:&lt;/span&gt; &lt;span class=\"pre\"&gt;``False`&lt;/span&gt;&lt;/code&gt;' inline=\"true\">Default: ``False`</cccode-inline></p></li></ul></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><ul class=\"simple\"><li><dl class=\"simple\"><dt><strong>missing_keys</strong> is a list of str containing any keys that are expected</dt><dd><p>by this module but missing from the provided <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">state_dict</cccode-inline>.</p></dd></dl></li><li><dl class=\"simple\"><dt><strong>unexpected_keys</strong> is a list of str containing the keys that are not</dt><dd><p>expected by this module but present in the provided <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">state_dict</cccode-inline>.</p></dd></dl></li></ul></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"><p><cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;NamedTuple&lt;/span&gt;&lt;/code&gt; with ' inline=\"true\">NamedTuple</cccode-inline> with <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;missing_keys&lt;/span&gt;&lt;/code&gt; and ' inline=\"true\">missing_keys</cccode-inline> and <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;unexpected_keys&lt;/span&gt;&lt;/code&gt; fields' inline=\"true\">unexpected_keys</cccode-inline> fields</p></dd></dl><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>If a parameter or buffer is registered as <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;None&lt;/span&gt;&lt;/code&gt; and its corresponding key\nexists in ' inline=\"true\">None</cccode-inline> and its corresponding key\nexists in <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict</cccode-inline></a>, <a class=\"reference internal\" href=\"#torch.nn.Module.load_state_dict\" title=\"torch.nn.Module.load_state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;load_state_dict()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">load_state_dict()</cccode-inline></a> will raise a\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;RuntimeError&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">RuntimeError</cccode-inline>.</p></div></dd></dl><dl class=\"py method\"><dd><p>Return an iterator over all modules in the network.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Yields</dt><dd class=\"field-odd\"><p><em>Module</em> \u2013 a module in the network</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"></dd></dl><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>Duplicate modules are returned only once. In the following\nexample, <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;l&lt;/span&gt;&lt;/code&gt; will be returned only once.' inline=\"true\">l</cccode-inline> will be returned only once.</p></div><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">l</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">,</span> <span class=\"n\">l</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"n\">m</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"p\">()):</span>\n<span class=\"gp\">... </span>    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"s1\">'-&gt;'</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">)</span>\n\n<span class=\"go\">0 -&gt; Sequential(</span>\n<span class=\"go\">  (0): Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">  (1): Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">)</span>\n<span class=\"go\">1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dd><p>Move all model parameters and buffers to the MTIA.</p><p>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on MTIA while being optimized.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>device</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.13)\"><em>int</em></a><em>, </em><em>optional</em>) \u2013 if specified, all parameters will be\ncopied to that device</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>self</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.named_buffers\"><span class=\"sig-name descname\"><span class=\"pre\">named_buffers</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prefix</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">recurse</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">remove_duplicate</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.named_buffers\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.named_buffers\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>prefix</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 prefix to prepend to all buffer names.</p></li><li><p><strong>recurse</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a><em>, </em><em>optional</em>) \u2013 if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.</p></li><li><p><strong>remove_duplicate</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a><em>, </em><em>optional</em>) \u2013 whether to remove the duplicated buffers in the result. Defaults to True.</p></li></ul></dd><dt class=\"field-even\">Yields</dt><dd class=\"field-even\"><p><em>(str, torch.Tensor)</em> \u2013 Tuple containing the name and buffer</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Iterator\" title=\"(in Python v3.13)\"><em>Iterator</em></a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Tuple\" title=\"(in Python v3.13)\"><em>Tuple</em></a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\">str</a>, <a class=\"reference internal\" href=\"../tensors.html#torch.Tensor\" title=\"torch.Tensor\"><em>Tensor</em></a>]]</p></dd></dl><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">buf</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">named_buffers</span><span class=\"p\">():</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">if</span> <span class=\"n\">name</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">'running_var'</span><span class=\"p\">]:</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">buf</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">())</span>\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dd><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Yields</dt><dd class=\"field-odd\"><p><em>(str, Module)</em> \u2013 Tuple containing a name and child module</p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Iterator\" title=\"(in Python v3.13)\"><em>Iterator</em></a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Tuple\" title=\"(in Python v3.13)\"><em>Tuple</em></a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\">str</a>, <a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.modules.module.Module\"><em>Module</em></a>]]</p></dd></dl><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">module</span> <span class=\"ow\">in</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">named_children</span><span class=\"p\">():</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">if</span> <span class=\"n\">name</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">'conv4'</span><span class=\"p\">,</span> <span class=\"s1\">'conv5'</span><span class=\"p\">]:</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">)</span>\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.named_modules\"><span class=\"sig-name descname\"><span class=\"pre\">named_modules</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">memo</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prefix</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">remove_duplicate</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.named_modules\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.named_modules\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>memo</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Optional\" title=\"(in Python v3.13)\"><em>Optional</em></a><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Set\" title=\"(in Python v3.13)\"><em>Set</em></a><em>[</em><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.modules.module.Module\"><em>Module</em></a><em>]</em><em>]</em>) \u2013 a memo to store the set of modules already added to the result</p></li><li><p><strong>prefix</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 a prefix that will be added to the name of the module</p></li><li><p><strong>remove_duplicate</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 whether to remove the duplicated module instances in the result\nor not</p></li></ul></dd><dt class=\"field-even\">Yields</dt><dd class=\"field-even\"><p><em>(str, Module)</em> \u2013 Tuple of name and module</p></dd></dl><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>Duplicate modules are returned only once. In the following\nexample, <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;l&lt;/span&gt;&lt;/code&gt; will be returned only once.' inline=\"true\">l</cccode-inline> will be returned only once.</p></div><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">l</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">,</span> <span class=\"n\">l</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"n\">m</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">named_modules</span><span class=\"p\">()):</span>\n<span class=\"gp\">... </span>    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"s1\">'-&gt;'</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">)</span>\n\n<span class=\"go\">0 -&gt; ('', Sequential(</span>\n<span class=\"go\">  (0): Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">  (1): Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">))</span>\n<span class=\"go\">1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</span>\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.named_parameters\"><span class=\"sig-name descname\"><span class=\"pre\">named_parameters</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prefix</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">recurse</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">remove_duplicate</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.named_parameters\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.named_parameters\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>prefix</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 prefix to prepend to all parameter names.</p></li><li><p><strong>recurse</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.</p></li><li><p><strong>remove_duplicate</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a><em>, </em><em>optional</em>) \u2013 whether to remove the duplicated\nparameters in the result. Defaults to True.</p></li></ul></dd><dt class=\"field-even\">Yields</dt><dd class=\"field-even\"><p><em>(str, Parameter)</em> \u2013 Tuple containing the name and parameter</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Iterator\" title=\"(in Python v3.13)\"><em>Iterator</em></a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Tuple\" title=\"(in Python v3.13)\"><em>Tuple</em></a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\">str</a>, <a class=\"reference internal\" href=\"torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter\" title=\"torch.nn.parameter.Parameter\"><em>Parameter</em></a>]]</p></dd></dl><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">param</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">named_parameters</span><span class=\"p\">():</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">if</span> <span class=\"n\">name</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">'bias'</span><span class=\"p\">]:</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">param</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">())</span>\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dd><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>recurse</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.</p></dd><dt class=\"field-even\">Yields</dt><dd class=\"field-even\"><p><em>Parameter</em> \u2013 module parameter</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">param</span> <span class=\"ow\">in</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">():</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">param</span><span class=\"p\">),</span> <span class=\"n\">param</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">())</span>\n<span class=\"go\">&lt;class 'torch.Tensor'&gt; (20L,)</span>\n<span class=\"go\">&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)</span>\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dd><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <a class=\"reference internal\" href=\"#torch.nn.Module.register_full_backward_hook\" title=\"torch.nn.Module.register_full_backward_hook\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;register_full_backward_hook()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">register_full_backward_hook()</cccode-inline></a> and\nthe behavior of this function will change in future versions.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Returns</dt><dd class=\"field-odd\"><p>a handle that can be used to remove the added hook by calling\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;handle.remove()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">handle.remove()</cccode-inline></p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"><p></p></dd></dl></dd></dl></dd></dl><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.utils.hooks.RemovableHandle</span></code><dl class=\"py class\"><dd><dl class=\"py method\"><dd><dl class=\"field-list simple\"><dd class=\"field-even\"><p></p></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_buffer\"><span class=\"sig-name descname\"><span class=\"pre\">register_buffer</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">name</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">tensor</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">persistent</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_buffer\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_buffer\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm\u2019s <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;running_mean&lt;/span&gt;&lt;/code&gt;\nis not a parameter, but is part of the module\u2019s state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting ' inline=\"true\">running_mean</cccode-inline>\nis not a parameter, but is part of the module\u2019s state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;persistent&lt;/span&gt;&lt;/code&gt; to ' inline=\"true\">persistent</cccode-inline> to <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt;. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module\u2019s\n' inline=\"true\">False</cccode-inline>. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module\u2019s\n<a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict</cccode-inline></a>.</p><p>Buffers can be accessed as attributes using given names.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 name of the buffer. The buffer can be accessed\nfrom this module using the given name</p></li><li><p><strong>tensor</strong> (<a class=\"reference internal\" href=\"../tensors.html#torch.Tensor\" title=\"torch.Tensor\"><em>Tensor</em></a><em> or </em><em>None</em>) \u2013 buffer to be registered. If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;None&lt;/span&gt;&lt;/code&gt;, then operations\nthat run on buffers, such as ' inline=\"true\">None</cccode-inline>, then operations\nthat run on buffers, such as <a class=\"reference internal\" href=\"#torch.nn.Module.cuda\" title=\"torch.nn.Module.cuda\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;cuda&lt;/span&gt;&lt;/code&gt;' inline=\"true\">cuda</cccode-inline></a>, are ignored. If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;None&lt;/span&gt;&lt;/code&gt;,\nthe buffer is ' inline=\"true\">None</cccode-inline>,\nthe buffer is <strong>not</strong> included in the module\u2019s <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict</cccode-inline></a>.</p></li><li><p><strong>persistent</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 whether the buffer is part of this module\u2019s\n<a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict</cccode-inline></a>.</p></li></ul></dd></dl><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">register_buffer</span><span class=\"p\">(</span><span class=\"s1\">'running_mean'</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">num_features</span><span class=\"p\">))</span>\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_forward_hook\"><span class=\"sig-name descname\"><span class=\"pre\">register_forward_hook</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">hook</span></span></em>, <em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prepend</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">with_kwargs</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">always_call</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_forward_hook\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_forward_hook\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Register a forward hook on the module.</p><p>The hook will be called every time after <a class=\"reference internal\" href=\"#torch.nn.Module.forward\" title=\"torch.nn.Module.forward\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">forward()</cccode-inline></a> has computed an output.</p><p>If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;with_kwargs&lt;/span&gt;&lt;/code&gt; is ' inline=\"true\">with_kwargs</cccode-inline> is <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt; or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won\u2019t be\npassed to the hooks and only to the ' inline=\"true\">False</cccode-inline> or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won\u2019t be\npassed to the hooks and only to the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward&lt;/span&gt;&lt;/code&gt;. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after ' inline=\"true\">forward</cccode-inline>. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after <a class=\"reference internal\" href=\"#torch.nn.Module.forward\" title=\"torch.nn.Module.forward\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">forward()</cccode-inline></a> is called. The hook\nshould have the following signature:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre><span class=\"n\">hook</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span> <span class=\"ow\">or</span> <span class=\"n\">modified</span> <span class=\"n\">output</span>\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div><p>If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;with_kwargs&lt;/span&gt;&lt;/code&gt; is ' inline=\"true\">with_kwargs</cccode-inline> is <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;, the forward hook will be passed the\n' inline=\"true\">True</cccode-inline>, the forward hook will be passed the\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;kwargs&lt;/span&gt;&lt;/code&gt; given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature:' inline=\"true\">kwargs</cccode-inline> given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre><span class=\"n\">hook</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">kwargs</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span> <span class=\"ow\">or</span> <span class=\"n\">modified</span> <span class=\"n\">output</span>\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>hook</strong> (<em>Callable</em>) \u2013 The user defined hook to be registered.</p></li><li><p><strong>prepend</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;, the provided ' inline=\"true\">True</cccode-inline>, the provided <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be fired\nbefore all existing ' inline=\"true\">hook</cccode-inline> will be fired\nbefore all existing <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward&lt;/span&gt;&lt;/code&gt; hooks on this\n' inline=\"true\">forward</cccode-inline> hooks on this\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.nn.modules.Module&lt;/span&gt;&lt;/code&gt;. Otherwise, the provided\n' inline=\"true\">torch.nn.modules.Module</cccode-inline>. Otherwise, the provided\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be fired after all existing ' inline=\"true\">hook</cccode-inline> will be fired after all existing <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward&lt;/span&gt;&lt;/code&gt; hooks on\nthis ' inline=\"true\">forward</cccode-inline> hooks on\nthis <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.nn.modules.Module&lt;/span&gt;&lt;/code&gt;. Note that global\n' inline=\"true\">torch.nn.modules.Module</cccode-inline>. Note that global\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward&lt;/span&gt;&lt;/code&gt; hooks registered with\n' inline=\"true\">forward</cccode-inline> hooks registered with\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;register_module_forward_hook()&lt;/span&gt;&lt;/code&gt; will fire before all hooks\nregistered by this method.\nDefault: ' inline=\"true\">register_module_forward_hook()</cccode-inline> will fire before all hooks\nregistered by this method.\nDefault: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt;' inline=\"true\">False</cccode-inline></p></li><li><p><strong>with_kwargs</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;, the ' inline=\"true\">True</cccode-inline>, the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be passed the\nkwargs given to the forward function.\nDefault: ' inline=\"true\">hook</cccode-inline> will be passed the\nkwargs given to the forward function.\nDefault: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt;' inline=\"true\">False</cccode-inline></p></li><li><p><strong>always_call</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt; the ' inline=\"true\">True</cccode-inline> the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: ' inline=\"true\">hook</cccode-inline> will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt;' inline=\"true\">False</cccode-inline></p></li></ul></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>a handle that can be used to remove the added hook by calling\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;handle.remove()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">handle.remove()</cccode-inline></p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"><p></p></dd></dl></dd></dl></dd></dl><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.utils.hooks.RemovableHandle</span></code><dl class=\"py class\"><dd><dl class=\"py method\"><dd><dl class=\"field-list simple\"><dd class=\"field-odd\"><p></p></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_forward_pre_hook\"><span class=\"sig-name descname\"><span class=\"pre\">register_forward_pre_hook</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">hook</span></span></em>, <em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prepend</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">with_kwargs</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_forward_pre_hook\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_forward_pre_hook\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <a class=\"reference internal\" href=\"#torch.nn.Module.forward\" title=\"torch.nn.Module.forward\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">forward()</cccode-inline></a> is invoked.</p><p>If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;with_kwargs&lt;/span&gt;&lt;/code&gt; is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won\u2019t be\npassed to the hooks and only to the ' inline=\"true\">with_kwargs</cccode-inline> is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won\u2019t be\npassed to the hooks and only to the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward&lt;/span&gt;&lt;/code&gt;. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature:' inline=\"true\">forward</cccode-inline>. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre><span class=\"n\">hook</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span> <span class=\"ow\">or</span> <span class=\"n\">modified</span> <span class=\"nb\">input</span>\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div><p>If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;with_kwargs&lt;/span&gt;&lt;/code&gt; is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature:' inline=\"true\">with_kwargs</cccode-inline> is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre><span class=\"n\">hook</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">kwargs</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span> <span class=\"ow\">or</span> <span class=\"n\">a</span> <span class=\"nb\">tuple</span> <span class=\"n\">of</span> <span class=\"n\">modified</span> <span class=\"nb\">input</span> <span class=\"ow\">and</span> <span class=\"n\">kwargs</span>\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>hook</strong> (<em>Callable</em>) \u2013 The user defined hook to be registered.</p></li><li><p><strong>prepend</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If true, the provided <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be fired before\nall existing ' inline=\"true\">hook</cccode-inline> will be fired before\nall existing <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward_pre&lt;/span&gt;&lt;/code&gt; hooks on this\n' inline=\"true\">forward_pre</cccode-inline> hooks on this\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.nn.modules.Module&lt;/span&gt;&lt;/code&gt;. Otherwise, the provided\n' inline=\"true\">torch.nn.modules.Module</cccode-inline>. Otherwise, the provided\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be fired after all existing ' inline=\"true\">hook</cccode-inline> will be fired after all existing <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward_pre&lt;/span&gt;&lt;/code&gt; hooks\non this ' inline=\"true\">forward_pre</cccode-inline> hooks\non this <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.nn.modules.Module&lt;/span&gt;&lt;/code&gt;. Note that global\n' inline=\"true\">torch.nn.modules.Module</cccode-inline>. Note that global\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;forward_pre&lt;/span&gt;&lt;/code&gt; hooks registered with\n' inline=\"true\">forward_pre</cccode-inline> hooks registered with\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;register_module_forward_pre_hook()&lt;/span&gt;&lt;/code&gt; will fire before all\nhooks registered by this method.\nDefault: ' inline=\"true\">register_module_forward_pre_hook()</cccode-inline> will fire before all\nhooks registered by this method.\nDefault: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt;' inline=\"true\">False</cccode-inline></p></li><li><p><strong>with_kwargs</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If true, the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be passed the kwargs\ngiven to the forward function.\nDefault: ' inline=\"true\">hook</cccode-inline> will be passed the kwargs\ngiven to the forward function.\nDefault: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt;' inline=\"true\">False</cccode-inline></p></li></ul></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>a handle that can be used to remove the added hook by calling\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;handle.remove()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">handle.remove()</cccode-inline></p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"><p></p></dd></dl></dd></dl></dd></dl><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.utils.hooks.RemovableHandle</span></code><dl class=\"py class\"><dd><dl class=\"py method\"><dd><dl class=\"field-list simple\"><dd class=\"field-odd\"><p></p></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_full_backward_hook\"><span class=\"sig-name descname\"><span class=\"pre\">register_full_backward_hook</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">hook</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prepend</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_full_backward_hook\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_full_backward_hook\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre><span class=\"n\">hook</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">grad_input</span><span class=\"p\">,</span> <span class=\"n\">grad_output</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">tuple</span><span class=\"p\">(</span><span class=\"n\">Tensor</span><span class=\"p\">)</span> <span class=\"ow\">or</span> <span class=\"kc\">None</span>\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div><p>The <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;grad_input&lt;/span&gt;&lt;/code&gt; and ' inline=\"true\">grad_input</cccode-inline> and <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;grad_output&lt;/span&gt;&lt;/code&gt; are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of ' inline=\"true\">grad_output</cccode-inline> are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;grad_input&lt;/span&gt;&lt;/code&gt; in\nsubsequent computations. ' inline=\"true\">grad_input</cccode-inline> in\nsubsequent computations. <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;grad_input&lt;/span&gt;&lt;/code&gt; will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin ' inline=\"true\">grad_input</cccode-inline> will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;grad_input&lt;/span&gt;&lt;/code&gt; and ' inline=\"true\">grad_input</cccode-inline> and <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;grad_output&lt;/span&gt;&lt;/code&gt; will be ' inline=\"true\">grad_output</cccode-inline> will be <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;None&lt;/span&gt;&lt;/code&gt; for all non-Tensor\narguments.' inline=\"true\">None</cccode-inline> for all non-Tensor\narguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module\u2019s forward function.</p><div class=\"admonition warning\"><p class=\"admonition-title\">Warning</p><p>Modifying inputs or outputs inplace is not allowed when using backward hooks and\nwill raise an error.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>hook</strong> (<em>Callable</em>) \u2013 The user-defined hook to be registered.</p></li><li><p><strong>prepend</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If true, the provided <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be fired before\nall existing ' inline=\"true\">hook</cccode-inline> will be fired before\nall existing <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;backward&lt;/span&gt;&lt;/code&gt; hooks on this\n' inline=\"true\">backward</cccode-inline> hooks on this\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.nn.modules.Module&lt;/span&gt;&lt;/code&gt;. Otherwise, the provided\n' inline=\"true\">torch.nn.modules.Module</cccode-inline>. Otherwise, the provided\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be fired after all existing ' inline=\"true\">hook</cccode-inline> will be fired after all existing <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;backward&lt;/span&gt;&lt;/code&gt; hooks on\nthis ' inline=\"true\">backward</cccode-inline> hooks on\nthis <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.nn.modules.Module&lt;/span&gt;&lt;/code&gt;. Note that global\n' inline=\"true\">torch.nn.modules.Module</cccode-inline>. Note that global\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;backward&lt;/span&gt;&lt;/code&gt; hooks registered with\n' inline=\"true\">backward</cccode-inline> hooks registered with\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;register_module_full_backward_hook()&lt;/span&gt;&lt;/code&gt; will fire before\nall hooks registered by this method.' inline=\"true\">register_module_full_backward_hook()</cccode-inline> will fire before\nall hooks registered by this method.</p></li></ul></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>a handle that can be used to remove the added hook by calling\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;handle.remove()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">handle.remove()</cccode-inline></p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"><p></p></dd></dl></dd></dl></dd></dl><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.utils.hooks.RemovableHandle</span></code><dl class=\"py class\"><dd><dl class=\"py method\"><dd><dl class=\"field-list simple\"><dd class=\"field-odd\"><p></p></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_full_backward_pre_hook\"><span class=\"sig-name descname\"><span class=\"pre\">register_full_backward_pre_hook</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">hook</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prepend</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_full_backward_pre_hook\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_full_backward_pre_hook\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre><span class=\"n\">hook</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">grad_output</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"ow\">or</span> <span class=\"kc\">None</span>\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div><p>The <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;grad_output&lt;/span&gt;&lt;/code&gt; is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of ' inline=\"true\">grad_output</cccode-inline> is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;grad_output&lt;/span&gt;&lt;/code&gt; in\nsubsequent computations. Entries in ' inline=\"true\">grad_output</cccode-inline> in\nsubsequent computations. Entries in <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;grad_output&lt;/span&gt;&lt;/code&gt; will be ' inline=\"true\">grad_output</cccode-inline> will be <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;None&lt;/span&gt;&lt;/code&gt; for\nall non-Tensor arguments.' inline=\"true\">None</cccode-inline> for\nall non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module\u2019s forward function.</p><div class=\"admonition warning\"><p class=\"admonition-title\">Warning</p><p>Modifying inputs inplace is not allowed when using backward hooks and\nwill raise an error.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>hook</strong> (<em>Callable</em>) \u2013 The user-defined hook to be registered.</p></li><li><p><strong>prepend</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If true, the provided <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be fired before\nall existing ' inline=\"true\">hook</cccode-inline> will be fired before\nall existing <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;backward_pre&lt;/span&gt;&lt;/code&gt; hooks on this\n' inline=\"true\">backward_pre</cccode-inline> hooks on this\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.nn.modules.Module&lt;/span&gt;&lt;/code&gt;. Otherwise, the provided\n' inline=\"true\">torch.nn.modules.Module</cccode-inline>. Otherwise, the provided\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;hook&lt;/span&gt;&lt;/code&gt; will be fired after all existing ' inline=\"true\">hook</cccode-inline> will be fired after all existing <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;backward_pre&lt;/span&gt;&lt;/code&gt; hooks\non this ' inline=\"true\">backward_pre</cccode-inline> hooks\non this <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.nn.modules.Module&lt;/span&gt;&lt;/code&gt;. Note that global\n' inline=\"true\">torch.nn.modules.Module</cccode-inline>. Note that global\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;backward_pre&lt;/span&gt;&lt;/code&gt; hooks registered with\n' inline=\"true\">backward_pre</cccode-inline> hooks registered with\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;register_module_full_backward_pre_hook()&lt;/span&gt;&lt;/code&gt; will fire before\nall hooks registered by this method.' inline=\"true\">register_module_full_backward_pre_hook()</cccode-inline> will fire before\nall hooks registered by this method.</p></li></ul></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>a handle that can be used to remove the added hook by calling\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;handle.remove()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">handle.remove()</cccode-inline></p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"><p></p></dd></dl></dd></dl></dd></dl><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.utils.hooks.RemovableHandle</span></code><dl class=\"py class\"><dd><dl class=\"py method\"><dd><dl class=\"field-list simple\"><dd class=\"field-odd\"><p></p></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Register a post-hook to be run after module\u2019s <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;load_state_dict()&lt;/span&gt;&lt;/code&gt; is called.' inline=\"true\">load_state_dict()</cccode-inline> is called.</p><dl class=\"simple\"><dt>It should have the following signature::</dt><dd><p>hook(module, incompatible_keys) -&gt; None</p></dd></dl><p>The <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;module&lt;/span&gt;&lt;/code&gt; argument is the current module that this hook is registered\non, and the ' inline=\"true\">module</cccode-inline> argument is the current module that this hook is registered\non, and the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;incompatible_keys&lt;/span&gt;&lt;/code&gt; argument is a ' inline=\"true\">incompatible_keys</cccode-inline> argument is a <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;NamedTuple&lt;/span&gt;&lt;/code&gt; consisting\nof attributes ' inline=\"true\">NamedTuple</cccode-inline> consisting\nof attributes <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;missing_keys&lt;/span&gt;&lt;/code&gt; and ' inline=\"true\">missing_keys</cccode-inline> and <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;unexpected_keys&lt;/span&gt;&lt;/code&gt;. ' inline=\"true\">unexpected_keys</cccode-inline>. <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;missing_keys&lt;/span&gt;&lt;/code&gt;\nis a ' inline=\"true\">missing_keys</cccode-inline>\nis a <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;list&lt;/span&gt;&lt;/code&gt; of ' inline=\"true\">list</cccode-inline> of <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;str&lt;/span&gt;&lt;/code&gt; containing the missing keys and\n' inline=\"true\">str</cccode-inline> containing the missing keys and\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;unexpected_keys&lt;/span&gt;&lt;/code&gt; is a ' inline=\"true\">unexpected_keys</cccode-inline> is a <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;list&lt;/span&gt;&lt;/code&gt; of ' inline=\"true\">list</cccode-inline> of <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;str&lt;/span&gt;&lt;/code&gt; containing the unexpected keys.' inline=\"true\">str</cccode-inline> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <a class=\"reference internal\" href=\"#torch.nn.Module.load_state_dict\" title=\"torch.nn.Module.load_state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;load_state_dict()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">load_state_dict()</cccode-inline></a> with\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;strict=True&lt;/span&gt;&lt;/code&gt; are affected by modifications the hook makes to\n' inline=\"true\">strict=True</cccode-inline> are affected by modifications the hook makes to\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;missing_keys&lt;/span&gt;&lt;/code&gt; or ' inline=\"true\">missing_keys</cccode-inline> or <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;unexpected_keys&lt;/span&gt;&lt;/code&gt;, as expected. Additions to either\nset of keys will result in an error being thrown when ' inline=\"true\">unexpected_keys</cccode-inline>, as expected. Additions to either\nset of keys will result in an error being thrown when <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;strict=True&lt;/span&gt;&lt;/code&gt;, and\nclearing out both missing and unexpected keys will avoid an error.' inline=\"true\">strict=True</cccode-inline>, and\nclearing out both missing and unexpected keys will avoid an error.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Returns</dt><dd class=\"field-odd\"><p>a handle that can be used to remove the added hook by calling\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;handle.remove()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">handle.remove()</cccode-inline></p></dd><dt class=\"field-even\">Return type</dt><dd class=\"field-even\"><p></p></dd></dl></dd></dl></dd></dl><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.utils.hooks.RemovableHandle</span></code><dl class=\"py class\"><dd><dl class=\"py method\"><dd><dl class=\"field-list simple\"><dd class=\"field-even\"><p></p></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Register a pre-hook to be run before module\u2019s <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;load_state_dict()&lt;/span&gt;&lt;/code&gt; is called.' inline=\"true\">load_state_dict()</cccode-inline> is called.</p><dl class=\"simple\"><dt>It should have the following signature::</dt><dd><p>hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950</p></dd></dl><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>hook</strong> (<em>Callable</em>) \u2013 Callable hook that will be invoked before\nloading the state dict.</p></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_module\"><span class=\"sig-name descname\"><span class=\"pre\">register_module</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">name</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">module</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_module\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_module\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Alias for <a class=\"reference internal\" href=\"#torch.nn.Module.add_module\" title=\"torch.nn.Module.add_module\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;add_module()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">add_module()</cccode-inline></a>.</p><dl class=\"field-list simple\"></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_parameter\"><span class=\"sig-name descname\"><span class=\"pre\">register_parameter</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">name</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">param</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_parameter\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_parameter\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 name of the parameter. The parameter can be accessed\nfrom this module using the given name</p></li><li><p><strong>param</strong> (<a class=\"reference internal\" href=\"torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter\" title=\"torch.nn.parameter.Parameter\"><em>Parameter</em></a><em> or </em><em>None</em>) \u2013 parameter to be added to the module. If\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;None&lt;/span&gt;&lt;/code&gt;, then operations that run on parameters, such as ' inline=\"true\">None</cccode-inline>, then operations that run on parameters, such as <a class=\"reference internal\" href=\"#torch.nn.Module.cuda\" title=\"torch.nn.Module.cuda\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;cuda&lt;/span&gt;&lt;/code&gt;' inline=\"true\">cuda</cccode-inline></a>,\nare ignored. If <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;None&lt;/span&gt;&lt;/code&gt;, the parameter is ' inline=\"true\">None</cccode-inline>, the parameter is <strong>not</strong> included in the\nmodule\u2019s <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict</cccode-inline></a>.</p></li></ul></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Register a post-hook for the <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict()</cccode-inline></a> method.</p><dl class=\"simple\"><dt>It should have the following signature::</dt><dd><p>hook(module, state_dict, prefix, local_metadata) -&gt; None</p></dd></dl><p>The registered hooks can modify the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt; inplace.' inline=\"true\">state_dict</cccode-inline> inplace.</p></dd></dl><dl class=\"py method\"><dd><p>Register a pre-hook for the <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">state_dict()</cccode-inline></a> method.</p><dl class=\"simple\"><dt>It should have the following signature::</dt><dd><p>hook(module, prefix, keep_vars) -&gt; None</p></dd></dl><p>The registered hooks can be used to perform pre-processing before the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict&lt;/span&gt;&lt;/code&gt;\ncall is made.' inline=\"true\">state_dict</cccode-inline>\ncall is made.</p></dd></dl><dl class=\"py method\"><dd><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters\u2019 <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;requires_grad&lt;/span&gt;&lt;/code&gt; attributes\nin-place.' inline=\"true\">requires_grad</cccode-inline> attributes\nin-place.</p><p>This method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).</p><p>See <a class=\"reference internal\" href=\"../notes/autograd.html#locally-disable-grad-doc\"><span class=\"std std-ref\">Locally disabling gradient computation</span></a> for a comparison between\n.requires_grad_() and several similar mechanisms that may be confused with it.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>requires_grad</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 whether autograd should record operations on\nparameters in this module. Default: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">True</cccode-inline>.</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>self</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Set extra state contained in the loaded state_dict.</p><p>This function is called from <a class=\"reference internal\" href=\"#torch.nn.Module.load_state_dict\" title=\"torch.nn.Module.load_state_dict\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;load_state_dict()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">load_state_dict()</cccode-inline></a> to handle any extra state\nfound within the state_dict. Implement this function and a corresponding\n<a class=\"reference internal\" href=\"#torch.nn.Module.get_extra_state\" title=\"torch.nn.Module.get_extra_state\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-func docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;get_extra_state()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">get_extra_state()</cccode-inline></a> for your module if you need to store extra state within its\nstate_dict.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>state</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.13)\"><em>dict</em></a>) \u2013 Extra state from the state_dict</p></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.set_submodule\"><span class=\"sig-name descname\"><span class=\"pre\">set_submodule</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">target</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">module</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.set_submodule\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.set_submodule\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Set the submodule given by <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;target&lt;/span&gt;&lt;/code&gt; if it exists, otherwise throw an error.' inline=\"true\">target</cccode-inline> if it exists, otherwise throw an error.</p><p>For example, let\u2019s say you have an <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;nn.Module&lt;/span&gt;&lt;/code&gt; ' inline=\"true\">nn.Module</cccode-inline><cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;A&lt;/span&gt;&lt;/code&gt; that\nlooks like this:' inline=\"true\">A</cccode-inline> that\nlooks like this:</p><div class=\"highlight-text notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre>A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-text notranslate\"><div class=\"highlight\"></div></div><p>(The diagram shows an <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;nn.Module&lt;/span&gt;&lt;/code&gt; ' inline=\"true\">nn.Module</cccode-inline><cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;A&lt;/span&gt;&lt;/code&gt;. ' inline=\"true\">A</cccode-inline>. <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;A&lt;/span&gt;&lt;/code&gt; has a nested\nsubmodule ' inline=\"true\">A</cccode-inline> has a nested\nsubmodule <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;net_b&lt;/span&gt;&lt;/code&gt;, which itself has two submodules ' inline=\"true\">net_b</cccode-inline>, which itself has two submodules <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;net_c&lt;/span&gt;&lt;/code&gt;\nand ' inline=\"true\">net_c</cccode-inline>\nand <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;linear&lt;/span&gt;&lt;/code&gt;. ' inline=\"true\">linear</cccode-inline>. <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;net_c&lt;/span&gt;&lt;/code&gt; then has a submodule ' inline=\"true\">net_c</cccode-inline> then has a submodule <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;conv&lt;/span&gt;&lt;/code&gt;.)' inline=\"true\">conv</cccode-inline>.)</p><p>To overide the <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;Conv2d&lt;/span&gt;&lt;/code&gt; with a new submodule ' inline=\"true\">Conv2d</cccode-inline> with a new submodule <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;Linear&lt;/span&gt;&lt;/code&gt;, you\nwould call\n' inline=\"true\">Linear</cccode-inline>, you\nwould call\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;set_submodule(\"net_b.net_c.conv\",&lt;/span&gt; &lt;span class=\"pre\"&gt;nn.Linear(33,&lt;/span&gt; &lt;span class=\"pre\"&gt;16))&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))</cccode-inline>.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>target</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)</p></li><li><p><strong>module</strong> (<a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.modules.module.Module\"><em>Module</em></a>) \u2013 The module to set the submodule to.</p></li></ul></dd><dt class=\"field-even\">Raises</dt><dd class=\"field-even\"><ul class=\"simple\"><li><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#ValueError\" title=\"(in Python v3.13)\"><strong>ValueError</strong></a> \u2013 If the target string is empty</p></li><li><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#AttributeError\" title=\"(in Python v3.13)\"><strong>AttributeError</strong></a> \u2013 If the target string references an invalid\n    path or resolves to something that is not an\n    <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;nn.Module&lt;/span&gt;&lt;/code&gt;' inline=\"true\">nn.Module</cccode-inline></p></li></ul></dd></dl></dd></dl><dl class=\"py method\"><dd><p>See <a class=\"reference internal\" href=\"torch.Tensor.share_memory_.html#torch.Tensor.share_memory_\" title=\"torch.Tensor.share_memory_\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.Tensor.share_memory_()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">torch.Tensor.share_memory_()</cccode-inline></a>.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"><p><em>T</em></p></dd></dl></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.state_dict\"><span class=\"sig-name descname\"><span class=\"pre\">state_dict</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">destination</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><span class=\"pre\">T_destination</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prefix</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><span class=\"pre\">str</span></a></span><span class=\"w\"></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">keep_vars</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><span class=\"pre\">bool</span></a></span><span class=\"w\"></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><span class=\"sig-return\"><span class=\"sig-return-typehint\"><span class=\"pre\">T_destination</span></span></span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.state_dict\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.state_dict\" title=\"Permalink to this definition\">\u00b6</a></dt><dt class=\"sig sig-object py\"><span class=\"sig-name descname\"><span class=\"pre\">state_dict</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prefix</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><span class=\"pre\">str</span></a></span><span class=\"w\"></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">keep_vars</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><span class=\"pre\">bool</span></a></span><span class=\"w\"></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><span class=\"sig-return\"><span class=\"sig-return-typehint\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Dict\" title=\"(in Python v3.13)\"><span class=\"pre\">Dict</span></a><span class=\"p\"><span class=\"pre\">[</span></span><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><span class=\"pre\">str</span></a><span class=\"p\"><span class=\"pre\">,</span></span><span class=\"w\"></span><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Any\" title=\"(in Python v3.13)\"><span class=\"pre\">Any</span></a><span class=\"p\"><span class=\"pre\">]</span></span></span></span></dt><dd><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;None&lt;/span&gt;&lt;/code&gt; are not included.' inline=\"true\">None</cccode-inline> are not included.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>The returned object is a shallow copy. It contains references\nto the module\u2019s parameters and buffers.</p></div><div class=\"admonition warning\"><p class=\"admonition-title\">Warning</p><p>Currently <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;state_dict()&lt;/span&gt;&lt;/code&gt; also accepts positional arguments for\n' inline=\"true\">state_dict()</cccode-inline> also accepts positional arguments for\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;destination&lt;/span&gt;&lt;/code&gt;, ' inline=\"true\">destination</cccode-inline>, <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;prefix&lt;/span&gt;&lt;/code&gt; and ' inline=\"true\">prefix</cccode-inline> and <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;keep_vars&lt;/span&gt;&lt;/code&gt; in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.' inline=\"true\">keep_vars</cccode-inline> in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.</p></div><div class=\"admonition warning\"><p class=\"admonition-title\">Warning</p><p>Please avoid the use of argument <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;destination&lt;/span&gt;&lt;/code&gt; as it is not\ndesigned for end-users.' inline=\"true\">destination</cccode-inline> as it is not\ndesigned for end-users.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>destination</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.13)\"><em>dict</em></a><em>, </em><em>optional</em>) \u2013 If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;OrderedDict&lt;/span&gt;&lt;/code&gt; will be created and returned.\nDefault: ' inline=\"true\">OrderedDict</cccode-inline> will be created and returned.\nDefault: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;None&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">None</cccode-inline>.</p></li><li><p><strong>prefix</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a><em>, </em><em>optional</em>) \u2013 a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: <cccode-inline by=\"tag_code\" html=\"&lt;code class=&quot;docutils literal notranslate&quot;&gt;&lt;span class=&quot;pre&quot;&gt;''&lt;/span&gt;&lt;/code&gt;.\" inline=\"true\">''</cccode-inline>.</p></li><li><p><strong>keep_vars</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a><em>, </em><em>optional</em>) \u2013 by default the <a class=\"reference internal\" href=\"../tensors.html#torch.Tensor\" title=\"torch.Tensor\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;Tensor&lt;/span&gt;&lt;/code&gt;' inline=\"true\">Tensor</cccode-inline></a> s\nreturned in the state dict are detached from autograd. If it\u2019s\nset to <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;, detaching will not be performed.\nDefault: ' inline=\"true\">True</cccode-inline>, detaching will not be performed.\nDefault: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">False</cccode-inline>.</p></li></ul></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>a dictionary containing a whole state of the module</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl><p>Example:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">state_dict</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">()</span>\n<span class=\"go\">['bias', 'weight']</span>\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.to\"><span class=\"sig-name descname\"><span class=\"pre\">to</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">device</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Optional\" title=\"(in Python v3.13)\"><span class=\"pre\">Optional</span></a><span class=\"p\"><span class=\"pre\">[</span></span><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Union\" title=\"(in Python v3.13)\"><span class=\"pre\">Union</span></a><span class=\"p\"><span class=\"pre\">[</span></span><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><span class=\"pre\">str</span></a><span class=\"p\"><span class=\"pre\">,</span></span><span class=\"w\"></span><a class=\"reference internal\" href=\"../tensor_attributes.html#torch.device\" title=\"torch.device\"><span class=\"pre\">device</span></a><span class=\"p\"><span class=\"pre\">,</span></span><span class=\"w\"></span><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.13)\"><span class=\"pre\">int</span></a><span class=\"p\"><span class=\"pre\">]</span></span><span class=\"p\"><span class=\"pre\">]</span></span></span><span class=\"w\"></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"></span><span class=\"default_value\"><span class=\"pre\">...</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">dtype</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Optional\" title=\"(in Python v3.13)\"><span class=\"pre\">Optional</span></a><span class=\"p\"><span class=\"pre\">[</span></span><a class=\"reference internal\" href=\"../tensor_attributes.html#torch.dtype\" title=\"torch.dtype\"><span class=\"pre\">dtype</span></a><span class=\"p\"><span class=\"pre\">]</span></span></span><span class=\"w\"></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"></span><span class=\"default_value\"><span class=\"pre\">...</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">non_blocking</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><span class=\"pre\">bool</span></a></span><span class=\"w\"></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"></span><span class=\"default_value\"><span class=\"pre\">...</span></span></em><span class=\"sig-paren\">)</span><span class=\"sig-return\"><span class=\"sig-return-typehint\"><span class=\"pre\">Self</span></span></span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.to\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.to\" title=\"Permalink to this definition\">\u00b6</a></dt><dt class=\"sig sig-object py\"><span class=\"sig-name descname\"><span class=\"pre\">to</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">dtype</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference internal\" href=\"../tensor_attributes.html#torch.dtype\" title=\"torch.dtype\"><span class=\"pre\">dtype</span></a></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">non_blocking</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><span class=\"pre\">bool</span></a></span><span class=\"w\"></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"></span><span class=\"default_value\"><span class=\"pre\">...</span></span></em><span class=\"sig-paren\">)</span><span class=\"sig-return\"><span class=\"sig-return-typehint\"><span class=\"pre\">Self</span></span></span></dt><dt class=\"sig sig-object py\"><span class=\"sig-name descname\"><span class=\"pre\">to</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">tensor</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference internal\" href=\"../tensors.html#torch.Tensor\" title=\"torch.Tensor\"><span class=\"pre\">Tensor</span></a></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">non_blocking</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"></span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><span class=\"pre\">bool</span></a></span><span class=\"w\"></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"></span><span class=\"default_value\"><span class=\"pre\">...</span></span></em><span class=\"sig-paren\">)</span><span class=\"sig-return\"><span class=\"sig-return-typehint\"><span class=\"pre\">Self</span></span></span></dt><dd><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>Its signature is similar to <a class=\"reference internal\" href=\"torch.Tensor.to.html#torch.Tensor.to\" title=\"torch.Tensor.to\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.Tensor.to()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">torch.Tensor.to()</cccode-inline></a>, but only accepts\nfloating point or complex <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;dtype&lt;/span&gt;&lt;/code&gt;s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to ' inline=\"true\">dtype</cccode-inline>s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;dtype&lt;/span&gt;&lt;/code&gt;\n(if given). The integral parameters and buffers will be moved\n' inline=\"true\">dtype</cccode-inline>\n(if given). The integral parameters and buffers will be moved\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;device&lt;/span&gt;&lt;/code&gt;, if that is given, but with dtypes unchanged. When\n' inline=\"true\">device</cccode-inline>, if that is given, but with dtypes unchanged. When\n<cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;non_blocking&lt;/span&gt;&lt;/code&gt; is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.' inline=\"true\">non_blocking</cccode-inline> is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.</p><p>See below for examples.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><p>Examples:</p><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl></dd></dl><pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">weight</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[ 0.1913, -0.3420],</span>\n<span class=\"go\">        [-0.5113, -0.2325]])</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">double</span><span class=\"p\">)</span>\n<span class=\"go\">Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">weight</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[ 0.1913, -0.3420],</span>\n<span class=\"go\">        [-0.5113, -0.2325]], dtype=torch.float64)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">gpu1</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">\"cuda:1\"</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">gpu1</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">half</span><span class=\"p\">,</span> <span class=\"n\">non_blocking</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"go\">Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">weight</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[ 0.1914, -0.3420],</span>\n<span class=\"go\">        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">cpu</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">\"cpu\"</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">cpu</span><span class=\"p\">)</span>\n<span class=\"go\">Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">weight</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[ 0.1914, -0.3420],</span>\n<span class=\"go\">        [-0.5112, -0.2324]], dtype=torch.float16)</span>\n\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cdouble</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">weight</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[ 0.3741+0.j,  0.2382+0.j],</span>\n<span class=\"go\">        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cdouble</span><span class=\"p\">))</span>\n<span class=\"go\">tensor([[0.6122+0.j, 0.1150+0.j],</span>\n<span class=\"go\">        [0.6122+0.j, 0.1150+0.j],</span>\n<span class=\"go\">        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</span>\n</pre><dl class=\"py class\"><dd><dl class=\"py method\"><dd><div class=\"highlight-default notranslate\"><div class=\"highlight\"></div></div></dd></dl><dl class=\"py method\"><dt class=\"sig sig-object py\" id=\"torch.nn.Module.to_empty\"><span class=\"sig-name descname\"><span class=\"pre\">to_empty</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">device</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">recurse</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.to_empty\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.to_empty\" title=\"Permalink to this definition\">\u00b6</a></dt><dd><p>Move the parameters and buffers to the specified device without copying storage.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><ul class=\"simple\"><li><p><strong>device</strong> (<a class=\"reference internal\" href=\"../tensor_attributes.html#torch.device\" title=\"torch.device\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.device&lt;/span&gt;&lt;/code&gt;' inline=\"true\">torch.device</cccode-inline></a>) \u2013 The desired device of the parameters\nand buffers in this module.</p></li><li><p><strong>recurse</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 Whether parameters and buffers of submodules should\nbe recursively moved to the specified device.</p></li></ul></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>self</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Set the module in training mode.</p><p>This has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. <a class=\"reference internal\" href=\"torch.nn.Dropout.html#torch.nn.Dropout\" title=\"torch.nn.Dropout\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;Dropout&lt;/span&gt;&lt;/code&gt;' inline=\"true\">Dropout</cccode-inline></a>, <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;BatchNorm&lt;/span&gt;&lt;/code&gt;,\netc.' inline=\"true\">BatchNorm</cccode-inline>,\netc.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>mode</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 whether to set training mode (<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;) or evaluation\nmode (' inline=\"true\">True</cccode-inline>) or evaluation\nmode (<cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;False&lt;/span&gt;&lt;/code&gt;). Default: ' inline=\"true\">False</cccode-inline>). Default: <cccode-inline by=\"tag_code\" html='&lt;code class=\"docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;True&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">True</cccode-inline>.</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>self</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Casts all parameters and buffers to <cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-attr docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;dst_type&lt;/span&gt;&lt;/code&gt;.' inline=\"true\">dst_type</cccode-inline>.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>dst_type</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#type\" title=\"(in Python v3.13)\"><em>type</em></a><em> or </em><em>string</em>) \u2013 the desired type</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>self</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Move all model parameters and buffers to the XPU.</p><p>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.</p><div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p>This method modifies the module in-place.</p></div><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>device</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.13)\"><em>int</em></a><em>, </em><em>optional</em>) \u2013 if specified, all parameters will be\ncopied to that device</p></dd><dt class=\"field-even\">Returns</dt><dd class=\"field-even\"><p>self</p></dd><dt class=\"field-odd\">Return type</dt><dd class=\"field-odd\"></dd></dl></dd></dl><dl class=\"py method\"><dd><p>Reset gradients of all model parameters.</p><p>See similar function under <a class=\"reference internal\" href=\"../optim.html#torch.optim.Optimizer\" title=\"torch.optim.Optimizer\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-class docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.optim.Optimizer&lt;/span&gt;&lt;/code&gt;' inline=\"true\">torch.optim.Optimizer</cccode-inline></a> for more context.</p><dl class=\"field-list simple\"><dt class=\"field-odd\">Parameters</dt><dd class=\"field-odd\"><p><strong>set_to_none</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 instead of setting to zero, set the grads to None.\nSee <a class=\"reference internal\" href=\"torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad\" title=\"torch.optim.Optimizer.zero_grad\"><cccode-inline by=\"tag_code\" html='&lt;code class=\"xref py py-meth docutils literal notranslate\"&gt;&lt;span class=\"pre\"&gt;torch.optim.Optimizer.zero_grad()&lt;/span&gt;&lt;/code&gt;' inline=\"true\">torch.optim.Optimizer.zero_grad()</cccode-inline></a> for details.</p></dd></dl></dd></dl></dd></dl>", "statics": {"title": 1, "list": 27, "list.text": 1140, "code": 26, "list.code-inline": 232}, "url": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module", "content": "# Module \u00b6\n\n1. class torch.nn. Module ( * args , ** kwargs ) [source] \u00b6\n2. Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes:\n\n```\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n```\n\n1. Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call `to()` , etc. Note As per the example above, an `__init__()` call to the parent class\nmust be made before assignment on the child. Variables training ( bool ) \u2013 Boolean represents whether this module is in training or\nevaluation mode. add_module ( name , module ) [source] \u00b6 Add a child module to the current module. The module can be accessed as an attribute using the given name. Parameters name ( str ) \u2013 name of the child module. The child module can be\naccessed from this module using the given name module ( Module ) \u2013 child module to be added to the module. Apply `fn` recursively to every submodule (as returned by `.children()` ) as well as self. Typical use includes initializing the parameters of a model\n(see also torch.nn.init ). Parameters fn ( `Module` -> None) \u2013 function to be applied to each submodule Returns self Return type Example:\n\n```\n>>> @torch.no_grad()\n>>> def init_weights(m):\n>>>     print(m)\n>>>     if type(m) == nn.Linear:\n>>>         m.weight.fill_(1.0)\n>>>         print(m.weight)\n>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n>>> net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n```\n\n1. Casts all floating point parameters and buffers to `bfloat16` datatype. Note This method modifies the module in-place. Returns self Return type Return an iterator over module buffers. Parameters recurse ( bool ) \u2013 if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Yields torch.Tensor \u2013 module buffer Return type Example:\n\n```\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n```\n\n1. Return an iterator over immediate children modules. Yields Module \u2013 a child module Return type compile ( * args , ** kwargs ) [source] \u00b6 Compile this Module\u2019s forward using `torch.compile()` . This Module\u2019s __call__ method is compiled and all arguments are passed as-is\nto `torch.compile()` . See `torch.compile()` for details on the arguments for this function. Move all model parameters and buffers to the CPU. Note This method modifies the module in-place. Returns self Return type Move all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on GPU while being optimized. Note This method modifies the module in-place. Parameters device ( int , optional ) \u2013 if specified, all parameters will be\ncopied to that device Returns self Return type Casts all floating point parameters and buffers to `double` datatype. Note This method modifies the module in-place. Returns self Return type Set the module in evaluation mode. This has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. `Dropout` , `BatchNorm` ,\netc. This is equivalent with `self.train(False)` . See Locally disabling gradient computation for a comparison between\n.eval() and several similar mechanisms that may be confused with it. Returns self Return type Set the extra representation of the module. To print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable. Return type Casts all floating point parameters and buffers to `float` datatype. Note This method modifies the module in-place. Returns self Return type forward ( * input ) \u00b6 Define the computation performed at every call. Should be overridden by all subclasses. Note Although the recipe for forward pass needs to be defined within\nthis function, one should call the `Module` instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them. Return the buffer given by `target` if it exists, otherwise throw an error. See the docstring for `get_submodule` for a more detailed\nexplanation of this method\u2019s functionality as well as how to\ncorrectly specify `target` . Parameters target ( str ) \u2013 The fully-qualified string name of the buffer\nto look for. (See `get_submodule` for how to specify a\nfully-qualified string.) Returns The buffer referenced by `target` Return type Raises AttributeError \u2013 If the target string references an invalid\n    path or resolves to something that is not a\n    buffer Return any extra state to include in the module\u2019s state_dict. Implement this and a corresponding `set_extra_state()` for your module\nif you need to store extra state. This function is called when building the\nmodule\u2019s state_dict(). Note that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes. Returns Any extra state to store in the module\u2019s state_dict Return type Return the parameter given by `target` if it exists, otherwise throw an error. See the docstring for `get_submodule` for a more detailed\nexplanation of this method\u2019s functionality as well as how to\ncorrectly specify `target` . Parameters target ( str ) \u2013 The fully-qualified string name of the Parameter\nto look for. (See `get_submodule` for how to specify a\nfully-qualified string.) Returns The Parameter referenced by `target` Return type torch.nn.Parameter Raises AttributeError \u2013 If the target string references an invalid\n    path or resolves to something that is not an `nn.Parameter` Return the submodule given by `target` if it exists, otherwise throw an error. For example, let\u2019s say you have an `nn.Module` `A` that\nlooks like this:\n\n```\nA(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)\n```\n\n1. (The diagram shows an `nn.Module` `A` . `A` has a nested\nsubmodule `net_b` , which itself has two submodules `net_c` and `linear` . `net_c` then has a submodule `conv` .) To check whether or not we have the `linear` submodule, we\nwould call `get_submodule(\"net_b.linear\")` . To check whether\nwe have the `conv` submodule, we would call `get_submodule(\"net_b.net_c.conv\")` . The runtime of `get_submodule` is bounded by the degree\nof module nesting in `target` . A query against `named_modules` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, `get_submodule` should always be\nused. Parameters target ( str ) \u2013 The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.) Returns The submodule referenced by `target` Return type Raises AttributeError \u2013 If the target string references an invalid\n    path or resolves to something that is not an `nn.Module` Casts all floating point parameters and buffers to `half` datatype. Note This method modifies the module in-place. Returns self Return type Move all model parameters and buffers to the IPU. This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on IPU while being optimized. Note This method modifies the module in-place. Parameters device ( int , optional ) \u2013 if specified, all parameters will be\ncopied to that device Returns self Return type load_state_dict ( state_dict , strict = True , assign = False ) [source] \u00b6 Copy parameters and buffers from `state_dict` into this module and its descendants. If `strict` is `True` , then\nthe keys of `state_dict` must exactly match the keys returned\nby this module\u2019s `state_dict()` function. Warning If `assign` is `True` the optimizer must be created after\nthe call to `load_state_dict` unless `get_swap_module_params_on_conversion()` is `True` . Parameters state_dict ( dict ) \u2013 a dict containing parameters and\npersistent buffers. strict ( bool , optional ) \u2013 whether to strictly enforce that the keys\nin `state_dict` match the keys returned by this module\u2019s `state_dict()` function. Default: `True` assign ( bool , optional ) \u2013 When `False` , the properties of the tensors\nin the current module are preserved while when `True` , the\nproperties of the Tensors in the state dict are preserved. The only\nexception is the `requires_grad` field of `Default: ``False`` Returns missing_keys is a list of str containing any keys that are expected by this module but missing from the provided `state_dict` . unexpected_keys is a list of str containing the keys that are not expected by this module but present in the provided `state_dict` . Return type `NamedTuple` with `missing_keys` and `unexpected_keys` fields Note If a parameter or buffer is registered as `None` and its corresponding key\nexists in `state_dict` , `load_state_dict()` will raise a `RuntimeError` . Return an iterator over all modules in the network. Yields Module \u2013 a module in the network Return type Note Duplicate modules are returned only once. In the following\nexample, `l` will be returned only once. Example:\n\n```\n>>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n...     print(idx, '->', m)\n\n0 -> Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)\n```\n\n1. Move all model parameters and buffers to the MTIA. This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on MTIA while being optimized. Note This method modifies the module in-place. Parameters device ( int , optional ) \u2013 if specified, all parameters will be\ncopied to that device Returns self Return type named_buffers ( prefix = '' , recurse = True , remove_duplicate = True ) [source] \u00b6 Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters prefix ( str ) \u2013 prefix to prepend to all buffer names. recurse ( bool , optional ) \u2013 if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True. remove_duplicate ( bool , optional ) \u2013 whether to remove the duplicated buffers in the result. Defaults to True. Yields (str, torch.Tensor) \u2013 Tuple containing the name and buffer Return type Iterator [ Tuple [ str , Tensor ]] Example:\n\n```\n>>> for name, buf in self.named_buffers():\n>>>     if name in ['running_var']:\n>>>         print(buf.size())\n```\n\n1. Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields (str, Module) \u2013 Tuple containing a name and child module Return type Iterator [ Tuple [ str , Module ]] Example:\n\n```\n>>> for name, module in model.named_children():\n>>>     if name in ['conv4', 'conv5']:\n>>>         print(module)\n```\n\n1. named_modules ( memo = None , prefix = '' , remove_duplicate = True ) [source] \u00b6 Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Parameters memo ( Optional [ Set [ Module ] ] ) \u2013 a memo to store the set of modules already added to the result prefix ( str ) \u2013 a prefix that will be added to the name of the module remove_duplicate ( bool ) \u2013 whether to remove the duplicated module instances in the result\nor not Yields (str, Module) \u2013 Tuple of name and module Note Duplicate modules are returned only once. In the following\nexample, `l` will be returned only once. Example:\n\n```\n>>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n...     print(idx, '->', m)\n\n0 -> ('', Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n```\n\n1. named_parameters ( prefix = '' , recurse = True , remove_duplicate = True ) [source] \u00b6 Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters prefix ( str ) \u2013 prefix to prepend to all parameter names. recurse ( bool ) \u2013 if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module. remove_duplicate ( bool , optional ) \u2013 whether to remove the duplicated\nparameters in the result. Defaults to True. Yields (str, Parameter) \u2013 Tuple containing the name and parameter Return type Iterator [ Tuple [ str , Parameter ]] Example:\n\n```\n>>> for name, param in self.named_parameters():\n>>>     if name in ['bias']:\n>>>         print(param.size())\n```\n\n1. Return an iterator over module parameters. This is typically passed to an optimizer. Parameters recurse ( bool ) \u2013 if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module. Yields Parameter \u2013 module parameter Return type Example:\n\n```\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n```\n\n1. Register a backward hook on the module. This function is deprecated in favor of `register_full_backward_hook()` and\nthe behavior of this function will change in future versions. Returns a handle that can be used to remove the added hook by calling `handle.remove()` Return type\n\n```\ntorch.utils.hooks.RemovableHandle\n```\n\n1. register_buffer ( name , tensor , persistent = True ) [source] \u00b6 Add a buffer to the module. This is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm\u2019s `running_mean` is not a parameter, but is part of the module\u2019s state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting `persistent` to `False` . The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module\u2019s `state_dict` . Buffers can be accessed as attributes using given names. Parameters name ( str ) \u2013 name of the buffer. The buffer can be accessed\nfrom this module using the given name tensor ( Tensor or None ) \u2013 buffer to be registered. If `None` , then operations\nthat run on buffers, such as `cuda` , are ignored. If `None` ,\nthe buffer is not included in the module\u2019s `state_dict` . persistent ( bool ) \u2013 whether the buffer is part of this module\u2019s `state_dict` . Example:\n\n```\n>>> self.register_buffer('running_mean', torch.zeros(num_features))\n```\n\n1. register_forward_hook ( hook , * , prepend = False , with_kwargs = False , always_call = False ) [source] \u00b6 Register a forward hook on the module. The hook will be called every time after `forward()` has computed an output. If `with_kwargs` is `False` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won\u2019t be\npassed to the hooks and only to the `forward` . The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after `forward()` is called. The hook\nshould have the following signature:\n\n```\nhook(module, args, output) -> None or modified output\n```\n\n1. If `with_kwargs` is `True` , the forward hook will be passed the `kwargs` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature:\n\n```\nhook(module, args, kwargs, output) -> None or modified output\n```\n\n1. Parameters hook ( Callable ) \u2013 The user defined hook to be registered. prepend ( bool ) \u2013 If `True` , the provided `hook` will be fired\nbefore all existing `forward` hooks on this `torch.nn.modules.Module` . Otherwise, the provided `hook` will be fired after all existing `forward` hooks on\nthis `torch.nn.modules.Module` . Note that global `forward` hooks registered with `register_module_forward_hook()` will fire before all hooks\nregistered by this method.\nDefault: `False` with_kwargs ( bool ) \u2013 If `True` , the `hook` will be passed the\nkwargs given to the forward function.\nDefault: `False` always_call ( bool ) \u2013 If `True` the `hook` will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: `False` Returns a handle that can be used to remove the added hook by calling `handle.remove()` Return type\n\n```\ntorch.utils.hooks.RemovableHandle\n```\n\n1. register_forward_pre_hook ( hook , * , prepend = False , with_kwargs = False ) [source] \u00b6 Register a forward pre-hook on the module. The hook will be called every time before `forward()` is invoked. If `with_kwargs` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won\u2019t be\npassed to the hooks and only to the `forward` . The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature:\n\n```\nhook(module, args) -> None or modified input\n```\n\n1. If `with_kwargs` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature:\n\n```\nhook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n```\n\n1. Parameters hook ( Callable ) \u2013 The user defined hook to be registered. prepend ( bool ) \u2013 If true, the provided `hook` will be fired before\nall existing `forward_pre` hooks on this `torch.nn.modules.Module` . Otherwise, the provided `hook` will be fired after all existing `forward_pre` hooks\non this `torch.nn.modules.Module` . Note that global `forward_pre` hooks registered with `register_module_forward_pre_hook()` will fire before all\nhooks registered by this method.\nDefault: `False` with_kwargs ( bool ) \u2013 If true, the `hook` will be passed the kwargs\ngiven to the forward function.\nDefault: `False` Returns a handle that can be used to remove the added hook by calling `handle.remove()` Return type\n\n```\ntorch.utils.hooks.RemovableHandle\n```\n\n1. register_full_backward_hook ( hook , prepend = False ) [source] \u00b6 Register a backward hook on the module. The hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature:\n\n```\nhook(module, grad_input, grad_output) -> tuple(Tensor) or None\n```\n\n1. The `grad_input` and `grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of `grad_input` in\nsubsequent computations. `grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin `grad_input` and `grad_output` will be `None` for all non-Tensor\narguments. For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module\u2019s forward function. Warning Modifying inputs or outputs inplace is not allowed when using backward hooks and\nwill raise an error. Parameters hook ( Callable ) \u2013 The user-defined hook to be registered. prepend ( bool ) \u2013 If true, the provided `hook` will be fired before\nall existing `backward` hooks on this `torch.nn.modules.Module` . Otherwise, the provided `hook` will be fired after all existing `backward` hooks on\nthis `torch.nn.modules.Module` . Note that global `backward` hooks registered with `register_module_full_backward_hook()` will fire before\nall hooks registered by this method. Returns a handle that can be used to remove the added hook by calling `handle.remove()` Return type\n\n```\ntorch.utils.hooks.RemovableHandle\n```\n\n1. register_full_backward_pre_hook ( hook , prepend = False ) [source] \u00b6 Register a backward pre-hook on the module. The hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature:\n\n```\nhook(module, grad_output) -> tuple[Tensor] or None\n```\n\n1. The `grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of `grad_output` in\nsubsequent computations. Entries in `grad_output` will be `None` for\nall non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module\u2019s forward function. Warning Modifying inputs inplace is not allowed when using backward hooks and\nwill raise an error. Parameters hook ( Callable ) \u2013 The user-defined hook to be registered. prepend ( bool ) \u2013 If true, the provided `hook` will be fired before\nall existing `backward_pre` hooks on this `torch.nn.modules.Module` . Otherwise, the provided `hook` will be fired after all existing `backward_pre` hooks\non this `torch.nn.modules.Module` . Note that global `backward_pre` hooks registered with `register_module_full_backward_pre_hook()` will fire before\nall hooks registered by this method. Returns a handle that can be used to remove the added hook by calling `handle.remove()` Return type\n\n```\ntorch.utils.hooks.RemovableHandle\n```\n\n1. Register a post-hook to be run after module\u2019s `load_state_dict()` is called. It should have the following signature:: hook(module, incompatible_keys) -> None The `module` argument is the current module that this hook is registered\non, and the `incompatible_keys` argument is a `NamedTuple` consisting\nof attributes `missing_keys` and `unexpected_keys` . `missing_keys` is a `list` of `str` containing the missing keys and `unexpected_keys` is a `list` of `str` containing the unexpected keys. The given incompatible_keys can be modified inplace if needed. Note that the checks performed when calling `load_state_dict()` with `strict=True` are affected by modifications the hook makes to `missing_keys` or `unexpected_keys` , as expected. Additions to either\nset of keys will result in an error being thrown when `strict=True` , and\nclearing out both missing and unexpected keys will avoid an error. Returns a handle that can be used to remove the added hook by calling `handle.remove()` Return type\n\n```\ntorch.utils.hooks.RemovableHandle\n```\n\n1. Register a pre-hook to be run before module\u2019s `load_state_dict()` is called. It should have the following signature:: hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  \\# noqa: B950 Parameters hook ( Callable ) \u2013 Callable hook that will be invoked before\nloading the state dict. register_module ( name , module ) [source] \u00b6 Alias for `add_module()` . register_parameter ( name , param ) [source] \u00b6 Add a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters name ( str ) \u2013 name of the parameter. The parameter can be accessed\nfrom this module using the given name param ( Parameter or None ) \u2013 parameter to be added to the module. If `None` , then operations that run on parameters, such as `cuda` ,\nare ignored. If `None` , the parameter is not included in the\nmodule\u2019s `state_dict` . Register a post-hook for the `state_dict()` method. It should have the following signature:: hook(module, state_dict, prefix, local_metadata) -> None The registered hooks can modify the `state_dict` inplace. Register a pre-hook for the `state_dict()` method. It should have the following signature:: hook(module, prefix, keep_vars) -> None The registered hooks can be used to perform pre-processing before the `state_dict` call is made. Change if autograd should record operations on parameters in this module. This method sets the parameters\u2019 `requires_grad` attributes\nin-place. This method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training). See Locally disabling gradient computation for a comparison between\n.requires_grad_() and several similar mechanisms that may be confused with it. Parameters requires_grad ( bool ) \u2013 whether autograd should record operations on\nparameters in this module. Default: `True` . Returns self Return type Set extra state contained in the loaded state_dict. This function is called from `load_state_dict()` to handle any extra state\nfound within the state_dict. Implement this function and a corresponding `get_extra_state()` for your module if you need to store extra state within its\nstate_dict. Parameters state ( dict ) \u2013 Extra state from the state_dict set_submodule ( target , module ) [source] \u00b6 Set the submodule given by `target` if it exists, otherwise throw an error. For example, let\u2019s say you have an `nn.Module` `A` that\nlooks like this:\n\n```\nA(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)\n```\n\n1. (The diagram shows an `nn.Module` `A` . `A` has a nested\nsubmodule `net_b` , which itself has two submodules `net_c` and `linear` . `net_c` then has a submodule `conv` .) To overide the `Conv2d` with a new submodule `Linear` , you\nwould call `set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))` . Parameters target ( str ) \u2013 The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.) module ( Module ) \u2013 The module to set the submodule to. Raises ValueError \u2013 If the target string is empty AttributeError \u2013 If the target string references an invalid\n    path or resolves to something that is not an `nn.Module` See `torch.Tensor.share_memory_()` . Return type T state_dict ( * , destination : T_destination , prefix : str = '' , keep_vars : bool = False ) T_destination [source] \u00b6 state_dict ( * , prefix : str = '' , keep_vars : bool = False ) Dict [ str , Any ] Return a dictionary containing references to the whole state of the module. Both parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to `None` are not included. Note The returned object is a shallow copy. It contains references\nto the module\u2019s parameters and buffers. Warning Currently `state_dict()` also accepts positional arguments for `destination` , `prefix` and `keep_vars` in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases. Warning Please avoid the use of argument `destination` as it is not\ndesigned for end-users. Parameters destination ( dict , optional ) \u2013 If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an `OrderedDict` will be created and returned.\nDefault: `None` . prefix ( str , optional ) \u2013 a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: `''` . keep_vars ( bool , optional ) \u2013 by default the `Tensor` s\nreturned in the state dict are detached from autograd. If it\u2019s\nset to `True` , detaching will not be performed.\nDefault: `False` . Returns a dictionary containing a whole state of the module Return type Example:\n\n```\n>>> module.state_dict().keys()\n['bias', 'weight']\n```\n\n1. to ( device : Optional [ Union [ str , device , int ] ] = ... , dtype : Optional [ dtype ] = ... , non_blocking : bool = ... ) Self [source] \u00b6 to ( dtype : dtype , non_blocking : bool = ... ) Self to ( tensor : Tensor , non_blocking : bool = ... ) Self Move and/or cast the parameters and buffers. This can be called as Its signature is similar to `torch.Tensor.to()` , but only accepts\nfloating point or complex `dtype` s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to `dtype` (if given). The integral parameters and buffers will be moved `device` , if that is given, but with dtypes unchanged. When `non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices. See below for examples. Note This method modifies the module in-place. Examples:\n\n```\n>>> linear = nn.Linear(2, 2)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n>>> linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n>>> gpu1 = torch.device(\"cuda:1\")\n>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n>>> cpu = torch.device(\"cpu\")\n>>> linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n>>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n>>> linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n```\n\n1. to_empty ( * , device , recurse = True ) [source] \u00b6 Move the parameters and buffers to the specified device without copying storage. Parameters device ( `torch.device` ) \u2013 The desired device of the parameters\nand buffers in this module. recurse ( bool ) \u2013 Whether parameters and buffers of submodules should\nbe recursively moved to the specified device. Returns self Return type Set the module in training mode. This has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. `Dropout` , `BatchNorm` ,\netc. Parameters mode ( bool ) \u2013 whether to set training mode ( `True` ) or evaluation\nmode ( `False` ). Default: `True` . Returns self Return type Casts all parameters and buffers to `dst_type` . Note This method modifies the module in-place. Parameters dst_type ( type or string ) \u2013 the desired type Returns self Return type Move all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized. Note This method modifies the module in-place. Parameters device ( int , optional ) \u2013 if specified, all parameters will be\ncopied to that device Returns self Return type Reset gradients of all model parameters. See similar function under `torch.optim.Optimizer` for more context. Parameters set_to_none ( bool ) \u2013 instead of setting to zero, set the grads to None.\nSee `torch.optim.Optimizer.zero_grad()` for details.\n", "html": "\n\n\n\n<!DOCTYPE html>\n<!--[if IE 8]><html class=\"no-js lt-ie9\" lang=\"en\" > <![endif]-->\n<!--[if gt IE 8]><!--> <html class=\"no-js\" lang=\"en\" > <!--<![endif]-->\n<head>\n  <meta charset=\"utf-8\">\n\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n\n  <title>Module &mdash; PyTorch 2.5 documentation</title>\n\n\n\n\n\n\n    <link rel=\"canonical\" href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html\"/>\n\n\n\n\n\n\n\n\n\n\n  <link rel=\"stylesheet\" href=\"../_static/css/theme.css\" type=\"text/css\" />\n  <!-- <link rel=\"stylesheet\" href=\"../_static/pygments.css\" type=\"text/css\" /> -->\n  <link rel=\"stylesheet\" href=\"../_static/pygments.css\" type=\"text/css\" />\n  <link rel=\"stylesheet\" href=\"../_static/css/theme.css\" type=\"text/css\" />\n  <link rel=\"stylesheet\" href=\"../_static/copybutton.css\" type=\"text/css\" />\n  <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css\" type=\"text/css\" />\n  <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css\" type=\"text/css\" />\n  <link rel=\"stylesheet\" href=\"../_static/katex-math.css\" type=\"text/css\" />\n  <link rel=\"stylesheet\" href=\"../_static/sphinx-dropdown.css\" type=\"text/css\" />\n  <link rel=\"stylesheet\" href=\"../_static/panels-bootstrap.min.css\" type=\"text/css\" />\n  <link rel=\"stylesheet\" href=\"../_static/css/jit.css\" type=\"text/css\" />\n    <link rel=\"index\" title=\"Index\" href=\"../genindex.html\" />\n    <link rel=\"search\" title=\"Search\" href=\"../search.html\" />\n    <link rel=\"next\" title=\"Sequential\" href=\"torch.nn.Sequential.html\" />\n    <link rel=\"prev\" title=\"UninitializedBuffer\" href=\"torch.nn.parameter.UninitializedBuffer.html\" />\n\n\n  <!-- Google Tag Manager -->\n    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\n    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],\n    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=\n    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);\n    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>\n    <!-- End Google Tag Manager -->\n\n\n\n\n  <script src=\"../_static/js/modernizr.min.js\"></script>\n\n  <!-- Preload the theme fonts -->\n\n<link rel=\"preload\" href=\"../_static/fonts/FreightSans/freight-sans-book.woff2\" as=\"font\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" href=\"../_static/fonts/FreightSans/freight-sans-medium.woff2\" as=\"font\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" href=\"../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2\" as=\"font\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" href=\"../_static/fonts/FreightSans/freight-sans-bold.woff2\" as=\"font\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" href=\"../_static/fonts/FreightSans/freight-sans-medium-italic.woff2\" as=\"font\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" href=\"../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2\" as=\"font\" type=\"font/woff2\" crossorigin=\"anonymous\">\n\n<!-- Preload the katex fonts -->\n\n<link rel=\"preload\" href=\"https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2\" as=\"font\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" href=\"https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2\" as=\"font\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" href=\"https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2\" as=\"font\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" href=\"https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2\" as=\"font\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" href=\"https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2\" as=\"font\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" href=\"https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2\" as=\"font\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" href=\"https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2\" as=\"font\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" href=\"https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2\" as=\"font\" type=\"font/woff2\" crossorigin=\"anonymous\">\n  <link rel=\"stylesheet\" href=\"https://use.fontawesome.com/releases/v5.15.2/css/all.css\" integrity=\"sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu\" crossorigin=\"anonymous\">\n</head>\n\n<div class=\"container-fluid header-holder tutorials-header\" id=\"header-holder\">\n  <div class=\"container\">\n    <div class=\"header-container\">\n      <a class=\"header-logo\" href=\"https://pytorch.org/\" aria-label=\"PyTorch\"></a>\n\n      <div class=\"main-menu\">\n        <ul>\n\n          <li class=\"main-menu-item\">\n          <div id=\"resourcesDropdownButton\" data-toggle=\"resources-dropdown\" class=\"resources-dropdown\">\n              <a class=\"with-down-arrow\">\n                Learn\n              </a>\n              <div class=\"resources-dropdown-menu\">\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/get-started\">\n                  <span class=dropdown-title>Get Started</span>\n                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>\n                </a>\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/tutorials\">\n                  <span class=\"dropdown-title\">Tutorials</span>\n                  <p>Whats new in PyTorch tutorials</p>\n                </a>\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/tutorials/beginner/basics/intro.html\">\n                  <span class=\"dropdown-title\">Learn the Basics</span>\n                  <p>Familiarize yourself with PyTorch concepts and modules</p>\n                </a>\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/tutorials/recipes/recipes_index.html\">\n                  <span class=\"dropdown-title\">PyTorch Recipes</span>\n                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>\n                </a>\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/tutorials/beginner/introyt.html\">\n                  <span class=\"dropdown-title\">Intro to PyTorch - YouTube Series</span>\n                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>\n                </a>\n              </div>\n            </div>\n          </li>\n\n          <li>\n          <div id=\"resourcesDropdownButton\" data-toggle=\"resources-dropdown\" class=\"resources-dropdown\">\n              <a class=\"with-down-arrow\">\n                Ecosystem\n              </a>\n              <div class=\"resources-dropdown-menu\">\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/ecosystem\">\n                  <span class=\"dropdown-title\">Tools</span>\n                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>\n                </a>\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/#community-module\">\n                  <span class=dropdown-title>Community</span>\n                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>\n                </a>\n                <a class=\"nav-dropdown-item\" href=\"https://discuss.pytorch.org/\" target=\"_blank\">\n                  <span class=dropdown-title>Forums</span>\n                  <p>A place to discuss PyTorch code, issues, install, research</p>\n                </a>\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/resources\">\n                  <span class=dropdown-title>Developer Resources</span>\n                  <p>Find resources and get questions answered</p>\n                </a>\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/ecosystem/contributor-awards-2023\">\n                  <span class=\"dropdown-title\">Contributor Awards - 2023</span>\n                  <p>Award winners announced at this year's PyTorch Conference</p>\n                </a>\n              </div>\n            </div>\n          </li>\n\n          <li>\n          <div id=\"resourcesDropdownButton\" data-toggle=\"resources-dropdown\" class=\"resources-dropdown\">\n              <a class=\"with-down-arrow\">\n                Edge\n              </a>\n              <div class=\"resources-dropdown-menu\">\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/edge\">\n                  <span class=\"dropdown-title\">About PyTorch Edge</span>\n                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>\n                </a>\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/executorch-overview\">\n                  <span class=\"dropdown-title\">ExecuTorch</span>\n                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>\n                </a>\n              </div>\n            </div>\n          </li>\n\n          <li class=\"main-menu-item\">\n            <div id=\"resourcesDropdownButton\" data-toggle=\"resources-dropdown\" class=\"resources-dropdown\">\n              <a class=\"with-down-arrow\">\n                Docs\n              </a>\n              <div class=\"resources-dropdown-menu\">\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/docs/stable/index.html\">\n                  <span class=\"dropdown-title\">PyTorch</span>\n                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>\n                </a>\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/pytorch-domains\">\n                  <span class=\"dropdown-title\">PyTorch Domains</span>\n                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>\n                </a>\n              </div>\n            </div>\n          </li>\n\n          <li>\n            <div id=\"resourcesDropdownButton\" data-toggle=\"resources-dropdown\" class=\"resources-dropdown\">\n              <a class=\"with-down-arrow\">\n                Blogs & News\n              </a>\n              <div class=\"resources-dropdown-menu\">\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/blog/\">\n                  <span class=\"dropdown-title\">PyTorch Blog</span>\n                  <p>Catch up on the latest technical news and happenings</p>\n                </a>\n                 <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/community-blog\">\n                  <span class=\"dropdown-title\">Community Blog</span>\n                  <p>Stories from the PyTorch ecosystem</p>\n                </a>\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/videos\">\n                  <span class=\"dropdown-title\">Videos</span>\n                  <p>Learn about the latest PyTorch tutorials, new, and more </p>\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/community-stories\">\n                  <span class=\"dropdown-title\">Community Stories</span>\n                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>\n                </a>\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/events\">\n                  <span class=\"dropdown-title\">Events</span>\n                  <p>Find events, webinars, and podcasts</p>\n                </a>\n            </div>\n          </li>\n\n          <li>\n            <div id=\"resourcesDropdownButton\" data-toggle=\"resources-dropdown\" class=\"resources-dropdown\">\n              <a class=\"with-down-arrow\">\n                About\n              </a>\n              <div class=\"resources-dropdown-menu\">\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/foundation\">\n                  <span class=\"dropdown-title\">PyTorch Foundation</span>\n                  <p>Learn more about the PyTorch Foundation</p>\n                </a>\n                <a class=\"nav-dropdown-item\" href=\"https://pytorch.org/governing-board\">\n                  <span class=\"dropdown-title\">Governing Board</span>\n                  <p></p>\n                </a>\n              </div>\n            </div>\n          </li>\n\n          <li class=\"main-menu-item\">\n            <div class=\"no-dropdown\">\n              <a href=\"https://pytorch.org/join\" data-cta=\"join\">\n                Become a Member\n              </a>\n            </div>\n          </li>\n          <li>\n           <div class=\"main-menu-item\">\n             <a href=\"https://github.com/pytorch/pytorch\" class=\"github-icon\">\n             </a>\n           </div>\n          </li>\n          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later.\n          <li>\n            <div class=\"main-menu-item\">\n             <a href=\"https://github.com/pytorch/pytorch\" class=\"search-icon\">\n             </a>\n            </div>\n          </li>\n          --->\n        </ul>\n      </div>\n\n      <a class=\"main-menu-open-button\" href=\"#\" data-behavior=\"open-mobile-menu\"></a>\n    </div>\n  </div>\n</div>\n\n<body class=\"pytorch-body\">\n\n\n\n\n\n    <div class=\"table-of-contents-link-wrapper\">\n      <span>Table of Contents</span>\n      <a href=\"#\" class=\"toggle-table-of-contents\" data-behavior=\"toggle-table-of-contents\"></a>\n    </div>\n\n    <nav data-toggle=\"wy-nav-shift\" class=\"pytorch-left-menu\" id=\"pytorch-left-menu\">\n      <div class=\"pytorch-side-scroll\">\n        <div class=\"pytorch-menu pytorch-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"main navigation\">\n          <div class=\"pytorch-left-menu-search\">\n\n    <div class=\"version\">\n      <a href='https://pytorch.org/docs/versions.html'>2.5 &#x25BC</a>\n    </div>\n\n\n\n\n\n\n<div role=\"search\">\n  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../search.html\" method=\"get\">\n    <input type=\"text\" name=\"q\" placeholder=\"Search Docs\" />\n    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n    <input type=\"hidden\" name=\"area\" value=\"default\" />\n  </form>\n</div>\n\n          </div>\n\n\n\n\n\n\n\n\n\n              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Community</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../community/build_ci_governance.html\">PyTorch Governance | Build + CI</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../community/contribution_guide.html\">PyTorch Contribution Guide</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../community/design.html\">PyTorch Design Philosophy</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../community/governance.html\">PyTorch Governance | Mechanics</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../community/persons_of_interest.html\">PyTorch Governance | Maintainers</a></li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Developer Notes</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/amp_examples.html\">Automatic Mixed Precision examples</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/autograd.html\">Autograd mechanics</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/broadcasting.html\">Broadcasting semantics</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/cpu_threading_torchscript_inference.html\">CPU threading and TorchScript inference</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/cuda.html\">CUDA semantics</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/custom_operators.html\">PyTorch Custom Operators Landing Page</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/ddp.html\">Distributed Data Parallel</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/extending.html\">Extending PyTorch</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/extending.func.html\">Extending torch.func with autograd.Function</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/faq.html\">Frequently Asked Questions</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/fsdp.html\">FSDP Notes</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/get_start_xpu.html\">Getting Started on Intel GPU</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/gradcheck.html\">Gradcheck mechanics</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/hip.html\">HIP (ROCm) semantics</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/large_scale_deployments.html\">Features for large-scale deployments</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/modules.html\">Modules</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/mps.html\">MPS backend</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/multiprocessing.html\">Multiprocessing best practices</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/numerical_accuracy.html\">Numerical accuracy</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/randomness.html\">Reproducibility</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/serialization.html\">Serialization semantics</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../notes/windows.html\">Windows FAQ</a></li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Language Bindings</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../cpp_index.html\">C++</a></li>\n<li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/javadoc/\">Javadoc</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../deploy.html\">torch::deploy</a></li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n<ul class=\"current\">\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../torch.html\">torch</a></li>\n<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../nn.html\">torch.nn</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../nn.functional.html\">torch.nn.functional</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../tensors.html\">torch.Tensor</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../tensor_attributes.html\">Tensor Attributes</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../tensor_view.html\">Tensor Views</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../amp.html\">torch.amp</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../autograd.html\">torch.autograd</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../library.html\">torch.library</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../cpu.html\">torch.cpu</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../cuda.html\">torch.cuda</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../torch_cuda_memory.html\">Understanding CUDA Memory Usage</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../torch_cuda_memory.html#generating-a-snapshot\">Generating a Snapshot</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../torch_cuda_memory.html#using-the-visualizer\">Using the visualizer</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../torch_cuda_memory.html#snapshot-api-reference\">Snapshot API Reference</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../mps.html\">torch.mps</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../xpu.html\">torch.xpu</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../mtia.html\">torch.mtia</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../meta.html\">Meta device</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../backends.html\">torch.backends</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../export.html\">torch.export</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../distributed.html\">torch.distributed</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../distributed.tensor.html\">torch.distributed.tensor</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../distributed.algorithms.join.html\">torch.distributed.algorithms.join</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../distributed.elastic.html\">torch.distributed.elastic</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../fsdp.html\">torch.distributed.fsdp</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../distributed.tensor.parallel.html\">torch.distributed.tensor.parallel</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../distributed.optim.html\">torch.distributed.optim</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../distributed.pipelining.html\">torch.distributed.pipelining</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../distributed.checkpoint.html\">torch.distributed.checkpoint</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../distributions.html\">torch.distributions</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../torch.compiler.html\">torch.compiler</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../fft.html\">torch.fft</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../func.html\">torch.func</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../futures.html\">torch.futures</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../fx.html\">torch.fx</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../fx.experimental.html\">torch.fx.experimental</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../hub.html\">torch.hub</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../jit.html\">torch.jit</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../linalg.html\">torch.linalg</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../monitor.html\">torch.monitor</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../signal.html\">torch.signal</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../special.html\">torch.special</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../torch.overrides.html\">torch.overrides</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../package.html\">torch.package</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../profiler.html\">torch.profiler</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../nn.init.html\">torch.nn.init</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../nn.attention.html\">torch.nn.attention</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../onnx.html\">torch.onnx</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../optim.html\">torch.optim</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../complex_numbers.html\">Complex Numbers</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../ddp_comm_hooks.html\">DDP Communication Hooks</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../quantization.html\">Quantization</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../rpc.html\">Distributed RPC Framework</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../random.html\">torch.random</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../masked.html\">torch.masked</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../nested.html\">torch.nested</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../size.html\">torch.Size</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../sparse.html\">torch.sparse</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../storage.html\">torch.Storage</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../testing.html\">torch.testing</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../utils.html\">torch.utils</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../benchmark_utils.html\">torch.utils.benchmark</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../bottleneck.html\">torch.utils.bottleneck</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../checkpoint.html\">torch.utils.checkpoint</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../cpp_extension.html\">torch.utils.cpp_extension</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../data.html\">torch.utils.data</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../deterministic.html\">torch.utils.deterministic</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../jit_utils.html\">torch.utils.jit</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../dlpack.html\">torch.utils.dlpack</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../mobile_optimizer.html\">torch.utils.mobile_optimizer</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../model_zoo.html\">torch.utils.model_zoo</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../tensorboard.html\">torch.utils.tensorboard</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../module_tracker.html\">torch.utils.module_tracker</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../type_info.html\">Type Info</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../named_tensor.html\">Named Tensors</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../name_inference.html\">Named Tensors operator coverage</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../config_mod.html\">torch.__config__</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../future_mod.html\">torch.__future__</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../logging.html\">torch._logging</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../torch_environment_variables.html\">Torch Environment Variables</a></li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Libraries</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/audio/stable\">torchaudio</a></li>\n<li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/data\">TorchData</a></li>\n<li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/torchrec\">TorchRec</a></li>\n<li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/serve\">TorchServe</a></li>\n<li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/text/stable\">torchtext</a></li>\n<li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/vision/stable\">torchvision</a></li>\n<li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/xla/\">PyTorch on XLA Devices</a></li>\n</ul>\n\n\n\n\n        </div>\n      </div>\n    </nav>\n\n    <div class=\"pytorch-container\">\n      <div class=\"pytorch-page-level-bar\" id=\"pytorch-page-level-bar\">\n        <div class=\"pytorch-breadcrumbs-wrapper\">\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<div role=\"navigation\" aria-label=\"breadcrumbs navigation\">\n\n  <ul class=\"pytorch-breadcrumbs\">\n\n      <li>\n        <a href=\"../index.html\">\n\n            Docs\n\n        </a> &gt;\n      </li>\n\n\n          <li><a href=\"../nn.html\">torch.nn</a> &gt;</li>\n\n      <li>Module</li>\n\n\n      <li class=\"pytorch-breadcrumbs-aside\">\n\n\n            <a href=\"../_sources/generated/torch.nn.Module.rst.txt\" rel=\"nofollow\"><img src=\"../_static/images/view-page-source-icon.svg\"></a>\n\n\n      </li>\n\n  </ul>\n\n\n</div>\n        </div>\n\n        <div class=\"pytorch-shortcuts-wrapper\" id=\"pytorch-shortcuts-wrapper\">\n          Shortcuts\n        </div>\n      </div>\n\n      <section data-toggle=\"wy-nav-shift\" id=\"pytorch-content-wrap\" class=\"pytorch-content-wrap\">\n        <div class=\"pytorch-content-left\">\n\n\n\n          <!-- Google Tag Manager (noscript) -->\n          <noscript><iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS\"\n          height=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"></iframe></noscript>\n          <!-- End Google Tag Manager (noscript) -->\n\n          <div class=\"rst-content\">\n\n            <div role=\"main\" class=\"main-content\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n             <article itemprop=\"articleBody\" id=\"pytorch-article\" class=\"pytorch-article\">\n\n  <div class=\"section\" id=\"module\">\n<h1>Module<a class=\"headerlink\" href=\"#module\" title=\"Permalink to this heading\">\u00b6</a></h1>\n<dl class=\"py class\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module\">\n<em class=\"property\"><span class=\"pre\">class</span><span class=\"w\"> </span></em><span class=\"sig-prename descclassname\"><span class=\"pre\">torch.nn.</span></span><span class=\"sig-name descname\"><span class=\"pre\">Module</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span><span class=\"n\"><span class=\"pre\">args</span></span></em>, <em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">**</span></span><span class=\"n\"><span class=\"pre\">kwargs</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Base class for all neural network modules.</p>\n<p>Your models should also subclass this class.</p>\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"nn\">nn</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn.functional</span> <span class=\"k\">as</span> <span class=\"nn\">F</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">Model</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv1</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv2</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n        <span class=\"k\">return</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n</pre></div>\n</div>\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <a class=\"reference internal\" href=\"#torch.nn.Module.to\" title=\"torch.nn.Module.to\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">to()</span></code></a>, etc.</p>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>As per the example above, an <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__()</span></code> call to the parent class\nmust be made before assignment on the child.</p>\n</div>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Variables</dt>\n<dd class=\"field-odd\"><p><strong>training</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 Boolean represents whether this module is in training or\nevaluation mode.</p>\n</dd>\n</dl>\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.add_module\">\n<span class=\"sig-name descname\"><span class=\"pre\">add_module</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">name</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">module</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.add_module\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.add_module\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Add a child module to the current module.</p>\n<p>The module can be accessed as an attribute using the given name.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><ul class=\"simple\">\n<li><p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 name of the child module. The child module can be\naccessed from this module using the given name</p></li>\n<li><p><strong>module</strong> (<a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\"><em>Module</em></a>) \u2013 child module to be added to the module.</p></li>\n</ul>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.apply\">\n<span class=\"sig-name descname\"><span class=\"pre\">apply</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">fn</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.apply\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.apply\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Apply <code class=\"docutils literal notranslate\"><span class=\"pre\">fn</span></code> recursively to every submodule (as returned by <code class=\"docutils literal notranslate\"><span class=\"pre\">.children()</span></code>) as well as self.</p>\n<p>Typical use includes initializing the parameters of a model\n(see also <a class=\"reference internal\" href=\"../nn.init.html#nn-init-doc\"><span class=\"std std-ref\">torch.nn.init</span></a>).</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><p><strong>fn</strong> (<a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Module</span></code></a> -&gt; None) \u2013 function to be applied to each submodule</p>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p>self</p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\">Module</a></p>\n</dd>\n</dl>\n<p>Example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"nd\">@torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">()</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">def</span> <span class=\"nf\">init_weights</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">):</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">if</span> <span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">:</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">weight</span><span class=\"o\">.</span><span class=\"n\">fill_</span><span class=\"p\">(</span><span class=\"mf\">1.0</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">weight</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"n\">init_weights</span><span class=\"p\">)</span>\n<span class=\"go\">Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[1., 1.],</span>\n<span class=\"go\">        [1., 1.]], requires_grad=True)</span>\n<span class=\"go\">Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[1., 1.],</span>\n<span class=\"go\">        [1., 1.]], requires_grad=True)</span>\n<span class=\"go\">Sequential(</span>\n<span class=\"go\">  (0): Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">  (1): Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">)</span>\n</pre></div>\n</div>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.bfloat16\">\n<span class=\"sig-name descname\"><span class=\"pre\">bfloat16</span></span><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.bfloat16\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.bfloat16\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Casts all floating point parameters and buffers to <code class=\"docutils literal notranslate\"><span class=\"pre\">bfloat16</span></code> datatype.</p>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>This method modifies the module in-place.</p>\n</div>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Returns</dt>\n<dd class=\"field-odd\"><p>self</p>\n</dd>\n<dt class=\"field-even\">Return type</dt>\n<dd class=\"field-even\"><p><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\">Module</a></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.buffers\">\n<span class=\"sig-name descname\"><span class=\"pre\">buffers</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">recurse</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.buffers\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.buffers\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Return an iterator over module buffers.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><p><strong>recurse</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.</p>\n</dd>\n<dt class=\"field-even\">Yields</dt>\n<dd class=\"field-even\"><p><em>torch.Tensor</em> \u2013 module buffer</p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Iterator\" title=\"(in Python v3.13)\"><em>Iterator</em></a>[<a class=\"reference internal\" href=\"../tensors.html#torch.Tensor\" title=\"torch.Tensor\"><em>Tensor</em></a>]</p>\n</dd>\n</dl>\n<p>Example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">buf</span> <span class=\"ow\">in</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">buffers</span><span class=\"p\">():</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">buf</span><span class=\"p\">),</span> <span class=\"n\">buf</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">())</span>\n<span class=\"go\">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>\n<span class=\"go\">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>\n</pre></div>\n</div>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.children\">\n<span class=\"sig-name descname\"><span class=\"pre\">children</span></span><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.children\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.children\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Return an iterator over immediate children modules.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Yields</dt>\n<dd class=\"field-odd\"><p><em>Module</em> \u2013 a child module</p>\n</dd>\n<dt class=\"field-even\">Return type</dt>\n<dd class=\"field-even\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Iterator\" title=\"(in Python v3.13)\"><em>Iterator</em></a>[<a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.modules.module.Module\"><em>Module</em></a>]</p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.compile\">\n<span class=\"sig-name descname\"><span class=\"pre\">compile</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span><span class=\"n\"><span class=\"pre\">args</span></span></em>, <em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">**</span></span><span class=\"n\"><span class=\"pre\">kwargs</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.compile\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.compile\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Compile this Module\u2019s forward using <a class=\"reference internal\" href=\"torch.compile.html#torch.compile\" title=\"torch.compile\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">torch.compile()</span></code></a>.</p>\n<p>This Module\u2019s <cite>__call__</cite> method is compiled and all arguments are passed as-is\nto <a class=\"reference internal\" href=\"torch.compile.html#torch.compile\" title=\"torch.compile\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">torch.compile()</span></code></a>.</p>\n<p>See <a class=\"reference internal\" href=\"torch.compile.html#torch.compile\" title=\"torch.compile\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">torch.compile()</span></code></a> for details on the arguments for this function.</p>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.cpu\">\n<span class=\"sig-name descname\"><span class=\"pre\">cpu</span></span><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.cpu\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.cpu\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Move all model parameters and buffers to the CPU.</p>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>This method modifies the module in-place.</p>\n</div>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Returns</dt>\n<dd class=\"field-odd\"><p>self</p>\n</dd>\n<dt class=\"field-even\">Return type</dt>\n<dd class=\"field-even\"><p><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\">Module</a></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.cuda\">\n<span class=\"sig-name descname\"><span class=\"pre\">cuda</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">device</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.cuda\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.cuda\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Move all model parameters and buffers to the GPU.</p>\n<p>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on GPU while being optimized.</p>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>This method modifies the module in-place.</p>\n</div>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><p><strong>device</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.13)\"><em>int</em></a><em>, </em><em>optional</em>) \u2013 if specified, all parameters will be\ncopied to that device</p>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p>self</p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\">Module</a></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.double\">\n<span class=\"sig-name descname\"><span class=\"pre\">double</span></span><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.double\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.double\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Casts all floating point parameters and buffers to <code class=\"docutils literal notranslate\"><span class=\"pre\">double</span></code> datatype.</p>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>This method modifies the module in-place.</p>\n</div>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Returns</dt>\n<dd class=\"field-odd\"><p>self</p>\n</dd>\n<dt class=\"field-even\">Return type</dt>\n<dd class=\"field-even\"><p><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\">Module</a></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.eval\">\n<span class=\"sig-name descname\"><span class=\"pre\">eval</span></span><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.eval\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.eval\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Set the module in evaluation mode.</p>\n<p>This has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. <a class=\"reference internal\" href=\"torch.nn.Dropout.html#torch.nn.Dropout\" title=\"torch.nn.Dropout\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Dropout</span></code></a>, <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BatchNorm</span></code>,\netc.</p>\n<p>This is equivalent with <a class=\"reference internal\" href=\"#torch.nn.Module.train\" title=\"torch.nn.Module.train\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">self.train(False)</span></code></a>.</p>\n<p>See <a class=\"reference internal\" href=\"../notes/autograd.html#locally-disable-grad-doc\"><span class=\"std std-ref\">Locally disabling gradient computation</span></a> for a comparison between\n<cite>.eval()</cite> and several similar mechanisms that may be confused with it.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Returns</dt>\n<dd class=\"field-odd\"><p>self</p>\n</dd>\n<dt class=\"field-even\">Return type</dt>\n<dd class=\"field-even\"><p><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\">Module</a></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.extra_repr\">\n<span class=\"sig-name descname\"><span class=\"pre\">extra_repr</span></span><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.extra_repr\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.extra_repr\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Set the extra representation of the module.</p>\n<p>To print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\">str</a></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.float\">\n<span class=\"sig-name descname\"><span class=\"pre\">float</span></span><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.float\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.float\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Casts all floating point parameters and buffers to <code class=\"docutils literal notranslate\"><span class=\"pre\">float</span></code> datatype.</p>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>This method modifies the module in-place.</p>\n</div>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Returns</dt>\n<dd class=\"field-odd\"><p>self</p>\n</dd>\n<dt class=\"field-even\">Return type</dt>\n<dd class=\"field-even\"><p><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\">Module</a></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.forward\">\n<span class=\"sig-name descname\"><span class=\"pre\">forward</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span><span class=\"n\"><span class=\"pre\">input</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#torch.nn.Module.forward\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Define the computation performed at every call.</p>\n<p>Should be overridden by all subclasses.</p>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Module</span></code></a> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n</div>\n<dl class=\"field-list simple\">\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.get_buffer\">\n<span class=\"sig-name descname\"><span class=\"pre\">get_buffer</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">target</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.get_buffer\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.get_buffer\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Return the buffer given by <code class=\"docutils literal notranslate\"><span class=\"pre\">target</span></code> if it exists, otherwise throw an error.</p>\n<p>See the docstring for <code class=\"docutils literal notranslate\"><span class=\"pre\">get_submodule</span></code> for a more detailed\nexplanation of this method\u2019s functionality as well as how to\ncorrectly specify <code class=\"docutils literal notranslate\"><span class=\"pre\">target</span></code>.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><p><strong>target</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 The fully-qualified string name of the buffer\nto look for. (See <code class=\"docutils literal notranslate\"><span class=\"pre\">get_submodule</span></code> for how to specify a\nfully-qualified string.)</p>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p>The buffer referenced by <code class=\"docutils literal notranslate\"><span class=\"pre\">target</span></code></p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><a class=\"reference internal\" href=\"../tensors.html#torch.Tensor\" title=\"torch.Tensor\">torch.Tensor</a></p>\n</dd>\n<dt class=\"field-even\">Raises</dt>\n<dd class=\"field-even\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#AttributeError\" title=\"(in Python v3.13)\"><strong>AttributeError</strong></a> \u2013 If the target string references an invalid\n    path or resolves to something that is not a\n    buffer</p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.get_extra_state\">\n<span class=\"sig-name descname\"><span class=\"pre\">get_extra_state</span></span><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.get_extra_state\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.get_extra_state\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Return any extra state to include in the module\u2019s state_dict.</p>\n<p>Implement this and a corresponding <a class=\"reference internal\" href=\"#torch.nn.Module.set_extra_state\" title=\"torch.nn.Module.set_extra_state\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">set_extra_state()</span></code></a> for your module\nif you need to store extra state. This function is called when building the\nmodule\u2019s <cite>state_dict()</cite>.</p>\n<p>Note that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Returns</dt>\n<dd class=\"field-odd\"><p>Any extra state to store in the module\u2019s state_dict</p>\n</dd>\n<dt class=\"field-even\">Return type</dt>\n<dd class=\"field-even\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#object\" title=\"(in Python v3.13)\">object</a></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.get_parameter\">\n<span class=\"sig-name descname\"><span class=\"pre\">get_parameter</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">target</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.get_parameter\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.get_parameter\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Return the parameter given by <code class=\"docutils literal notranslate\"><span class=\"pre\">target</span></code> if it exists, otherwise throw an error.</p>\n<p>See the docstring for <code class=\"docutils literal notranslate\"><span class=\"pre\">get_submodule</span></code> for a more detailed\nexplanation of this method\u2019s functionality as well as how to\ncorrectly specify <code class=\"docutils literal notranslate\"><span class=\"pre\">target</span></code>.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><p><strong>target</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 The fully-qualified string name of the Parameter\nto look for. (See <code class=\"docutils literal notranslate\"><span class=\"pre\">get_submodule</span></code> for how to specify a\nfully-qualified string.)</p>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p>The Parameter referenced by <code class=\"docutils literal notranslate\"><span class=\"pre\">target</span></code></p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p>torch.nn.Parameter</p>\n</dd>\n<dt class=\"field-even\">Raises</dt>\n<dd class=\"field-even\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#AttributeError\" title=\"(in Python v3.13)\"><strong>AttributeError</strong></a> \u2013 If the target string references an invalid\n    path or resolves to something that is not an\n    <code class=\"docutils literal notranslate\"><span class=\"pre\">nn.Parameter</span></code></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.get_submodule\">\n<span class=\"sig-name descname\"><span class=\"pre\">get_submodule</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">target</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.get_submodule\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.get_submodule\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Return the submodule given by <code class=\"docutils literal notranslate\"><span class=\"pre\">target</span></code> if it exists, otherwise throw an error.</p>\n<p>For example, let\u2019s say you have an <code class=\"docutils literal notranslate\"><span class=\"pre\">nn.Module</span></code> <code class=\"docutils literal notranslate\"><span class=\"pre\">A</span></code> that\nlooks like this:</p>\n<div class=\"highlight-text notranslate\"><div class=\"highlight\"><pre><span></span>A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)\n</pre></div>\n</div>\n<p>(The diagram shows an <code class=\"docutils literal notranslate\"><span class=\"pre\">nn.Module</span></code> <code class=\"docutils literal notranslate\"><span class=\"pre\">A</span></code>. <code class=\"docutils literal notranslate\"><span class=\"pre\">A</span></code> has a nested\nsubmodule <code class=\"docutils literal notranslate\"><span class=\"pre\">net_b</span></code>, which itself has two submodules <code class=\"docutils literal notranslate\"><span class=\"pre\">net_c</span></code>\nand <code class=\"docutils literal notranslate\"><span class=\"pre\">linear</span></code>. <code class=\"docutils literal notranslate\"><span class=\"pre\">net_c</span></code> then has a submodule <code class=\"docutils literal notranslate\"><span class=\"pre\">conv</span></code>.)</p>\n<p>To check whether or not we have the <code class=\"docutils literal notranslate\"><span class=\"pre\">linear</span></code> submodule, we\nwould call <code class=\"docutils literal notranslate\"><span class=\"pre\">get_submodule(&quot;net_b.linear&quot;)</span></code>. To check whether\nwe have the <code class=\"docutils literal notranslate\"><span class=\"pre\">conv</span></code> submodule, we would call\n<code class=\"docutils literal notranslate\"><span class=\"pre\">get_submodule(&quot;net_b.net_c.conv&quot;)</span></code>.</p>\n<p>The runtime of <code class=\"docutils literal notranslate\"><span class=\"pre\">get_submodule</span></code> is bounded by the degree\nof module nesting in <code class=\"docutils literal notranslate\"><span class=\"pre\">target</span></code>. A query against\n<code class=\"docutils literal notranslate\"><span class=\"pre\">named_modules</span></code> achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, <code class=\"docutils literal notranslate\"><span class=\"pre\">get_submodule</span></code> should always be\nused.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><p><strong>target</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)</p>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p>The submodule referenced by <code class=\"docutils literal notranslate\"><span class=\"pre\">target</span></code></p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\">torch.nn.Module</a></p>\n</dd>\n<dt class=\"field-even\">Raises</dt>\n<dd class=\"field-even\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#AttributeError\" title=\"(in Python v3.13)\"><strong>AttributeError</strong></a> \u2013 If the target string references an invalid\n    path or resolves to something that is not an\n    <code class=\"docutils literal notranslate\"><span class=\"pre\">nn.Module</span></code></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.half\">\n<span class=\"sig-name descname\"><span class=\"pre\">half</span></span><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.half\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.half\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Casts all floating point parameters and buffers to <code class=\"docutils literal notranslate\"><span class=\"pre\">half</span></code> datatype.</p>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>This method modifies the module in-place.</p>\n</div>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Returns</dt>\n<dd class=\"field-odd\"><p>self</p>\n</dd>\n<dt class=\"field-even\">Return type</dt>\n<dd class=\"field-even\"><p><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\">Module</a></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.ipu\">\n<span class=\"sig-name descname\"><span class=\"pre\">ipu</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">device</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.ipu\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.ipu\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Move all model parameters and buffers to the IPU.</p>\n<p>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on IPU while being optimized.</p>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>This method modifies the module in-place.</p>\n</div>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><p><strong>device</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.13)\"><em>int</em></a><em>, </em><em>optional</em>) \u2013 if specified, all parameters will be\ncopied to that device</p>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p>self</p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\">Module</a></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.load_state_dict\">\n<span class=\"sig-name descname\"><span class=\"pre\">load_state_dict</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">state_dict</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">strict</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">assign</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.load_state_dict\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.load_state_dict\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Copy parameters and buffers from <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">state_dict</span></code></a> into this module and its descendants.</p>\n<p>If <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">strict</span></code> is <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code>, then\nthe keys of <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">state_dict</span></code></a> must exactly match the keys returned\nby this module\u2019s <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">state_dict()</span></code></a> function.</p>\n<div class=\"admonition warning\">\n<p class=\"admonition-title\">Warning</p>\n<p>If <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">assign</span></code> is <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code> the optimizer must be created after\nthe call to <a class=\"reference internal\" href=\"#torch.nn.Module.load_state_dict\" title=\"torch.nn.Module.load_state_dict\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">load_state_dict</span></code></a> unless\n<a class=\"reference internal\" href=\"../future_mod.html#torch.__future__.get_swap_module_params_on_conversion\" title=\"torch.__future__.get_swap_module_params_on_conversion\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">get_swap_module_params_on_conversion()</span></code></a> is <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code>.</p>\n</div>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><ul class=\"simple\">\n<li><p><strong>state_dict</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.13)\"><em>dict</em></a>) \u2013 a dict containing parameters and\npersistent buffers.</p></li>\n<li><p><strong>strict</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a><em>, </em><em>optional</em>) \u2013 whether to strictly enforce that the keys\nin <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">state_dict</span></code></a> match the keys returned by this module\u2019s\n<a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">state_dict()</span></code></a> function. Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code></p></li>\n<li><p><strong>assign</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a><em>, </em><em>optional</em>) \u2013 When <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code>, the properties of the tensors\nin the current module are preserved while when <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code>, the\nproperties of the Tensors in the state dict are preserved. The only\nexception is the <code class=\"docutils literal notranslate\"><span class=\"pre\">requires_grad</span></code> field of <code class=\"xref py py-class docutils literal notranslate\">\n<span class=\"pre\">Default:</span> <span class=\"pre\">``False`</span></code></p></li>\n</ul>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p><ul class=\"simple\">\n<li><dl class=\"simple\">\n<dt><strong>missing_keys</strong> is a list of str containing any keys that are expected</dt><dd><p>by this module but missing from the provided <code class=\"docutils literal notranslate\"><span class=\"pre\">state_dict</span></code>.</p>\n</dd>\n</dl>\n</li>\n<li><dl class=\"simple\">\n<dt><strong>unexpected_keys</strong> is a list of str containing the keys that are not</dt><dd><p>expected by this module but present in the provided <code class=\"docutils literal notranslate\"><span class=\"pre\">state_dict</span></code>.</p>\n</dd>\n</dl>\n</li>\n</ul>\n</p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">NamedTuple</span></code> with <code class=\"docutils literal notranslate\"><span class=\"pre\">missing_keys</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">unexpected_keys</span></code> fields</p>\n</dd>\n</dl>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>If a parameter or buffer is registered as <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> and its corresponding key\nexists in <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">state_dict</span></code></a>, <a class=\"reference internal\" href=\"#torch.nn.Module.load_state_dict\" title=\"torch.nn.Module.load_state_dict\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">load_state_dict()</span></code></a> will raise a\n<code class=\"docutils literal notranslate\"><span class=\"pre\">RuntimeError</span></code>.</p>\n</div>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.modules\">\n<span class=\"sig-name descname\"><span class=\"pre\">modules</span></span><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.modules\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.modules\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Return an iterator over all modules in the network.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Yields</dt>\n<dd class=\"field-odd\"><p><em>Module</em> \u2013 a module in the network</p>\n</dd>\n<dt class=\"field-even\">Return type</dt>\n<dd class=\"field-even\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Iterator\" title=\"(in Python v3.13)\"><em>Iterator</em></a>[<a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.modules.module.Module\"><em>Module</em></a>]</p>\n</dd>\n</dl>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>Duplicate modules are returned only once. In the following\nexample, <code class=\"docutils literal notranslate\"><span class=\"pre\">l</span></code> will be returned only once.</p>\n</div>\n<p>Example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">l</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">,</span> <span class=\"n\">l</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"n\">m</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"p\">()):</span>\n<span class=\"gp\">... </span>    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&gt;&#39;</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">)</span>\n\n<span class=\"go\">0 -&gt; Sequential(</span>\n<span class=\"go\">  (0): Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">  (1): Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">)</span>\n<span class=\"go\">1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>\n</pre></div>\n</div>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.mtia\">\n<span class=\"sig-name descname\"><span class=\"pre\">mtia</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">device</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.mtia\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.mtia\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Move all model parameters and buffers to the MTIA.</p>\n<p>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on MTIA while being optimized.</p>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>This method modifies the module in-place.</p>\n</div>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><p><strong>device</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.13)\"><em>int</em></a><em>, </em><em>optional</em>) \u2013 if specified, all parameters will be\ncopied to that device</p>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p>self</p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\">Module</a></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.named_buffers\">\n<span class=\"sig-name descname\"><span class=\"pre\">named_buffers</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prefix</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">recurse</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">remove_duplicate</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.named_buffers\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.named_buffers\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><ul class=\"simple\">\n<li><p><strong>prefix</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 prefix to prepend to all buffer names.</p></li>\n<li><p><strong>recurse</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a><em>, </em><em>optional</em>) \u2013 if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.</p></li>\n<li><p><strong>remove_duplicate</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a><em>, </em><em>optional</em>) \u2013 whether to remove the duplicated buffers in the result. Defaults to True.</p></li>\n</ul>\n</dd>\n<dt class=\"field-even\">Yields</dt>\n<dd class=\"field-even\"><p><em>(str, torch.Tensor)</em> \u2013 Tuple containing the name and buffer</p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Iterator\" title=\"(in Python v3.13)\"><em>Iterator</em></a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Tuple\" title=\"(in Python v3.13)\"><em>Tuple</em></a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\">str</a>, <a class=\"reference internal\" href=\"../tensors.html#torch.Tensor\" title=\"torch.Tensor\"><em>Tensor</em></a>]]</p>\n</dd>\n</dl>\n<p>Example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">buf</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">named_buffers</span><span class=\"p\">():</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">if</span> <span class=\"n\">name</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">&#39;running_var&#39;</span><span class=\"p\">]:</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">buf</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">())</span>\n</pre></div>\n</div>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.named_children\">\n<span class=\"sig-name descname\"><span class=\"pre\">named_children</span></span><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.named_children\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.named_children\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Yields</dt>\n<dd class=\"field-odd\"><p><em>(str, Module)</em> \u2013 Tuple containing a name and child module</p>\n</dd>\n<dt class=\"field-even\">Return type</dt>\n<dd class=\"field-even\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Iterator\" title=\"(in Python v3.13)\"><em>Iterator</em></a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Tuple\" title=\"(in Python v3.13)\"><em>Tuple</em></a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\">str</a>, <a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.modules.module.Module\"><em>Module</em></a>]]</p>\n</dd>\n</dl>\n<p>Example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">module</span> <span class=\"ow\">in</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">named_children</span><span class=\"p\">():</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">if</span> <span class=\"n\">name</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">&#39;conv4&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;conv5&#39;</span><span class=\"p\">]:</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">)</span>\n</pre></div>\n</div>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.named_modules\">\n<span class=\"sig-name descname\"><span class=\"pre\">named_modules</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">memo</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prefix</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">remove_duplicate</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.named_modules\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.named_modules\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><ul class=\"simple\">\n<li><p><strong>memo</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Optional\" title=\"(in Python v3.13)\"><em>Optional</em></a><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Set\" title=\"(in Python v3.13)\"><em>Set</em></a><em>[</em><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.modules.module.Module\"><em>Module</em></a><em>]</em><em>]</em>) \u2013 a memo to store the set of modules already added to the result</p></li>\n<li><p><strong>prefix</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 a prefix that will be added to the name of the module</p></li>\n<li><p><strong>remove_duplicate</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 whether to remove the duplicated module instances in the result\nor not</p></li>\n</ul>\n</dd>\n<dt class=\"field-even\">Yields</dt>\n<dd class=\"field-even\"><p><em>(str, Module)</em> \u2013 Tuple of name and module</p>\n</dd>\n</dl>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>Duplicate modules are returned only once. In the following\nexample, <code class=\"docutils literal notranslate\"><span class=\"pre\">l</span></code> will be returned only once.</p>\n</div>\n<p>Example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">l</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">,</span> <span class=\"n\">l</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"n\">m</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">named_modules</span><span class=\"p\">()):</span>\n<span class=\"gp\">... </span>    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&gt;&#39;</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">)</span>\n\n<span class=\"go\">0 -&gt; (&#39;&#39;, Sequential(</span>\n<span class=\"go\">  (0): Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">  (1): Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"go\">))</span>\n<span class=\"go\">1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>\n</pre></div>\n</div>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.named_parameters\">\n<span class=\"sig-name descname\"><span class=\"pre\">named_parameters</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prefix</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">recurse</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">remove_duplicate</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.named_parameters\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.named_parameters\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><ul class=\"simple\">\n<li><p><strong>prefix</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 prefix to prepend to all parameter names.</p></li>\n<li><p><strong>recurse</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.</p></li>\n<li><p><strong>remove_duplicate</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a><em>, </em><em>optional</em>) \u2013 whether to remove the duplicated\nparameters in the result. Defaults to True.</p></li>\n</ul>\n</dd>\n<dt class=\"field-even\">Yields</dt>\n<dd class=\"field-even\"><p><em>(str, Parameter)</em> \u2013 Tuple containing the name and parameter</p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Iterator\" title=\"(in Python v3.13)\"><em>Iterator</em></a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Tuple\" title=\"(in Python v3.13)\"><em>Tuple</em></a>[<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\">str</a>, <a class=\"reference internal\" href=\"torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter\" title=\"torch.nn.parameter.Parameter\"><em>Parameter</em></a>]]</p>\n</dd>\n</dl>\n<p>Example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">param</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">named_parameters</span><span class=\"p\">():</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">if</span> <span class=\"n\">name</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">&#39;bias&#39;</span><span class=\"p\">]:</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">param</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">())</span>\n</pre></div>\n</div>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.parameters\">\n<span class=\"sig-name descname\"><span class=\"pre\">parameters</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">recurse</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.parameters\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.parameters\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Return an iterator over module parameters.</p>\n<p>This is typically passed to an optimizer.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><p><strong>recurse</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.</p>\n</dd>\n<dt class=\"field-even\">Yields</dt>\n<dd class=\"field-even\"><p><em>Parameter</em> \u2013 module parameter</p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Iterator\" title=\"(in Python v3.13)\"><em>Iterator</em></a>[<a class=\"reference internal\" href=\"torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter\" title=\"torch.nn.parameter.Parameter\"><em>Parameter</em></a>]</p>\n</dd>\n</dl>\n<p>Example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">param</span> <span class=\"ow\">in</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">():</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">param</span><span class=\"p\">),</span> <span class=\"n\">param</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">())</span>\n<span class=\"go\">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>\n<span class=\"go\">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>\n</pre></div>\n</div>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_backward_hook\">\n<span class=\"sig-name descname\"><span class=\"pre\">register_backward_hook</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">hook</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_backward_hook\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_backward_hook\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Register a backward hook on the module.</p>\n<p>This function is deprecated in favor of <a class=\"reference internal\" href=\"#torch.nn.Module.register_full_backward_hook\" title=\"torch.nn.Module.register_full_backward_hook\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">register_full_backward_hook()</span></code></a> and\nthe behavior of this function will change in future versions.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Returns</dt>\n<dd class=\"field-odd\"><p>a handle that can be used to remove the added hook by calling\n<code class=\"docutils literal notranslate\"><span class=\"pre\">handle.remove()</span></code></p>\n</dd>\n<dt class=\"field-even\">Return type</dt>\n<dd class=\"field-even\"><p><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.utils.hooks.RemovableHandle</span></code></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_buffer\">\n<span class=\"sig-name descname\"><span class=\"pre\">register_buffer</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">name</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">tensor</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">persistent</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_buffer\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_buffer\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Add a buffer to the module.</p>\n<p>This is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm\u2019s <code class=\"docutils literal notranslate\"><span class=\"pre\">running_mean</span></code>\nis not a parameter, but is part of the module\u2019s state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">persistent</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code>. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module\u2019s\n<a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">state_dict</span></code></a>.</p>\n<p>Buffers can be accessed as attributes using given names.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><ul class=\"simple\">\n<li><p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 name of the buffer. The buffer can be accessed\nfrom this module using the given name</p></li>\n<li><p><strong>tensor</strong> (<a class=\"reference internal\" href=\"../tensors.html#torch.Tensor\" title=\"torch.Tensor\"><em>Tensor</em></a><em> or </em><em>None</em>) \u2013 buffer to be registered. If <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>, then operations\nthat run on buffers, such as <a class=\"reference internal\" href=\"#torch.nn.Module.cuda\" title=\"torch.nn.Module.cuda\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">cuda</span></code></a>, are ignored. If <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>,\nthe buffer is <strong>not</strong> included in the module\u2019s <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">state_dict</span></code></a>.</p></li>\n<li><p><strong>persistent</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 whether the buffer is part of this module\u2019s\n<a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">state_dict</span></code></a>.</p></li>\n</ul>\n</dd>\n</dl>\n<p>Example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">register_buffer</span><span class=\"p\">(</span><span class=\"s1\">&#39;running_mean&#39;</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">num_features</span><span class=\"p\">))</span>\n</pre></div>\n</div>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_forward_hook\">\n<span class=\"sig-name descname\"><span class=\"pre\">register_forward_hook</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">hook</span></span></em>, <em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prepend</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">with_kwargs</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">always_call</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_forward_hook\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_forward_hook\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Register a forward hook on the module.</p>\n<p>The hook will be called every time after <a class=\"reference internal\" href=\"#torch.nn.Module.forward\" title=\"torch.nn.Module.forward\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">forward()</span></code></a> has computed an output.</p>\n<p>If <code class=\"docutils literal notranslate\"><span class=\"pre\">with_kwargs</span></code> is <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code> or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won\u2019t be\npassed to the hooks and only to the <code class=\"docutils literal notranslate\"><span class=\"pre\">forward</span></code>. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after <a class=\"reference internal\" href=\"#torch.nn.Module.forward\" title=\"torch.nn.Module.forward\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">forward()</span></code></a> is called. The hook\nshould have the following signature:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">hook</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span> <span class=\"ow\">or</span> <span class=\"n\">modified</span> <span class=\"n\">output</span>\n</pre></div>\n</div>\n<p>If <code class=\"docutils literal notranslate\"><span class=\"pre\">with_kwargs</span></code> is <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code>, the forward hook will be passed the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">kwargs</span></code> given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">hook</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">kwargs</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span> <span class=\"ow\">or</span> <span class=\"n\">modified</span> <span class=\"n\">output</span>\n</pre></div>\n</div>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><ul class=\"simple\">\n<li><p><strong>hook</strong> (<em>Callable</em>) \u2013 The user defined hook to be registered.</p></li>\n<li><p><strong>prepend</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code>, the provided <code class=\"docutils literal notranslate\"><span class=\"pre\">hook</span></code> will be fired\nbefore all existing <code class=\"docutils literal notranslate\"><span class=\"pre\">forward</span></code> hooks on this\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.nn.modules.Module</span></code>. Otherwise, the provided\n<code class=\"docutils literal notranslate\"><span class=\"pre\">hook</span></code> will be fired after all existing <code class=\"docutils literal notranslate\"><span class=\"pre\">forward</span></code> hooks on\nthis <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.nn.modules.Module</span></code>. Note that global\n<code class=\"docutils literal notranslate\"><span class=\"pre\">forward</span></code> hooks registered with\n<code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">register_module_forward_hook()</span></code> will fire before all hooks\nregistered by this method.\nDefault: <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code></p></li>\n<li><p><strong>with_kwargs</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code>, the <code class=\"docutils literal notranslate\"><span class=\"pre\">hook</span></code> will be passed the\nkwargs given to the forward function.\nDefault: <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code></p></li>\n<li><p><strong>always_call</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code> the <code class=\"docutils literal notranslate\"><span class=\"pre\">hook</span></code> will be run regardless of\nwhether an exception is raised while calling the Module.\nDefault: <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code></p></li>\n</ul>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p>a handle that can be used to remove the added hook by calling\n<code class=\"docutils literal notranslate\"><span class=\"pre\">handle.remove()</span></code></p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.utils.hooks.RemovableHandle</span></code></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_forward_pre_hook\">\n<span class=\"sig-name descname\"><span class=\"pre\">register_forward_pre_hook</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">hook</span></span></em>, <em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prepend</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">with_kwargs</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_forward_pre_hook\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_forward_pre_hook\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Register a forward pre-hook on the module.</p>\n<p>The hook will be called every time before <a class=\"reference internal\" href=\"#torch.nn.Module.forward\" title=\"torch.nn.Module.forward\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">forward()</span></code></a> is invoked.</p>\n<p>If <code class=\"docutils literal notranslate\"><span class=\"pre\">with_kwargs</span></code> is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won\u2019t be\npassed to the hooks and only to the <code class=\"docutils literal notranslate\"><span class=\"pre\">forward</span></code>. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">hook</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span> <span class=\"ow\">or</span> <span class=\"n\">modified</span> <span class=\"nb\">input</span>\n</pre></div>\n</div>\n<p>If <code class=\"docutils literal notranslate\"><span class=\"pre\">with_kwargs</span></code> is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">hook</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">kwargs</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span> <span class=\"ow\">or</span> <span class=\"n\">a</span> <span class=\"nb\">tuple</span> <span class=\"n\">of</span> <span class=\"n\">modified</span> <span class=\"nb\">input</span> <span class=\"ow\">and</span> <span class=\"n\">kwargs</span>\n</pre></div>\n</div>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><ul class=\"simple\">\n<li><p><strong>hook</strong> (<em>Callable</em>) \u2013 The user defined hook to be registered.</p></li>\n<li><p><strong>prepend</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If true, the provided <code class=\"docutils literal notranslate\"><span class=\"pre\">hook</span></code> will be fired before\nall existing <code class=\"docutils literal notranslate\"><span class=\"pre\">forward_pre</span></code> hooks on this\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.nn.modules.Module</span></code>. Otherwise, the provided\n<code class=\"docutils literal notranslate\"><span class=\"pre\">hook</span></code> will be fired after all existing <code class=\"docutils literal notranslate\"><span class=\"pre\">forward_pre</span></code> hooks\non this <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.nn.modules.Module</span></code>. Note that global\n<code class=\"docutils literal notranslate\"><span class=\"pre\">forward_pre</span></code> hooks registered with\n<code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">register_module_forward_pre_hook()</span></code> will fire before all\nhooks registered by this method.\nDefault: <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code></p></li>\n<li><p><strong>with_kwargs</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If true, the <code class=\"docutils literal notranslate\"><span class=\"pre\">hook</span></code> will be passed the kwargs\ngiven to the forward function.\nDefault: <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code></p></li>\n</ul>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p>a handle that can be used to remove the added hook by calling\n<code class=\"docutils literal notranslate\"><span class=\"pre\">handle.remove()</span></code></p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.utils.hooks.RemovableHandle</span></code></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_full_backward_hook\">\n<span class=\"sig-name descname\"><span class=\"pre\">register_full_backward_hook</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">hook</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prepend</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_full_backward_hook\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_full_backward_hook\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Register a backward hook on the module.</p>\n<p>The hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">hook</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">grad_input</span><span class=\"p\">,</span> <span class=\"n\">grad_output</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">tuple</span><span class=\"p\">(</span><span class=\"n\">Tensor</span><span class=\"p\">)</span> <span class=\"ow\">or</span> <span class=\"kc\">None</span>\n</pre></div>\n</div>\n<p>The <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">grad_input</span></code> and <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">grad_output</span></code> are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">grad_input</span></code> in\nsubsequent computations. <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">grad_input</span></code> will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">grad_input</span></code> and <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">grad_output</span></code> will be <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> for all non-Tensor\narguments.</p>\n<p>For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module\u2019s forward function.</p>\n<div class=\"admonition warning\">\n<p class=\"admonition-title\">Warning</p>\n<p>Modifying inputs or outputs inplace is not allowed when using backward hooks and\nwill raise an error.</p>\n</div>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><ul class=\"simple\">\n<li><p><strong>hook</strong> (<em>Callable</em>) \u2013 The user-defined hook to be registered.</p></li>\n<li><p><strong>prepend</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If true, the provided <code class=\"docutils literal notranslate\"><span class=\"pre\">hook</span></code> will be fired before\nall existing <code class=\"docutils literal notranslate\"><span class=\"pre\">backward</span></code> hooks on this\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.nn.modules.Module</span></code>. Otherwise, the provided\n<code class=\"docutils literal notranslate\"><span class=\"pre\">hook</span></code> will be fired after all existing <code class=\"docutils literal notranslate\"><span class=\"pre\">backward</span></code> hooks on\nthis <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.nn.modules.Module</span></code>. Note that global\n<code class=\"docutils literal notranslate\"><span class=\"pre\">backward</span></code> hooks registered with\n<code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">register_module_full_backward_hook()</span></code> will fire before\nall hooks registered by this method.</p></li>\n</ul>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p>a handle that can be used to remove the added hook by calling\n<code class=\"docutils literal notranslate\"><span class=\"pre\">handle.remove()</span></code></p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.utils.hooks.RemovableHandle</span></code></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_full_backward_pre_hook\">\n<span class=\"sig-name descname\"><span class=\"pre\">register_full_backward_pre_hook</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">hook</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prepend</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_full_backward_pre_hook\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_full_backward_pre_hook\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Register a backward pre-hook on the module.</p>\n<p>The hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">hook</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">grad_output</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"ow\">or</span> <span class=\"kc\">None</span>\n</pre></div>\n</div>\n<p>The <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">grad_output</span></code> is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">grad_output</span></code> in\nsubsequent computations. Entries in <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">grad_output</span></code> will be <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> for\nall non-Tensor arguments.</p>\n<p>For technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module\u2019s forward function.</p>\n<div class=\"admonition warning\">\n<p class=\"admonition-title\">Warning</p>\n<p>Modifying inputs inplace is not allowed when using backward hooks and\nwill raise an error.</p>\n</div>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><ul class=\"simple\">\n<li><p><strong>hook</strong> (<em>Callable</em>) \u2013 The user-defined hook to be registered.</p></li>\n<li><p><strong>prepend</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 If true, the provided <code class=\"docutils literal notranslate\"><span class=\"pre\">hook</span></code> will be fired before\nall existing <code class=\"docutils literal notranslate\"><span class=\"pre\">backward_pre</span></code> hooks on this\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.nn.modules.Module</span></code>. Otherwise, the provided\n<code class=\"docutils literal notranslate\"><span class=\"pre\">hook</span></code> will be fired after all existing <code class=\"docutils literal notranslate\"><span class=\"pre\">backward_pre</span></code> hooks\non this <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.nn.modules.Module</span></code>. Note that global\n<code class=\"docutils literal notranslate\"><span class=\"pre\">backward_pre</span></code> hooks registered with\n<code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">register_module_full_backward_pre_hook()</span></code> will fire before\nall hooks registered by this method.</p></li>\n</ul>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p>a handle that can be used to remove the added hook by calling\n<code class=\"docutils literal notranslate\"><span class=\"pre\">handle.remove()</span></code></p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.utils.hooks.RemovableHandle</span></code></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_load_state_dict_post_hook\">\n<span class=\"sig-name descname\"><span class=\"pre\">register_load_state_dict_post_hook</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">hook</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_load_state_dict_post_hook\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_load_state_dict_post_hook\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Register a post-hook to be run after module\u2019s <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">load_state_dict()</span></code> is called.</p>\n<dl class=\"simple\">\n<dt>It should have the following signature::</dt><dd><p>hook(module, incompatible_keys) -&gt; None</p>\n</dd>\n</dl>\n<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">module</span></code> argument is the current module that this hook is registered\non, and the <code class=\"docutils literal notranslate\"><span class=\"pre\">incompatible_keys</span></code> argument is a <code class=\"docutils literal notranslate\"><span class=\"pre\">NamedTuple</span></code> consisting\nof attributes <code class=\"docutils literal notranslate\"><span class=\"pre\">missing_keys</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">unexpected_keys</span></code>. <code class=\"docutils literal notranslate\"><span class=\"pre\">missing_keys</span></code>\nis a <code class=\"docutils literal notranslate\"><span class=\"pre\">list</span></code> of <code class=\"docutils literal notranslate\"><span class=\"pre\">str</span></code> containing the missing keys and\n<code class=\"docutils literal notranslate\"><span class=\"pre\">unexpected_keys</span></code> is a <code class=\"docutils literal notranslate\"><span class=\"pre\">list</span></code> of <code class=\"docutils literal notranslate\"><span class=\"pre\">str</span></code> containing the unexpected keys.</p>\n<p>The given incompatible_keys can be modified inplace if needed.</p>\n<p>Note that the checks performed when calling <a class=\"reference internal\" href=\"#torch.nn.Module.load_state_dict\" title=\"torch.nn.Module.load_state_dict\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">load_state_dict()</span></code></a> with\n<code class=\"docutils literal notranslate\"><span class=\"pre\">strict=True</span></code> are affected by modifications the hook makes to\n<code class=\"docutils literal notranslate\"><span class=\"pre\">missing_keys</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">unexpected_keys</span></code>, as expected. Additions to either\nset of keys will result in an error being thrown when <code class=\"docutils literal notranslate\"><span class=\"pre\">strict=True</span></code>, and\nclearing out both missing and unexpected keys will avoid an error.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Returns</dt>\n<dd class=\"field-odd\"><p>a handle that can be used to remove the added hook by calling\n<code class=\"docutils literal notranslate\"><span class=\"pre\">handle.remove()</span></code></p>\n</dd>\n<dt class=\"field-even\">Return type</dt>\n<dd class=\"field-even\"><p><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.utils.hooks.RemovableHandle</span></code></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_load_state_dict_pre_hook\">\n<span class=\"sig-name descname\"><span class=\"pre\">register_load_state_dict_pre_hook</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">hook</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_load_state_dict_pre_hook\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_load_state_dict_pre_hook\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Register a pre-hook to be run before module\u2019s <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">load_state_dict()</span></code> is called.</p>\n<dl class=\"simple\">\n<dt>It should have the following signature::</dt><dd><p>hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950</p>\n</dd>\n</dl>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><p><strong>hook</strong> (<em>Callable</em>) \u2013 Callable hook that will be invoked before\nloading the state dict.</p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_module\">\n<span class=\"sig-name descname\"><span class=\"pre\">register_module</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">name</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">module</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_module\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_module\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Alias for <a class=\"reference internal\" href=\"#torch.nn.Module.add_module\" title=\"torch.nn.Module.add_module\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">add_module()</span></code></a>.</p>\n<dl class=\"field-list simple\">\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_parameter\">\n<span class=\"sig-name descname\"><span class=\"pre\">register_parameter</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">name</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">param</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_parameter\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_parameter\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Add a parameter to the module.</p>\n<p>The parameter can be accessed as an attribute using given name.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><ul class=\"simple\">\n<li><p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 name of the parameter. The parameter can be accessed\nfrom this module using the given name</p></li>\n<li><p><strong>param</strong> (<a class=\"reference internal\" href=\"torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter\" title=\"torch.nn.parameter.Parameter\"><em>Parameter</em></a><em> or </em><em>None</em>) \u2013 parameter to be added to the module. If\n<code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>, then operations that run on parameters, such as <a class=\"reference internal\" href=\"#torch.nn.Module.cuda\" title=\"torch.nn.Module.cuda\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">cuda</span></code></a>,\nare ignored. If <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>, the parameter is <strong>not</strong> included in the\nmodule\u2019s <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">state_dict</span></code></a>.</p></li>\n</ul>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_state_dict_post_hook\">\n<span class=\"sig-name descname\"><span class=\"pre\">register_state_dict_post_hook</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">hook</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_state_dict_post_hook\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_state_dict_post_hook\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Register a post-hook for the <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">state_dict()</span></code></a> method.</p>\n<dl class=\"simple\">\n<dt>It should have the following signature::</dt><dd><p>hook(module, state_dict, prefix, local_metadata) -&gt; None</p>\n</dd>\n</dl>\n<p>The registered hooks can modify the <code class=\"docutils literal notranslate\"><span class=\"pre\">state_dict</span></code> inplace.</p>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.register_state_dict_pre_hook\">\n<span class=\"sig-name descname\"><span class=\"pre\">register_state_dict_pre_hook</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">hook</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.register_state_dict_pre_hook\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.register_state_dict_pre_hook\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Register a pre-hook for the <a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\" title=\"torch.nn.Module.state_dict\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">state_dict()</span></code></a> method.</p>\n<dl class=\"simple\">\n<dt>It should have the following signature::</dt><dd><p>hook(module, prefix, keep_vars) -&gt; None</p>\n</dd>\n</dl>\n<p>The registered hooks can be used to perform pre-processing before the <code class=\"docutils literal notranslate\"><span class=\"pre\">state_dict</span></code>\ncall is made.</p>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.requires_grad_\">\n<span class=\"sig-name descname\"><span class=\"pre\">requires_grad_</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">requires_grad</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.requires_grad_\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.requires_grad_\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Change if autograd should record operations on parameters in this module.</p>\n<p>This method sets the parameters\u2019 <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">requires_grad</span></code> attributes\nin-place.</p>\n<p>This method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).</p>\n<p>See <a class=\"reference internal\" href=\"../notes/autograd.html#locally-disable-grad-doc\"><span class=\"std std-ref\">Locally disabling gradient computation</span></a> for a comparison between\n<cite>.requires_grad_()</cite> and several similar mechanisms that may be confused with it.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><p><strong>requires_grad</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 whether autograd should record operations on\nparameters in this module. Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code>.</p>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p>self</p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\">Module</a></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.set_extra_state\">\n<span class=\"sig-name descname\"><span class=\"pre\">set_extra_state</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">state</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.set_extra_state\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.set_extra_state\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Set extra state contained in the loaded <cite>state_dict</cite>.</p>\n<p>This function is called from <a class=\"reference internal\" href=\"#torch.nn.Module.load_state_dict\" title=\"torch.nn.Module.load_state_dict\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">load_state_dict()</span></code></a> to handle any extra state\nfound within the <cite>state_dict</cite>. Implement this function and a corresponding\n<a class=\"reference internal\" href=\"#torch.nn.Module.get_extra_state\" title=\"torch.nn.Module.get_extra_state\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">get_extra_state()</span></code></a> for your module if you need to store extra state within its\n<cite>state_dict</cite>.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><p><strong>state</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.13)\"><em>dict</em></a>) \u2013 Extra state from the <cite>state_dict</cite></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.set_submodule\">\n<span class=\"sig-name descname\"><span class=\"pre\">set_submodule</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">target</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">module</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.set_submodule\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.set_submodule\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Set the submodule given by <code class=\"docutils literal notranslate\"><span class=\"pre\">target</span></code> if it exists, otherwise throw an error.</p>\n<p>For example, let\u2019s say you have an <code class=\"docutils literal notranslate\"><span class=\"pre\">nn.Module</span></code> <code class=\"docutils literal notranslate\"><span class=\"pre\">A</span></code> that\nlooks like this:</p>\n<div class=\"highlight-text notranslate\"><div class=\"highlight\"><pre><span></span>A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)\n</pre></div>\n</div>\n<p>(The diagram shows an <code class=\"docutils literal notranslate\"><span class=\"pre\">nn.Module</span></code> <code class=\"docutils literal notranslate\"><span class=\"pre\">A</span></code>. <code class=\"docutils literal notranslate\"><span class=\"pre\">A</span></code> has a nested\nsubmodule <code class=\"docutils literal notranslate\"><span class=\"pre\">net_b</span></code>, which itself has two submodules <code class=\"docutils literal notranslate\"><span class=\"pre\">net_c</span></code>\nand <code class=\"docutils literal notranslate\"><span class=\"pre\">linear</span></code>. <code class=\"docutils literal notranslate\"><span class=\"pre\">net_c</span></code> then has a submodule <code class=\"docutils literal notranslate\"><span class=\"pre\">conv</span></code>.)</p>\n<p>To overide the <code class=\"docutils literal notranslate\"><span class=\"pre\">Conv2d</span></code> with a new submodule <code class=\"docutils literal notranslate\"><span class=\"pre\">Linear</span></code>, you\nwould call\n<code class=\"docutils literal notranslate\"><span class=\"pre\">set_submodule(&quot;net_b.net_c.conv&quot;,</span> <span class=\"pre\">nn.Linear(33,</span> <span class=\"pre\">16))</span></code>.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><ul class=\"simple\">\n<li><p><strong>target</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a>) \u2013 The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)</p></li>\n<li><p><strong>module</strong> (<a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.modules.module.Module\"><em>Module</em></a>) \u2013 The module to set the submodule to.</p></li>\n</ul>\n</dd>\n<dt class=\"field-even\">Raises</dt>\n<dd class=\"field-even\"><ul class=\"simple\">\n<li><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#ValueError\" title=\"(in Python v3.13)\"><strong>ValueError</strong></a> \u2013 If the target string is empty</p></li>\n<li><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#AttributeError\" title=\"(in Python v3.13)\"><strong>AttributeError</strong></a> \u2013 If the target string references an invalid\n    path or resolves to something that is not an\n    <code class=\"docutils literal notranslate\"><span class=\"pre\">nn.Module</span></code></p></li>\n</ul>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.share_memory\">\n<span class=\"sig-name descname\"><span class=\"pre\">share_memory</span></span><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.share_memory\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.share_memory\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>See <a class=\"reference internal\" href=\"torch.Tensor.share_memory_.html#torch.Tensor.share_memory_\" title=\"torch.Tensor.share_memory_\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">torch.Tensor.share_memory_()</span></code></a>.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><em>T</em></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.state_dict\">\n<span class=\"sig-name descname\"><span class=\"pre\">state_dict</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">destination</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"> </span><span class=\"n\"><span class=\"pre\">T_destination</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prefix</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"> </span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><span class=\"pre\">str</span></a></span><span class=\"w\"> </span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"> </span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">keep_vars</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"> </span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><span class=\"pre\">bool</span></a></span><span class=\"w\"> </span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"> </span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span> <span class=\"sig-return\"><span class=\"sig-return-icon\">&#x2192;</span> <span class=\"sig-return-typehint\"><span class=\"pre\">T_destination</span></span></span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.state_dict\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.state_dict\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dt class=\"sig sig-object py\">\n<span class=\"sig-name descname\"><span class=\"pre\">state_dict</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prefix</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"> </span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><span class=\"pre\">str</span></a></span><span class=\"w\"> </span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"> </span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">keep_vars</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"> </span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><span class=\"pre\">bool</span></a></span><span class=\"w\"> </span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"> </span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span> <span class=\"sig-return\"><span class=\"sig-return-icon\">&#x2192;</span> <span class=\"sig-return-typehint\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Dict\" title=\"(in Python v3.13)\"><span class=\"pre\">Dict</span></a><span class=\"p\"><span class=\"pre\">[</span></span><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><span class=\"pre\">str</span></a><span class=\"p\"><span class=\"pre\">,</span></span><span class=\"w\"> </span><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Any\" title=\"(in Python v3.13)\"><span class=\"pre\">Any</span></a><span class=\"p\"><span class=\"pre\">]</span></span></span></span></dt>\n<dd><p>Return a dictionary containing references to the whole state of the module.</p>\n<p>Both parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> are not included.</p>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>The returned object is a shallow copy. It contains references\nto the module\u2019s parameters and buffers.</p>\n</div>\n<div class=\"admonition warning\">\n<p class=\"admonition-title\">Warning</p>\n<p>Currently <code class=\"docutils literal notranslate\"><span class=\"pre\">state_dict()</span></code> also accepts positional arguments for\n<code class=\"docutils literal notranslate\"><span class=\"pre\">destination</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">prefix</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">keep_vars</span></code> in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.</p>\n</div>\n<div class=\"admonition warning\">\n<p class=\"admonition-title\">Warning</p>\n<p>Please avoid the use of argument <code class=\"docutils literal notranslate\"><span class=\"pre\">destination</span></code> as it is not\ndesigned for end-users.</p>\n</div>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><ul class=\"simple\">\n<li><p><strong>destination</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.13)\"><em>dict</em></a><em>, </em><em>optional</em>) \u2013 If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an <code class=\"docutils literal notranslate\"><span class=\"pre\">OrderedDict</span></code> will be created and returned.\nDefault: <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>.</p></li>\n<li><p><strong>prefix</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><em>str</em></a><em>, </em><em>optional</em>) \u2013 a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">''</span></code>.</p></li>\n<li><p><strong>keep_vars</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a><em>, </em><em>optional</em>) \u2013 by default the <a class=\"reference internal\" href=\"../tensors.html#torch.Tensor\" title=\"torch.Tensor\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Tensor</span></code></a> s\nreturned in the state dict are detached from autograd. If it\u2019s\nset to <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code>, detaching will not be performed.\nDefault: <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code>.</p></li>\n</ul>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p>a dictionary containing a whole state of the module</p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.13)\">dict</a></p>\n</dd>\n</dl>\n<p>Example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">state_dict</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">()</span>\n<span class=\"go\">[&#39;bias&#39;, &#39;weight&#39;]</span>\n</pre></div>\n</div>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.to\">\n<span class=\"sig-name descname\"><span class=\"pre\">to</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">device</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"> </span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Optional\" title=\"(in Python v3.13)\"><span class=\"pre\">Optional</span></a><span class=\"p\"><span class=\"pre\">[</span></span><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Union\" title=\"(in Python v3.13)\"><span class=\"pre\">Union</span></a><span class=\"p\"><span class=\"pre\">[</span></span><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.13)\"><span class=\"pre\">str</span></a><span class=\"p\"><span class=\"pre\">,</span></span><span class=\"w\"> </span><a class=\"reference internal\" href=\"../tensor_attributes.html#torch.device\" title=\"torch.device\"><span class=\"pre\">device</span></a><span class=\"p\"><span class=\"pre\">,</span></span><span class=\"w\"> </span><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.13)\"><span class=\"pre\">int</span></a><span class=\"p\"><span class=\"pre\">]</span></span><span class=\"p\"><span class=\"pre\">]</span></span></span><span class=\"w\"> </span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"> </span><span class=\"default_value\"><span class=\"pre\">...</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">dtype</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"> </span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Optional\" title=\"(in Python v3.13)\"><span class=\"pre\">Optional</span></a><span class=\"p\"><span class=\"pre\">[</span></span><a class=\"reference internal\" href=\"../tensor_attributes.html#torch.dtype\" title=\"torch.dtype\"><span class=\"pre\">dtype</span></a><span class=\"p\"><span class=\"pre\">]</span></span></span><span class=\"w\"> </span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"> </span><span class=\"default_value\"><span class=\"pre\">...</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">non_blocking</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"> </span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><span class=\"pre\">bool</span></a></span><span class=\"w\"> </span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"> </span><span class=\"default_value\"><span class=\"pre\">...</span></span></em><span class=\"sig-paren\">)</span> <span class=\"sig-return\"><span class=\"sig-return-icon\">&#x2192;</span> <span class=\"sig-return-typehint\"><span class=\"pre\">Self</span></span></span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.to\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.to\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dt class=\"sig sig-object py\">\n<span class=\"sig-name descname\"><span class=\"pre\">to</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">dtype</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"> </span><span class=\"n\"><a class=\"reference internal\" href=\"../tensor_attributes.html#torch.dtype\" title=\"torch.dtype\"><span class=\"pre\">dtype</span></a></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">non_blocking</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"> </span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><span class=\"pre\">bool</span></a></span><span class=\"w\"> </span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"> </span><span class=\"default_value\"><span class=\"pre\">...</span></span></em><span class=\"sig-paren\">)</span> <span class=\"sig-return\"><span class=\"sig-return-icon\">&#x2192;</span> <span class=\"sig-return-typehint\"><span class=\"pre\">Self</span></span></span></dt>\n<dt class=\"sig sig-object py\">\n<span class=\"sig-name descname\"><span class=\"pre\">to</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">tensor</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"> </span><span class=\"n\"><a class=\"reference internal\" href=\"../tensors.html#torch.Tensor\" title=\"torch.Tensor\"><span class=\"pre\">Tensor</span></a></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">non_blocking</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"> </span><span class=\"n\"><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><span class=\"pre\">bool</span></a></span><span class=\"w\"> </span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"> </span><span class=\"default_value\"><span class=\"pre\">...</span></span></em><span class=\"sig-paren\">)</span> <span class=\"sig-return\"><span class=\"sig-return-icon\">&#x2192;</span> <span class=\"sig-return-typehint\"><span class=\"pre\">Self</span></span></span></dt>\n<dd><p>Move and/or cast the parameters and buffers.</p>\n<p>This can be called as</p>\n<dl class=\"py function\">\n<dt class=\"sig sig-object py\">\n<span class=\"sig-name descname\"><span class=\"pre\">to</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">device</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">dtype</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">non_blocking</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.to\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a></dt>\n<dd></dd></dl>\n\n<dl class=\"py function\">\n<dt class=\"sig sig-object py\">\n<span class=\"sig-name descname\"><span class=\"pre\">to</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">dtype</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">non_blocking</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.to\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a></dt>\n<dd></dd></dl>\n\n<dl class=\"py function\">\n<dt class=\"sig sig-object py\">\n<span class=\"sig-name descname\"><span class=\"pre\">to</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">tensor</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">non_blocking</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.to\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a></dt>\n<dd></dd></dl>\n\n<dl class=\"py function\">\n<dt class=\"sig sig-object py\">\n<span class=\"sig-name descname\"><span class=\"pre\">to</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">memory_format</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">torch.channels_last</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.to\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a></dt>\n<dd></dd></dl>\n\n<p>Its signature is similar to <a class=\"reference internal\" href=\"torch.Tensor.to.html#torch.Tensor.to\" title=\"torch.Tensor.to\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">torch.Tensor.to()</span></code></a>, but only accepts\nfloating point or complex <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">dtype</span></code>s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">dtype</span></code>\n(if given). The integral parameters and buffers will be moved\n<code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">device</span></code>, if that is given, but with dtypes unchanged. When\n<code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">non_blocking</span></code> is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.</p>\n<p>See below for examples.</p>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>This method modifies the module in-place.</p>\n</div>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><ul class=\"simple\">\n<li><p><strong>device</strong> (<a class=\"reference internal\" href=\"../tensor_attributes.html#torch.device\" title=\"torch.device\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.device</span></code></a>) \u2013 the desired device of the parameters\nand buffers in this module</p></li>\n<li><p><strong>dtype</strong> (<a class=\"reference internal\" href=\"../tensor_attributes.html#torch.dtype\" title=\"torch.dtype\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.dtype</span></code></a>) \u2013 the desired floating point or complex dtype of\nthe parameters and buffers in this module</p></li>\n<li><p><strong>tensor</strong> (<a class=\"reference internal\" href=\"../tensors.html#torch.Tensor\" title=\"torch.Tensor\"><em>torch.Tensor</em></a>) \u2013 Tensor whose dtype and device are the desired\ndtype and device for all parameters and buffers in this module</p></li>\n<li><p><strong>memory_format</strong> (<a class=\"reference internal\" href=\"../tensor_attributes.html#torch.memory_format\" title=\"torch.memory_format\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.memory_format</span></code></a>) \u2013 the desired memory\nformat for 4D parameters and buffers in this module (keyword\nonly argument)</p></li>\n</ul>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p>self</p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\">Module</a></p>\n</dd>\n</dl>\n<p>Examples:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">weight</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[ 0.1913, -0.3420],</span>\n<span class=\"go\">        [-0.5113, -0.2325]])</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">double</span><span class=\"p\">)</span>\n<span class=\"go\">Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">weight</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[ 0.1913, -0.3420],</span>\n<span class=\"go\">        [-0.5113, -0.2325]], dtype=torch.float64)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">gpu1</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:1&quot;</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">gpu1</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">half</span><span class=\"p\">,</span> <span class=\"n\">non_blocking</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"go\">Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">weight</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[ 0.1914, -0.3420],</span>\n<span class=\"go\">        [-0.5112, -0.2324]], dtype=torch.float16, device=&#39;cuda:1&#39;)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">cpu</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cpu&quot;</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">cpu</span><span class=\"p\">)</span>\n<span class=\"go\">Linear(in_features=2, out_features=2, bias=True)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">weight</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[ 0.1914, -0.3420],</span>\n<span class=\"go\">        [-0.5112, -0.2324]], dtype=torch.float16)</span>\n\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cdouble</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">weight</span>\n<span class=\"go\">Parameter containing:</span>\n<span class=\"go\">tensor([[ 0.3741+0.j,  0.2382+0.j],</span>\n<span class=\"go\">        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cdouble</span><span class=\"p\">))</span>\n<span class=\"go\">tensor([[0.6122+0.j, 0.1150+0.j],</span>\n<span class=\"go\">        [0.6122+0.j, 0.1150+0.j],</span>\n<span class=\"go\">        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</span>\n</pre></div>\n</div>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.to_empty\">\n<span class=\"sig-name descname\"><span class=\"pre\">to_empty</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">device</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">recurse</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.to_empty\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.to_empty\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Move the parameters and buffers to the specified device without copying storage.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><ul class=\"simple\">\n<li><p><strong>device</strong> (<a class=\"reference internal\" href=\"../tensor_attributes.html#torch.device\" title=\"torch.device\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.device</span></code></a>) \u2013 The desired device of the parameters\nand buffers in this module.</p></li>\n<li><p><strong>recurse</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 Whether parameters and buffers of submodules should\nbe recursively moved to the specified device.</p></li>\n</ul>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p>self</p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\">Module</a></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.train\">\n<span class=\"sig-name descname\"><span class=\"pre\">train</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">mode</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.train\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.train\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Set the module in training mode.</p>\n<p>This has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. <a class=\"reference internal\" href=\"torch.nn.Dropout.html#torch.nn.Dropout\" title=\"torch.nn.Dropout\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Dropout</span></code></a>, <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BatchNorm</span></code>,\netc.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><p><strong>mode</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 whether to set training mode (<code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code>) or evaluation\nmode (<code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code>). Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code>.</p>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p>self</p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\">Module</a></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.type\">\n<span class=\"sig-name descname\"><span class=\"pre\">type</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">dst_type</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.type\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.type\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Casts all parameters and buffers to <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">dst_type</span></code>.</p>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>This method modifies the module in-place.</p>\n</div>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><p><strong>dst_type</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#type\" title=\"(in Python v3.13)\"><em>type</em></a><em> or </em><em>string</em>) \u2013 the desired type</p>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p>self</p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\">Module</a></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.xpu\">\n<span class=\"sig-name descname\"><span class=\"pre\">xpu</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">device</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.xpu\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.xpu\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Move all model parameters and buffers to the XPU.</p>\n<p>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.</p>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>This method modifies the module in-place.</p>\n</div>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><p><strong>device</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.13)\"><em>int</em></a><em>, </em><em>optional</em>) \u2013 if specified, all parameters will be\ncopied to that device</p>\n</dd>\n<dt class=\"field-even\">Returns</dt>\n<dd class=\"field-even\"><p>self</p>\n</dd>\n<dt class=\"field-odd\">Return type</dt>\n<dd class=\"field-odd\"><p><a class=\"reference internal\" href=\"#torch.nn.Module\" title=\"torch.nn.Module\">Module</a></p>\n</dd>\n</dl>\n</dd></dl>\n\n<dl class=\"py method\">\n<dt class=\"sig sig-object py\" id=\"torch.nn.Module.zero_grad\">\n<span class=\"sig-name descname\"><span class=\"pre\">zero_grad</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">set_to_none</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"../_modules/torch/nn/modules/module.html#Module.zero_grad\"><span class=\"viewcode-link\"><span class=\"pre\">[source]</span></span></a><a class=\"headerlink\" href=\"#torch.nn.Module.zero_grad\" title=\"Permalink to this definition\">\u00b6</a></dt>\n<dd><p>Reset gradients of all model parameters.</p>\n<p>See similar function under <a class=\"reference internal\" href=\"../optim.html#torch.optim.Optimizer\" title=\"torch.optim.Optimizer\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.optim.Optimizer</span></code></a> for more context.</p>\n<dl class=\"field-list simple\">\n<dt class=\"field-odd\">Parameters</dt>\n<dd class=\"field-odd\"><p><strong>set_to_none</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.13)\"><em>bool</em></a>) \u2013 instead of setting to zero, set the grads to None.\nSee <a class=\"reference internal\" href=\"torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad\" title=\"torch.optim.Optimizer.zero_grad\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">torch.optim.Optimizer.zero_grad()</span></code></a> for details.</p>\n</dd>\n</dl>\n</dd></dl>\n\n</dd></dl>\n\n</div>\n\n\n             </article>\n\n            </div>\n            <footer>\n\n    <div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"footer navigation\">\n\n        <a href=\"torch.nn.Sequential.html\" class=\"btn btn-neutral float-right\" title=\"Sequential\" accesskey=\"n\" rel=\"next\">Next <img src=\"../_static/images/chevron-right-orange.svg\" class=\"next-page\"></a>\n\n\n        <a href=\"torch.nn.parameter.UninitializedBuffer.html\" class=\"btn btn-neutral\" title=\"UninitializedBuffer\" accesskey=\"p\" rel=\"prev\"><img src=\"../_static/images/chevron-right-orange.svg\" class=\"previous-page\"> Previous</a>\n\n    </div>\n\n\n\n\n    <hr>\n\n\n\n  <div role=\"contentinfo\">\n    <p>\n        &copy; Copyright 2023, PyTorch Contributors.\n\n    </p>\n  </div>\n\n      <div>\n        Built with <a href=\"http://sphinx-doc.org/\">Sphinx</a> using a <a href=\"https://github.com/rtfd/sphinx_rtd_theme\">theme</a> provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n      </div>\n\n\n</footer>\n\n          </div>\n<script>\n\nvar match = window.location.href.match(/\\/_[a-zA-Z0-9_]*.html|_dynamo/gi);\nvar url = window.location.href.lastIndexOf(match[match.length-1]);\n\nif (url)\n  {\n    var div = '<div class=\"admonition note\"><p class=\"admonition-title\">Note</p><p><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'\n    document.getElementById(\"pytorch-article\").insertAdjacentHTML('afterBegin', div)\n  }\n</script>\n        </div>\n\n        <div class=\"pytorch-content-right\" id=\"pytorch-content-right\">\n          <div class=\"pytorch-right-menu\" id=\"pytorch-right-menu\">\n            <div class=\"pytorch-side-scroll\" id=\"pytorch-side-scroll-right\">\n              <ul>\n<li><a class=\"reference internal\" href=\"#\">Module</a><ul>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module</span></code></a><ul>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.add_module\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.add_module()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.apply\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.apply()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.bfloat16\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.bfloat16()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.buffers\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.buffers()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.children\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.children()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.compile\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.compile()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.cpu\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.cpu()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.cuda\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.cuda()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.double\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.double()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.eval\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.eval()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.extra_repr\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.extra_repr()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.float\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.float()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.forward\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.forward()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.get_buffer\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.get_buffer()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.get_extra_state\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.get_extra_state()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.get_parameter\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.get_parameter()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.get_submodule\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.get_submodule()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.half\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.half()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.ipu\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.ipu()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.load_state_dict\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.load_state_dict()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.modules\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.modules()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.mtia\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.mtia()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.named_buffers\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.named_buffers()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.named_children\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.named_children()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.named_modules\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.named_modules()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.named_parameters\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.named_parameters()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.parameters\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.parameters()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.register_backward_hook\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.register_backward_hook()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.register_buffer\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.register_buffer()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.register_forward_hook\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.register_forward_hook()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.register_forward_pre_hook\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.register_forward_pre_hook()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.register_full_backward_hook\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.register_full_backward_hook()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.register_full_backward_pre_hook\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.register_full_backward_pre_hook()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.register_load_state_dict_post_hook\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.register_load_state_dict_post_hook()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.register_load_state_dict_pre_hook\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.register_load_state_dict_pre_hook()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.register_module\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.register_module()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.register_parameter\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.register_parameter()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.register_state_dict_post_hook\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.register_state_dict_post_hook()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.register_state_dict_pre_hook\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.register_state_dict_pre_hook()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.requires_grad_\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.requires_grad_()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.set_extra_state\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.set_extra_state()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.set_submodule\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.set_submodule()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.share_memory\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.share_memory()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.state_dict\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.state_dict()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.to\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.to()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.to_empty\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.to_empty()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.train\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.train()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.type\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.type()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.xpu\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.xpu()</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#torch.nn.Module.zero_grad\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Module.zero_grad()</span></code></a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n\n            </div>\n          </div>\n        </div>\n      </section>\n    </div>\n\n\n\n\n\n\n\n       <script type=\"text/javascript\" id=\"documentation_options\" data-url_root=\"../\" src=\"../_static/documentation_options.js\"></script>\n         <script data-url_root=\"../\" id=\"documentation_options\" src=\"../_static/documentation_options.js\"></script>\n         <script src=\"../_static/jquery.js\"></script>\n         <script src=\"../_static/underscore.js\"></script>\n         <script src=\"../_static/_sphinx_javascript_frameworks_compat.js\"></script>\n         <script src=\"../_static/doctools.js\"></script>\n         <script src=\"../_static/sphinx_highlight.js\"></script>\n         <script src=\"../_static/clipboard.min.js\"></script>\n         <script src=\"../_static/copybutton.js\"></script>\n\n\n\n\n  <script type=\"text/javascript\" src=\"../_static/js/vendor/popper.min.js\"></script>\n  <script type=\"text/javascript\" src=\"../_static/js/vendor/bootstrap.min.js\"></script>\n  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js\"></script>\n  <script type=\"text/javascript\" src=\"../_static/js/theme.js\"></script>\n\n  <script type=\"text/javascript\">\n      jQuery(function () {\n          SphinxRtdTheme.Navigation.enable(true);\n      });\n  </script>\n\n<script script type=\"text/javascript\">\n  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];\n</script>\n\n<img height=\"1\" width=\"1\" style=\"border-style:none;\" alt=\"\" src=\"https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0\"/>\n\n\n  <!-- Begin Footer -->\n\n  <div class=\"container-fluid docs-tutorials-resources\" id=\"docs-tutorials-resources\">\n    <div class=\"container\">\n      <div class=\"row\">\n        <div class=\"col-md-4 text-center\">\n          <h2>Docs</h2>\n          <p>Access comprehensive developer documentation for PyTorch</p>\n          <a class=\"with-right-arrow\" href=\"https://pytorch.org/docs/stable/index.html\">View Docs</a>\n        </div>\n\n        <div class=\"col-md-4 text-center\">\n          <h2>Tutorials</h2>\n          <p>Get in-depth tutorials for beginners and advanced developers</p>\n          <a class=\"with-right-arrow\" href=\"https://pytorch.org/tutorials\">View Tutorials</a>\n        </div>\n\n        <div class=\"col-md-4 text-center\">\n          <h2>Resources</h2>\n          <p>Find development resources and get your questions answered</p>\n          <a class=\"with-right-arrow\" href=\"https://pytorch.org/resources\">View Resources</a>\n        </div>\n      </div>\n    </div>\n  </div>\n\n  <footer class=\"site-footer\">\n    <div class=\"container footer-container\">\n      <div class=\"footer-logo-wrapper\">\n        <a href=\"https://pytorch.org/\" class=\"footer-logo\"></a>\n      </div>\n\n      <div class=\"footer-links-wrapper\">\n        <div class=\"footer-links-col\">\n          <ul>\n            <li class=\"list-title\"><a href=\"https://pytorch.org/\">PyTorch</a></li>\n            <li><a href=\"https://pytorch.org/get-started\">Get Started</a></li>\n            <li><a href=\"https://pytorch.org/features\">Features</a></li>\n            <li><a href=\"https://pytorch.org/ecosystem\">Ecosystem</a></li>\n            <li><a href=\"https://pytorch.org/blog/\">Blog</a></li>\n            <li><a href=\"https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md\">Contributing</a></li>\n          </ul>\n        </div>\n\n        <div class=\"footer-links-col\">\n          <ul>\n            <li class=\"list-title\"><a href=\"https://pytorch.org/resources\">Resources</a></li>\n            <li><a href=\"https://pytorch.org/tutorials\">Tutorials</a></li>\n            <li><a href=\"https://pytorch.org/docs/stable/index.html\">Docs</a></li>\n            <li><a href=\"https://discuss.pytorch.org\" target=\"_blank\">Discuss</a></li>\n            <li><a href=\"https://github.com/pytorch/pytorch/issues\" target=\"_blank\">Github Issues</a></li>\n            <li><a href=\"https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf\" target=\"_blank\">Brand Guidelines</a></li>\n          </ul>\n        </div>\n\n        <div class=\"footer-links-col\">\n          <ul>\n            <li class=\"list-title\">Stay up to date</li>\n            <li><a href=\"https://www.facebook.com/pytorch\" target=\"_blank\">Facebook</a></li>\n            <li><a href=\"https://twitter.com/pytorch\" target=\"_blank\">Twitter</a></li>\n            <li><a href=\"https://www.youtube.com/pytorch\" target=\"_blank\">YouTube</a></li>\n            <li><a href=\"https://www.linkedin.com/company/pytorch\" target=\"_blank\">LinkedIn</a></li>\n          </ul>\n          </div>\n\n        <div class=\"footer-links-col\">\n          <ul>\n            <li class=\"list-title\">PyTorch Podcasts</li>\n            <li><a href=\"https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5\" target=\"_blank\">Spotify</a></li>\n            <li><a href=\"https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008\" target=\"_blank\">Apple</a></li>\n            <li><a href=\"https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D\" target=\"_blank\">Google</a></li>\n            <li><a href=\"https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?\" target=\"_blank\">Amazon</a></li>\n          </ul>\n         </div>\n        </div>\n\n        <div class=\"privacy-policy\">\n          <ul>\n            <li class=\"privacy-policy-links\"><a href=\"https://www.linuxfoundation.org/terms/\" target=\"_blank\">Terms</a></li>\n            <li class=\"privacy-policy-links\">|</li>\n            <li class=\"privacy-policy-links\"><a href=\"https://www.linuxfoundation.org/privacy-policy/\" target=\"_blank\">Privacy</a></li>\n          </ul>\n        </div>\n        <div class=\"copyright\">\n        <p>\u00a9 Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.\n          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see\n          <a href=\"https://www.linuxfoundation.org/policies/\">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source\n          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,\n          please see <a href=\"https://www.lfprojects.org/policies/\">www.lfprojects.org/policies/</a>.</p>\n      </div>\n     </div>\n\n  </footer>\n\n  <div class=\"cookie-banner-wrapper\">\n  <div class=\"container\">\n    <p class=\"gdpr-notice\">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook\u2019s Cookies Policy applies. Learn more, including about available controls: <a href=\"https://www.facebook.com/policies/cookies/\">Cookies Policy</a>.</p>\n    <img class=\"close-button\" src=\"../_static/images/pytorch-x.svg\">\n  </div>\n</div>\n\n  <!-- End Footer -->\n\n  <!-- Begin Mobile Menu -->\n\n  <div class=\"mobile-main-menu\">\n    <div class=\"container-fluid\">\n      <div class=\"container\">\n        <div class=\"mobile-main-menu-header-container\">\n          <a class=\"header-logo\" href=\"https://pytorch.org/\" aria-label=\"PyTorch\"></a>\n          <a class=\"main-menu-close-button\" href=\"#\" data-behavior=\"close-mobile-menu\"></a>\n        </div>\n      </div>\n    </div>\n\n    <div class=\"mobile-main-menu-links-container\">\n      <div class=\"main-menu\">\n        <ul>\n           <li class=\"resources-mobile-menu-title\">\n             <a>Learn</a>\n           </li>\n           <ul class=\"resources-mobile-menu-items\">\n             <li>\n               <a href=\"https://pytorch.org/get-started\">Get Started</a>\n             </li>\n             <li>\n               <a href=\"https://pytorch.org/tutorials\">Tutorials</a>\n             </li>\n             <li>\n               <a href=\"https://pytorch.org/tutorials/beginner/basics/intro.html\">Learn the Basics</a>\n             </li>\n             <li>\n               <a href=\"https://pytorch.org/tutorials/recipes/recipes_index.html\">PyTorch Recipes</a>\n             </li>\n             <li>\n               <a href=\"https://pytorch.org/tutorials/beginner/introyt.html\">Introduction to PyTorch - YouTube Series</a>\n             </li>\n           </ul>\n           <li class=\"resources-mobile-menu-title\">\n             <a>Ecosystem</a>\n           </li>\n           <ul class=\"resources-mobile-menu-items\">\n             <li>\n               <a href=\"https://pytorch.org/ecosystem\">Tools</a>\n             </li>\n             <li>\n               <a href=\"https://pytorch.org/#community-module\">Community</a>\n             </li>\n             <li>\n               <a href=\"https://discuss.pytorch.org/\">Forums</a>\n             </li>\n             <li>\n               <a href=\"https://pytorch.org/resources\">Developer Resources</a>\n             </li>\n             <li>\n               <a href=\"https://pytorch.org/ecosystem/contributor-awards-2023\">Contributor Awards - 2023</a>\n             </li>\n           </ul>\n\n           <li class=\"resources-mobile-menu-title\">\n             <a>Edge</a>\n           </li>\n\n           <ul class=\"resources-mobile-menu-items\">\n             <li>\n               <a href=\"https://pytorch.org/edge\">About PyTorch Edge</a>\n             </li>\n\n             <li>\n               <a href=\"https://pytorch.org/executorch-overview\">ExecuTorch</a>\n             </li>\n           </ul>\n\n           <li class=\"resources-mobile-menu-title\">\n             <a>Docs</a>\n           </li>\n\n           <ul class=\"resources-mobile-menu-items\">\n            <li>\n              <a href=\"https://pytorch.org/docs/stable/index.html\">PyTorch</a>\n            </li>\n\n            <li>\n              <a href=\"https://pytorch.org/pytorch-domains\">PyTorch Domains</a>\n            </li>\n          </ul>\n\n          <li class=\"resources-mobile-menu-title\">\n            <a>Blog & News</a>\n          </li>\n\n           <ul class=\"resources-mobile-menu-items\">\n            <li>\n              <a href=\"https://pytorch.org/blog/\">PyTorch Blog</a>\n            </li>\n            <li>\n              <a href=\"https://pytorch.org/community-blog\">Community Blog</a>\n            </li>\n\n            <li>\n              <a href=\"https://pytorch.org/videos\">Videos</a>\n            </li>\n\n            <li>\n              <a href=\"https://pytorch.org/community-stories\">Community Stories</a>\n            </li>\n            <li>\n              <a href=\"https://pytorch.org/events\">Events</a>\n            </li>\n          </ul>\n\n          <li class=\"resources-mobile-menu-title\">\n            <a>About</a>\n          </li>\n\n          <ul class=\"resources-mobile-menu-items\">\n            <li>\n              <a href=\"https://pytorch.org/foundation\">PyTorch Foundation</a>\n            </li>\n            <li>\n              <a href=\"https://pytorch.org/governing-board\">Governing Board</a>\n            </li>\n          </ul>\n        </ul>\n      </div>\n    </div>\n  </div>\n\n  <!-- End Mobile Menu -->\n\n  <script type=\"text/javascript\" src=\"../_static/js/vendor/anchor.min.js\"></script>\n\n  <script type=\"text/javascript\">\n    $(document).ready(function() {\n      mobileMenu.bind();\n      mobileTOC.bind();\n      pytorchAnchors.bind();\n      sideMenus.bind();\n      scrollToAnchor.bind();\n      highlightNavigation.bind();\n      mainMenuDropdown.bind();\n      filterTags.bind();\n\n      // Add class to links that have code blocks, since we cannot create links in code blocks\n      $(\"article.pytorch-article a span.pre\").each(function(e) {\n        $(this).closest(\"a\").addClass(\"has-code\");\n      });\n    })\n  </script>\n</body>\n</html>\n"}
