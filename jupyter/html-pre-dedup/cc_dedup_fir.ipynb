{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from xinghe.spark import *\n",
    "from app.common.json_util import *\n",
    "from xinghe.s3 import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import re\n",
    "import hashlib\n",
    "from lxml.etree import HTML\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "config = {\n",
    "    \"spark_conf_name\": \"spark_4\",\n",
    "    \"skip_success_check\": True,\n",
    "}\n",
    "\n",
    "\n",
    "MAX_OUTPUT_ROW_SIZE = 1024 * 1024 * 1024 * 1.5\n",
    "DUMPS = [\n",
    "    ...\n",
    "]\n",
    "\n",
    "ERROR_PATH = \"s3://xxx/\"\n",
    "CC_WARC = 's3://xxx/'\n",
    "output_path = \"s3://xxx/\"\n",
    "spark = new_spark_session(\"cc_dumps.dedup.fir\", config)\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# html source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 cc warc path list\n",
    "warc_paths = []\n",
    "for dump in DUMPS:\n",
    "    dump_path = f'{CC_WARC}{dump}/'\n",
    "    warc_paths.extend([x for x in list(list_s3_objects(dump_path, recursive=True)) if \"/warc/\" in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def html_to_content(html_str: str, url: str) -> str:\n",
    "    if html_str.strip() and isinstance(html_str,str):\n",
    "        html_str = re.sub(r'<\\?[^>]*\\?>', '', html_str.strip())\n",
    "        try:\n",
    "            html_etree = HTML(html_str)\n",
    "        except:\n",
    "            return None\n",
    "        if html_etree:\n",
    "            for element in html_etree.xpath('//*[self::script or self::style]'):\n",
    "                element.getparent().remove(element)\n",
    "            text = ''.join(html_etree.xpath(\"//text()\"))\n",
    "            cleaned_text = re.sub(r'[^\\w\\s]', '', text, flags=re.UNICODE)\n",
    "            cleaned_text = re.sub(r'\\s+', '', cleaned_text).strip()\n",
    "            return sha256_hash(cleaned_text)\n",
    "\n",
    "def sha256_hash(string):\n",
    "    return hashlib.sha256(string.encode()).hexdigest()\n",
    "    \n",
    "# 异常日志\n",
    "def get_s3_doctor(target_theme):\n",
    "    partition_id = str(uuid.uuid4())\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d\")\n",
    "    error_log_path = f\"{ERROR_PATH}{target_theme}/{current_time}/{partition_id}.jsonl\"\n",
    "    s3_doc_writer = S3DocWriter(path=error_log_path)\n",
    "    return s3_doc_writer\n",
    "\n",
    "def parse_path_to_html(iter):\n",
    "    seen = set()\n",
    "    \n",
    "    # 初始化错误日志写入器\n",
    "    s3_doc_writer = get_s3_doctor(\"dedup_fir\")\n",
    "    error_info = None          # 错误信息初始化\n",
    "    \n",
    "    for fpath in iter:\n",
    "        try:\n",
    "            # 读取文件并处理\n",
    "            for zz in read_s3_rows(fpath, use_stream=True):\n",
    "                try:\n",
    "                    detail_datas = json_loads(zz.value)\n",
    "                    # 安全地获取字段，提供默认值\n",
    "                    html_content = detail_datas.get(\"html\", \"\")\n",
    "                    url = detail_datas.get(\"url\", \"\")\n",
    "                    track_id = detail_datas.get(\"track_id\", \"\")\n",
    "                    \n",
    "                    hash_html = html_to_content(html_content, url) if html_content else None\n",
    "                    if hash_html and hash_html not in seen:  # 保持原有的去重逻辑\n",
    "                        seen.add(hash_html)\n",
    "                        line = {\n",
    "                            \"sub_path\": fpath.split('/')[4],\n",
    "                            \"hash_html\": hash_html,\n",
    "                            \"track_id\": track_id,\n",
    "                            \"file_path\": fpath,\n",
    "                        }\n",
    "                        yield Row(**{\"value\": json_dumps(line)})\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    # 记录数据解析错误\n",
    "                    error_info = {\n",
    "                        \"error_type\": type(e).__name__,\n",
    "                        \"error_message\": str(e),\n",
    "                        \"traceback\": traceback.format_exc(),\n",
    "                        \"input_data\": zz.value if hasattr(zz, 'value') else str(zz),\n",
    "                        \"file_path\": fpath,\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    s3_doc_writer.write(error_info)\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            # 记录文件读取错误\n",
    "            error_info = {\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"error_message\": str(e),\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "                \"input_data\": \"N/A\",\n",
    "                \"file_path\": fpath,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            s3_doc_writer.write(error_info)\n",
    "            continue\n",
    "    \n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapPartitions 对 warc path 并行解析数据\n",
    "schema = StructType([\n",
    "    StructField(\"value\", StringType(), True),\n",
    "])\n",
    "page_content = sc.parallelize(warc_paths, len(warc_paths))\n",
    "dump_html_df = page_content.mapPartitions(parse_path_to_html).toDF(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# 写出s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"skip_output_version\"] = True\n",
    "config[\"output_compression\"] = \"gz\"\n",
    "write_any_path(dump_html_df, output_path, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (ipykernel)",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
