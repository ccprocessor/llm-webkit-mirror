{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xinghe.spark import *\n",
    "from app.common.json_util import *\n",
    "from xinghe.s3 import *\n",
    "\n",
    "config = {\n",
    "    \"spark_conf_name\": \"spark_4\",\n",
    "    \"skip_success_check\": True,\n",
    "}\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from pyspark.sql.functions import from_json, to_json, struct, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "DUMPS = [\n",
    "    ...\n",
    "]\n",
    "MAX_OUTPUT_ROW_SIZE = 1024 * 1024 * 1024 * 1.5\n",
    "base_input_path = f\"s3a://xxx/\"\n",
    "already_exist_id_path = \"s3://xxx/\"\n",
    "output_path = \"s3://xxx/\"\n",
    "spark = new_spark_session(\"cc_dumps.dedup.sec\", config)\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# 未去重的hash data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_paths = []\n",
    "for dump in DUMPS:\n",
    "    input_path = f\"{base_input_path}{dump}\"\n",
    "    dump_paths.append(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = spark.read.format(\"json\").load(dump_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# 已去重的ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_exist_id_v_df = read_any_path(spark, already_exist_id_path, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 Schema\n",
    "schema = StructType([\n",
    "    StructField(\"hash_html\", StringType(), True),\n",
    "    StructField(\"sub_path\", StringType(), True),\n",
    "])\n",
    "df_with_struct = already_exist_id_v_df.withColumn(\"json_struct\", from_json(already_exist_id_v_df.value, schema))\n",
    "already_exist_id_df = df_with_struct.select(\"json_struct.hash_html\", col(\"json_struct.sub_path\").alias(\"sub_path_exist\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# filter 历史未去重的剩余id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = input_df.join(already_exist_id_df, on=\"hash_html\", how=\"left\")\n",
    "undedup_id_df = join_df.filter(col(\"sub_path_exist\").isNull()).select([\"track_id\", \"sub_path\", \"hash_html\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# 分区去重-->全局去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_partition(partition):\n",
    "    sorted_partition = sorted(partition, key=itemgetter('hash_html'))\n",
    "    return (next(group) for _, group in groupby(sorted_partition, key=itemgetter('hash_html')))\n",
    "dedup_part_df = undedup_id_df.rdd.mapPartitions(deduplicate_partition).toDF(undedup_id_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_df = dedup_part_df.dropDuplicates([\"hash_html\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# 写出s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_col = struct(dedup_df[\"track_id\"],dedup_df[\"sub_path\"],dedup_df[\"hash_html\"],)\n",
    "output_df = dedup_df.withColumn(\"value\", to_json(struct_col)).select(\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"skip_output_version\"] = True\n",
    "config[\"output_compression\"] = \"gz\"\n",
    "write_any_path(output_df, output_path, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (ipykernel)",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
