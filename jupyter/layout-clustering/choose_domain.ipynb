{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xinghe.spark import *\n",
    "from app.common.json_util import *\n",
    "from xinghe.s3 import *\n",
    "\n",
    "config = {\n",
    "    \"spark_conf_name\": \"spark_4\",\n",
    "    \"skip_success_check\": True,\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import uuid\n",
    "import heapq\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.functions import from_json, row_number, col, sum as _sum, to_json, struct, pandas_udf, PandasUDFType, \\\n",
    "    lit, spark_partition_id\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType\n",
    "\n",
    "BASE_PARTITION_ID = 0\n",
    "OUTPUT_PATH = \"s3://xxx/\"\n",
    "INPUT_PATH = \"s3://xxx/\"\n",
    "COUNT_MAP = [\n",
    "    (\"1-1600\", 1, 1600),\n",
    "    (\"1600-1.5w\", 1600, 15000),\n",
    "    (\"1.5w-10w\", 15000, 100000),\n",
    "    (\"10w\", 100000, None)\n",
    "]\n",
    "DATA_SIZE_PER_BATCH = 1000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_list_to_chunks(values, n_chunks):\n",
    "    assignments = dict()\n",
    "    chunks = [(0, i) for i in range(n_chunks)]\n",
    "    heapq.heapify(chunks)\n",
    "\n",
    "    indexed_values = sorted([(val[\"count\"], idx, val[\"domain\"]) for idx, val in enumerate(values)], key=lambda x: -x[0])\n",
    "    for weight, idx, name in indexed_values:\n",
    "        current_sum, chunk_id = heapq.heappop(chunks)\n",
    "        assignments[name] = chunk_id\n",
    "        new_sum = current_sum + weight\n",
    "        heapq.heappush(chunks, (new_sum, chunk_id))\n",
    "        yield Row(domain=name, partition_id=chunk_id)\n",
    "\n",
    "\n",
    "def write_by_partitionid(_iter):\n",
    "    detail_data = None\n",
    "    s3_writer = None\n",
    "    for index, detail_data in _iter.iterrows():\n",
    "        line = {\n",
    "            \"domain\": detail_data[\"domain\"],\n",
    "            \"count\": detail_data[\"count\"],\n",
    "            \"partition_id\": detail_data[\"partition_id\"],\n",
    "            \"files\": detail_data[\"files\"].tolist(),\n",
    "        }\n",
    "        if s3_writer:\n",
    "            s3_writer.write(line)\n",
    "        else:\n",
    "            partition_id = detail_data[\"partition_id\"] + BASE_PARTITION_ID\n",
    "            output_file = f\"{OUTPUT_PATH}{count_data[0]}_{total_count}/{partition_id}.jsonl\"\n",
    "            s3_writer = S3DocWriter(output_file)\n",
    "            s3_writer.write(line)\n",
    "\n",
    "    if detail_data is not None:\n",
    "        s3_writer.flush()\n",
    "    yield {\"write_size\": index}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# choose domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark(spark_name: str):\n",
    "    global spark\n",
    "    spark = new_spark_session(f\"layout.{spark_name}\", config)\n",
    "    sc = spark.sparkContext\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "def get_input_path(INPUT_PATH):\n",
    "    try:\n",
    "        with open(\"./already_exist.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            already_exist = eval(content) if content else []\n",
    "    except:\n",
    "        already_exist = []\n",
    "    input_path_lst = [f.replace(\"s3\", \"s3a\") for f in list(list_s3_objects(INPUT_PATH, recursive=True)) if\n",
    "                      f.endswith(\".jsonl\") and f not in already_exist]\n",
    "    if input_path_lst:\n",
    "        with open(\"./already_exist.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            already_exist.extend(input_path_lst)\n",
    "            f.write(str(already_exist))\n",
    "    get_input_df(input_path_lst)\n",
    "\n",
    "\n",
    "def get_input_df(input_path_lst):\n",
    "    df = spark.read.format(\"json\").load(input_path_lst)\n",
    "    df.cache()\n",
    "    filter_batch_df(df)\n",
    "\n",
    "\n",
    "def filter_batch_df(df: DataFrame):\n",
    "    for i in range(4):\n",
    "        global count_data\n",
    "        count_data = COUNT_MAP[i]\n",
    "        if count_data[2] is None:\n",
    "            filter_df = df.filter(col(\"count\") > count_data[1])\n",
    "        else:\n",
    "            filter_df = df.filter(col(\"count\") > count_data[1]).filter(col(\"count\") <= count_data[2])\n",
    "        total_batch_count(filter_df)\n",
    "        parse_partition_id(filter_df, i)\n",
    "\n",
    "\n",
    "def total_batch_count(filter_df: DataFrame):\n",
    "    global total_count\n",
    "    total_count = filter_df.select(_sum(\"count\")).collect()[0][0]\n",
    "\n",
    "\n",
    "def parse_partition_id(filter_df: DataFrame, i: int):\n",
    "    NUM_PARTITIONS = round(total_count / DATA_SIZE_PER_BATCH)\n",
    "    if i == 0:\n",
    "        repart_df = filter_df.select([\"domain\"]).repartition(NUM_PARTITIONS, col(\"domain\"))\n",
    "        partition_df = repart_df.withColumn(\"partition_id\", spark_partition_id())\n",
    "    else:\n",
    "        # 分区数取决于total_count 和每批次的数据量级\n",
    "        weight_datas = filter_df.select([\"domain\", \"count\"]).collect()\n",
    "        partition_list = list(divide_list_to_chunks(weight_datas, NUM_PARTITIONS))\n",
    "        partition_df = spark.createDataFrame(partition_list)\n",
    "    join_to_write(filter_df, partition_df)\n",
    "\n",
    "\n",
    "def join_to_write(filter_df: DataFrame, partition_df: DataFrame, ):\n",
    "    df_with_weight = filter_df.join(partition_df, on=\"domain\")\n",
    "\n",
    "    output_schema = StructType([\n",
    "        StructField('write_size', LongType(), True),\n",
    "    ])\n",
    "\n",
    "    @pandas_udf(output_schema, PandasUDFType.GROUPED_MAP)\n",
    "    def pandas_udf_repartition(data_collected_series):\n",
    "        result = write_by_partitionid(data_collected_series)\n",
    "        if result:\n",
    "            return pd.DataFrame(result)\n",
    "\n",
    "    output_df = df_with_weight.groupby('partition_id').apply(pandas_udf_repartition)\n",
    "    output_df.count()\n",
    "\n",
    "\n",
    "def close_spark():\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark_name = \"choose_domain\"\n",
    "    create_spark(spark_name)\n",
    "    get_input_path(INPUT_PATH)\n",
    "    close_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (ipykernel)",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
