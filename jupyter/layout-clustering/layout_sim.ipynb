{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xinghe.spark import *\n",
    "from app.common.json_util import *\n",
    "from xinghe.s3 import *\n",
    "from xinghe.s3.read import *\n",
    "\n",
    "config = {\n",
    "    \"spark_conf_name\": \"spark_4\",\n",
    "    \"skip_success_check\": True,\n",
    "    \"spark.executorEnv.LLM_WEB_KIT_CFG_PATH\": \"/share/xxx.jsonc\",\n",
    "    \"spark.yarn.queue\": \"pipeline.ehtml\",\n",
    "    \"spark.sql.shuffle.partitions\": 100000,\n",
    "}\n",
    "\n",
    "from llm_web_kit.libs.standard_utils import compress_and_decompress_str\n",
    "from llm_web_kit.html_layout.html_layout_cosin import cluster_html_struct, get_feature, similarity, sum_tags\n",
    "\n",
    "import base64\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "import zlib\n",
    "import uuid\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Union\n",
    "from copy import deepcopy, copy\n",
    "from urllib.parse import quote, unquote, urlparse, parse_qs\n",
    "from datetime import datetime\n",
    "from lxml import html\n",
    "from collections import defaultdict\n",
    "from func_timeout import FunctionTimedOut, func_timeout\n",
    "\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.functions import row_number, col, collect_list, struct, expr, rand, count, pandas_udf, PandasUDFType, \\\n",
    "    round as _round, lit, to_json, from_json, explode\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, BinaryType, StringType, LongType\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"LLM_WEB_KIT_CFG_PATH\"] = \"/share/renpengli/.llm-web-kit.jsonc\"\n",
    "\n",
    "TIMEOUT_SECONDS = 60 * 2\n",
    "MAX_OUTPUT_ROW_SIZE = 1024 * 1024 * 1024 * 1.7\n",
    "MAX_OUTPUT_FILE_SIZE = 1024 * 1024 * 1024 * 10\n",
    "SIMILARITY_THRESHOLD = 0.95\n",
    "NUM_PARTITIONS = 100000\n",
    "WRITE_NUM_PARTITIONS = 20000\n",
    "ERROR_PATH = \"s3://xxx/\"\n",
    "INPUT_PATH = \"s3://xxx/\"\n",
    "BASE_OUTPUT_PATH = \"s3://xxx/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_doctor(target_theme):\n",
    "    partition_id = str(uuid.uuid4())\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d\")\n",
    "    error_log_path = f\"{ERROR_PATH}{target_theme}/{current_time}/{partition_id}.jsonl\"\n",
    "    s3_doc_writer = S3DocWriter(path=error_log_path)\n",
    "    return s3_doc_writer\n",
    "\n",
    "\n",
    "def parse_output_data(row_data):\n",
    "    row_data.update({\"layout_id\": '_'.join([row_data[\"url_host_name\"], str(row_data[\"layout_id\"])])})\n",
    "    new_row_data_json = json_dumps(row_data)\n",
    "    if len(new_row_data_json) < MAX_OUTPUT_ROW_SIZE:\n",
    "        return {\"value\": new_row_data_json, \"layout_id\": row_data[\"layout_id\"]}\n",
    "    return None\n",
    "\n",
    "\n",
    "def calculating_similarity(feature_dict, feature, max_layer_n):\n",
    "    for k, v in feature_dict.items():\n",
    "        if any(similarity(feature, h[\"feature\"], max_layer_n) >= SIMILARITY_THRESHOLD for h in v):\n",
    "            return int(k)\n",
    "    return -2\n",
    "\n",
    "\n",
    "def parse_similarity(_iter):\n",
    "    s3_doc_writer = get_s3_doctor(\"parse_similarity\")\n",
    "    error_info = None\n",
    "    is_no_layout_id = False\n",
    "    for row in _iter:\n",
    "        layout_dict = json_loads(row.layout_dict)\n",
    "        layout_list = layout_dict.get(\"layout_list\", [])\n",
    "        if len(layout_list) == 1 and layout_list[0] == -1:\n",
    "            is_no_layout_id = True\n",
    "        feature_dict = layout_dict.get(\"feature_dict\", {})\n",
    "        max_layer_n = layout_dict.get(\"max_layer_n\", 5)\n",
    "        domain = row.domain\n",
    "        count = row.count\n",
    "        file_d = row.file\n",
    "        offset = file_d.offset\n",
    "        length = file_d.length\n",
    "        record_count = file_d.record_count\n",
    "        idx = 0\n",
    "        try:\n",
    "            for detail_data in read_s3_lines_with_range(file_d[\"filepath\"], use_stream=True,\n",
    "                                                        bytes_range=(offset, offset + length)):\n",
    "                idx += 1\n",
    "                if idx > record_count:\n",
    "                    break\n",
    "                detail_data = json_loads(detail_data)\n",
    "                if is_no_layout_id is True:\n",
    "                    layout_id = -1\n",
    "                else:\n",
    "                    try:\n",
    "                        feature = get_feature(detail_data[\"html\"])\n",
    "                        if feature is None or not feature.get(\"tags\"):\n",
    "                            layout_id = -3\n",
    "                        else:\n",
    "                            layout_id = func_timeout(TIMEOUT_SECONDS, calculating_similarity,\n",
    "                                                     (feature_dict, feature, max_layer_n,))\n",
    "                    except FunctionTimedOut as e:\n",
    "                        error_info = {\n",
    "                            \"error_type\": type(e).__name__,\n",
    "                            \"error_message\": str(e),\n",
    "                            \"traceback\": traceback.format_exc(),\n",
    "                            \"input_data\": str(detail_data),\n",
    "                            \"timestamp\": datetime.now().isoformat()\n",
    "                        }\n",
    "                        s3_doc_writer.write(error_info)\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        error_info = {\n",
    "                            \"error_type\": type(e).__name__,\n",
    "                            \"error_message\": str(e),\n",
    "                            \"traceback\": traceback.format_exc(),\n",
    "                            \"input_data\": str(detail_data),\n",
    "                            \"timestamp\": datetime.now().isoformat()\n",
    "                        }\n",
    "                        s3_doc_writer.write(error_info)\n",
    "                        continue\n",
    "                line = {\n",
    "                    \"track_id\": detail_data[\"track_id\"],\n",
    "                    \"html\": detail_data[\"html\"],\n",
    "                    \"url\": detail_data[\"url\"],\n",
    "                    \"layout_id\": layout_id,\n",
    "                    \"max_layer_n\": max_layer_n,\n",
    "                    \"url_host_name\": domain,\n",
    "                    \"raw_warc_path\": detail_data[\"raw_warc_path\"]\n",
    "                }\n",
    "                json_line = parse_output_data(line)\n",
    "                if json_line is not None:\n",
    "                    yield json_line\n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"error_message\": str(e),\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "                \"input_data\": str(row),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            s3_doc_writer.write(error_info)\n",
    "            continue\n",
    "\n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n",
    "\n",
    "\n",
    "def save_s3_by_layout(outdata_list):\n",
    "    s3_doc_writer = get_s3_doctor(\"similarity_write\")\n",
    "    error_info = None\n",
    "    json_line = None\n",
    "    s3_writer = None\n",
    "    index = 0\n",
    "    output_file_size = 0\n",
    "    for index, row in enumerate(outdata_list):\n",
    "        try:\n",
    "            if output_file_size > MAX_OUTPUT_FILE_SIZE:\n",
    "                if json_line:\n",
    "                    s3_writer.flush()\n",
    "                    s3_writer = None\n",
    "                    output_file_size = 0\n",
    "            json_line = json_loads(row.value)\n",
    "            if s3_writer:\n",
    "                output_file_size += s3_writer.write(json_line)\n",
    "            else:\n",
    "                partition_id = str(uuid.uuid4())\n",
    "                output_file = f\"{OUTPUT_PATH}{partition_id}.jsonl.gz\"\n",
    "                s3_writer = S3DocWriter(output_file)\n",
    "                output_file_size += s3_writer.write(json_line)\n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"error_message\": str(e),\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "                \"input_data\": row.value,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            s3_doc_writer.write(error_info)\n",
    "            continue\n",
    "\n",
    "    if json_line:\n",
    "        s3_writer.flush()\n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n",
    "    yield {\"write_size\": index}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# main func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_input_path(input_path):\n",
    "    try:\n",
    "        with open(\"./is_similarity_complated.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            already_exist = [i for i in content.split(\",\") if i] if content else []\n",
    "    except:\n",
    "        already_exist = []\n",
    "    input_path_lst = [i for i in [f.replace(\"s3\", \"s3a\") for f in list(list_s3_objects(input_path, recursive=False))] if\n",
    "                      i not in already_exist]\n",
    "    return input_path_lst\n",
    "\n",
    "\n",
    "def parse_path(batch):\n",
    "    path_list = batch.split('/')\n",
    "    global OUTPUT_PATH\n",
    "    OUTPUT_PATH = f\"{BASE_OUTPUT_PATH}{path_list[-3]}/{path_list[-2]}/\"\n",
    "    return path_list\n",
    "\n",
    "\n",
    "def create_spark(spark_name: str):\n",
    "    global spark\n",
    "    spark = new_spark_session(f\"layout.similarity.{spark_name}\", config)\n",
    "    sc = spark.sparkContext\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "def get_domain_df(batch):\n",
    "    input_f_df = spark.read.format(\"json\").load(batch).filter(col(\"layout_dict\").isNotNull())\n",
    "    input_df = input_f_df.withColumn(\"file\", explode(col(\"files\"))).drop(\"files\")\n",
    "    similarity_every_domain(input_df)\n",
    "\n",
    "\n",
    "def similarity_every_domain(input_df: DataFrame):\n",
    "    schema = StructType([\n",
    "        StructField('value', StringType(), True),\n",
    "        StructField('layout_id', StringType(), True),\n",
    "    ])\n",
    "\n",
    "    all_domain_df = input_df.repartition(NUM_PARTITIONS).rdd.mapPartitions(parse_similarity).toDF(schema)\n",
    "    write_by_layoutid(all_domain_df)\n",
    "\n",
    "\n",
    "def write_by_layoutid(all_domain_df: DataFrame):\n",
    "    output_schema = StructType([\n",
    "        StructField('write_size', IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    final_df = all_domain_df.repartition(WRITE_NUM_PARTITIONS, col(\"layout_id\")).sortWithinPartitions(col(\"layout_id\"))\n",
    "    out_df = final_df.rdd.mapPartitions(save_s3_by_layout)\n",
    "    out_df.count()\n",
    "\n",
    "\n",
    "def close_spark():\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_path_lst = parse_input_path(INPUT_PATH)\n",
    "    for batch in input_path_lst:\n",
    "        path_list = parse_path(batch)\n",
    "        spark_name = '_'.join([path_list[-3], path_list[-2]])\n",
    "        create_spark(spark_name)\n",
    "        get_domain_df(batch)\n",
    "        close_spark()\n",
    "        with open(\"./is_similarity_complated.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(batch + \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python webkit_venv (ipykernel)",
   "language": "python",
   "name": "webkit_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
