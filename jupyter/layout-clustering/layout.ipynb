{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xinghe.spark import *\n",
    "from app.common.json_util import *\n",
    "from xinghe.s3 import *\n",
    "from xinghe.s3.read import *\n",
    "from xinghe.ops.spark import spark_resize_file\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"LLM_WEB_KIT_CFG_PATH\"] = \"/xxx/.llm-web-kit.jsonc\"\n",
    "from llm_web_kit.html_layout.html_layout_cosin import cluster_html_struct, get_feature, similarity, sum_tags\n",
    "\n",
    "import uuid\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from func_timeout import FunctionTimedOut, func_timeout\n",
    "\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.functions import row_number, col, collect_list, struct, expr, pandas_udf, PandasUDFType, \\\n",
    "    round as _round, lit, to_json, explode\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType\n",
    "\n",
    "config = {\n",
    "    \"spark_conf_name\": \"spark_4\",\n",
    "    \"skip_success_check\": True,\n",
    "    \"spark.executorEnv.LLM_WEB_KIT_CFG_PATH\": \"/xxx/.llm-web-kit.jsonc\",\n",
    "}\n",
    "\n",
    "COUNT_MAP = [  # 按批选择域名的标准\n",
    "    (\"1-1600\", 1, 1600),\n",
    "    (\"1600-1.5w\", 1600, 15000),\n",
    "    (\"1.5w-10w\", 15000, 100000),\n",
    "    (\"10w\", 100000, None)\n",
    "]\n",
    "BASE_PARTITION_ID = 0  # 初始化分区id\n",
    "DATA_SIZE_PER_BATCH = 1000000000  # 每批次数据量级\n",
    "LAYOUT_TIMEOUT_SECONDS = 3600 * 5  # 计算layout超时时间5min\n",
    "SIM_TIMEOUT_SECONDS = 60 * 1  # 相似度匹配超时时间60s\n",
    "MAX_OUTPUT_ROW_SIZE = 1024 * 1024 * 1024 * 1.7  # 每条数据最大bytes\n",
    "MAX_OUTPUT_FILE_SIZE = 1024 * 1024 * 1024 * 2\n",
    "MAX_LAYOUTLIST_SIZE = 200  # 最大 layoutid_size/domain\n",
    "SIMILARITY_THRESHOLD = 0.95  # 最大相似度匹配值\n",
    "RATE_MAP = {\"10w\": 0.1, \"1.5w-10w\": 0.2, \"1600-1.5w\": 0.5, \"1-1600\": 1, \"500-1600\": 1}  # 不同域名量级对应的筛选标准\n",
    "NUM_PARTITIONS = 100000  # 自定义分区数\n",
    "WRITE_NUM_PARTITIONS = 20000  # 自定义write分区数\n",
    "\n",
    "ERROR_PATH = \"s3://xxx/\"  # 异常日志地址\n",
    "INPUT_PATH = \"s3://xxx/\"  # 输入数据地址\n",
    "CHOOSE_OUTPUT_PATH = \"s3://xxx/\"  # 第一步choose domain输出数据地址\n",
    "BASE_LAYOUT_OUTPUT_PATH = \"s3://xxx/\"  # 第二步layout输出数据基础地址\n",
    "BASE_SIM_OUTPUT_PATH = \"s3://xxx/\"  # 第三步sim输出数据基础地址\n",
    "BASE_INDEX_OUTPUT_PATH = \"s3://xxx/\"  # 第四步index输出数据基础地址\n",
    "BASE_DOMAIN_PATH = \"s3://xxx/\"  # 过程中 valid domain 存储地址\n",
    "BASE_BATCH_PATH = \"s3://xxx/\"  # 过程中 batch layout 存储地址"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 异常日志\n",
    "def get_s3_doctor(target_theme):\n",
    "    partition_id = str(uuid.uuid4())\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d\")\n",
    "    error_log_path = f\"{ERROR_PATH}{target_theme}/{current_time}/{partition_id}.jsonl\"\n",
    "    s3_doc_writer = S3DocWriter(path=error_log_path)\n",
    "    return s3_doc_writer\n",
    "\n",
    "\n",
    "# 自定义生成分区id\n",
    "def divide_list_to_chunks(values, n_chunks):\n",
    "    assignments = dict()\n",
    "    chunks = [(0, i) for i in range(n_chunks)]\n",
    "    heapq.heapify(chunks)\n",
    "\n",
    "    indexed_values = sorted([(val[\"count\"], idx, val[\"domain\"]) for idx, val in enumerate(values)], key=lambda x: -x[0])\n",
    "    for weight, idx, name in indexed_values:\n",
    "        current_sum, chunk_id = heapq.heappop(chunks)\n",
    "        assignments[name] = chunk_id\n",
    "        new_sum = current_sum + weight\n",
    "        heapq.heappush(chunks, (new_sum, chunk_id))\n",
    "        yield Row(domain=name, partition_id=chunk_id)\n",
    "\n",
    "\n",
    "# 依据分区id将数据写入s3\n",
    "def write_by_partitionid(_iter):\n",
    "    detail_data = None\n",
    "    s3_writer = None\n",
    "    for index, detail_data in _iter.iterrows():\n",
    "        line = {\n",
    "            \"domain\": detail_data[\"domain\"],\n",
    "            \"count\": detail_data[\"count\"],\n",
    "            \"partition_id\": detail_data[\"partition_id\"],\n",
    "            \"files\": detail_data[\"files\"].tolist(),\n",
    "        }\n",
    "        if s3_writer:\n",
    "            s3_writer.write(line)\n",
    "        else:\n",
    "            partition_id = detail_data[\"partition_id\"] + BASE_PARTITION_ID\n",
    "            output_file = f\"{CHOOSE_OUTPUT_PATH}{count_data[0]}_{total_count}/{partition_id}.jsonl\"\n",
    "            s3_writer = S3DocWriter(output_file)\n",
    "            s3_writer.write(line)\n",
    "\n",
    "    if detail_data is not None:\n",
    "        s3_writer.flush()\n",
    "    yield {\"write_size\": index}\n",
    "\n",
    "\n",
    "# 依据index获取详情数据\n",
    "def get_all_domain_data(_iter):\n",
    "    s3_doc_writer = get_s3_doctor(\"get_feature\")\n",
    "    error_info = None\n",
    "    for row in _iter:\n",
    "        valid_count = row.valid_count\n",
    "        file_d = row.file\n",
    "        offset = file_d.offset\n",
    "        length = file_d.length\n",
    "        record_count = file_d.record_count\n",
    "        try:\n",
    "            for detail_data in read_s3_by_offset_limit(file_d.filepath, offset, limit=record_count):\n",
    "                try:\n",
    "                    detail_data = json_loads(detail_data.value)\n",
    "                    feature = get_feature(detail_data[\"html\"])\n",
    "                    if feature is None or not feature.get(\"tags\"):\n",
    "                        continue\n",
    "                    layer_n, total_n = sum_tags(feature[\"tags\"])\n",
    "                    line = {\n",
    "                        \"date\": detail_data[\"date\"],\n",
    "                        \"track_id\": detail_data[\"track_id\"],\n",
    "                        \"url\": detail_data[\"url\"],\n",
    "                        \"raw_warc_path\": detail_data[\"raw_warc_path\"],\n",
    "                        \"domain\": row.domain,\n",
    "                        \"sub_path\": row.domain,\n",
    "                        \"valid_count\": valid_count,\n",
    "                        \"feature\": feature,\n",
    "                        \"layer_n\": layer_n,\n",
    "                        \"total_n\": total_n\n",
    "                    }\n",
    "                    line = json_dumps(line)\n",
    "                    if len(line) < MAX_OUTPUT_ROW_SIZE:\n",
    "                        yield Row(**{\"value\": line, \"domain\": row.domain, \"valid_count\": valid_count})\n",
    "                    else:\n",
    "                        error_info = {\n",
    "                            \"error_type\": \"EOFError\",\n",
    "                            \"error_message\": \"Memory more than required for vector is (2147483648)\",\n",
    "                            \"traceback\": traceback.format_exc(),\n",
    "                            \"input_data\": detail_data,\n",
    "                            \"timestamp\": datetime.now().isoformat()\n",
    "                        }\n",
    "                        s3_doc_writer.write(error_info)\n",
    "                        continue\n",
    "                except Exception as e:\n",
    "                    error_info = {\n",
    "                        \"error_type\": type(e).__name__,\n",
    "                        \"error_message\": str(e),\n",
    "                        \"traceback\": traceback.format_exc(),\n",
    "                        \"input_data\": detail_data,\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    s3_doc_writer.write(error_info)\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"error_message\": str(e),\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "                \"input_data\": str(row),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            s3_doc_writer.write(error_info)\n",
    "            continue\n",
    "\n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n",
    "\n",
    "\n",
    "# 输出数据过滤\n",
    "def crush_output_data(output_data):\n",
    "    output_data_json = json_dumps(output_data)\n",
    "    if len(output_data_json) < MAX_OUTPUT_ROW_SIZE:\n",
    "        return {\"value\": output_data_json}\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# 解析批次数据\n",
    "def parse_batch_data(fpath):\n",
    "    sample_list = []\n",
    "    index = 0\n",
    "    for domain_v in read_s3_rows(fpath, use_stream=True):\n",
    "        index += 1\n",
    "        if index != 0 and not index % 800:\n",
    "            yield sample_list\n",
    "            sample_list = []\n",
    "        domain_data = json_loads(domain_v.value)\n",
    "        try:\n",
    "            lines = {\n",
    "                \"feature\": domain_data[\"feature\"],\n",
    "                \"layer_n\": domain_data[\"layer_n\"],\n",
    "                \"total_n\": domain_data[\"total_n\"],\n",
    "                \"track_id\": domain_data[\"track_id\"],\n",
    "                \"url\": domain_data[\"url\"],\n",
    "                \"domain\": domain_data[\"domain\"],\n",
    "                \"raw_warc_path\": domain_data[\"raw_warc_path\"],\n",
    "                \"date\": domain_data[\"date\"]\n",
    "            }\n",
    "            sample_list.append(lines)\n",
    "        except:\n",
    "            pass\n",
    "    if sample_list:\n",
    "        yield sample_list\n",
    "\n",
    "\n",
    "# 计算layout\n",
    "def calculating_layout(current_host_name, sample_list):\n",
    "    cluster_datas, layout_list = cluster_html_struct(sample_list)\n",
    "    feature_dict = defaultdict(list)\n",
    "    max_layer_n = cluster_datas[0][\"max_layer_n\"]\n",
    "    # 每个layout类别抽取3个网页\n",
    "    for r in cluster_datas:\n",
    "        layout_id = r[\"layout_id\"]\n",
    "        if layout_id == -1:\n",
    "            continue\n",
    "        if len(feature_dict[layout_id]) < 3:\n",
    "            cr = copy(r)\n",
    "            feature_dict[layout_id].append(cr)\n",
    "    if layout_list:\n",
    "        layout_tmp_dict = crush_output_data(\n",
    "            {\"domain\": current_host_name, \"sub_path\": current_host_name, \"feature_dict\": dict(feature_dict),\n",
    "             \"layout_list\": layout_list, \"max_layer_n\": max_layer_n})\n",
    "        yield layout_tmp_dict\n",
    "\n",
    "\n",
    "# 解析layout\n",
    "def parse_layout(domain_list):\n",
    "    s3_doc_writer = get_s3_doctor(\"parse_layout\")\n",
    "    error_info = None\n",
    "    for fpath in domain_list:\n",
    "        try:\n",
    "            for sample_list in parse_batch_data(fpath):\n",
    "                try:\n",
    "                    if len(sample_list) > 1:\n",
    "                        current_host_name = sample_list[0][\"domain\"]\n",
    "                        print(\n",
    "                            f\"current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, sample_list:{len(sample_list)}, current_host_name:{current_host_name}\")\n",
    "                        for line in func_timeout(LAYOUT_TIMEOUT_SECONDS, calculating_layout,\n",
    "                                                 (current_host_name, sample_list,)):\n",
    "                            if line is not None:\n",
    "                                yield line\n",
    "                except FunctionTimedOut as e:\n",
    "                    error_info = {\n",
    "                        \"error_type\": type(e).__name__,\n",
    "                        \"error_message\": str(e),\n",
    "                        \"traceback\": traceback.format_exc(),\n",
    "                        \"input_data\": str(sample_list),\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    s3_doc_writer.write(error_info)\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    error_info = {\n",
    "                        \"error_type\": type(e).__name__,\n",
    "                        \"error_message\": str(e),\n",
    "                        \"traceback\": traceback.format_exc(),\n",
    "                        \"input_data\": str(sample_list),\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    s3_doc_writer.write(error_info)\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"error_message\": str(e),\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "                \"input_data\": fpath,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            s3_doc_writer.write(error_info)\n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n",
    "\n",
    "\n",
    "# 通过相似度合并layout\n",
    "def layout_similarity(layout_d1, layout_d2):\n",
    "    max_layer_n = max(layout_d1[\"max_layer_n\"], layout_d2[\"max_layer_n\"])\n",
    "    layout_last = layout_d1\n",
    "    layout_last[\"max_layer_n\"] = max_layer_n\n",
    "    layout_list1 = layout_d1[\"layout_list\"]\n",
    "    layout_list2 = layout_d2[\"layout_list\"]\n",
    "    if len(layout_list1) > MAX_LAYOUTLIST_SIZE or len(layout_list2) > MAX_LAYOUTLIST_SIZE:\n",
    "        return layout_d1 if len(layout_list1) > len(layout_list2) else layout_d2\n",
    "    max_layout_id = max(layout_list1)\n",
    "    feature_dict1 = layout_d1[\"feature_dict\"]\n",
    "    feature_dict2 = layout_d2[\"feature_dict\"]\n",
    "    ls_v = []\n",
    "    [ls_v.extend(v) for k, v in feature_dict1.items()]\n",
    "    exist_layout_num = 0\n",
    "    for new_k, new_v in feature_dict2.items():\n",
    "        add_tmp_dict_v = True\n",
    "        for new_d in new_v:\n",
    "            if any(similarity(new_d[\"feature\"], h[\"feature\"], max_layer_n) >= SIMILARITY_THRESHOLD for h in ls_v):\n",
    "                add_tmp_dict_v = False\n",
    "                exist_layout_num += 1\n",
    "                break\n",
    "        if add_tmp_dict_v is True:\n",
    "            max_layout_id += 1\n",
    "            layout_last[\"feature_dict\"][str(max_layout_id)] = new_v\n",
    "            layout_last[\"layout_list\"].append(max_layout_id)\n",
    "    return layout_last\n",
    "\n",
    "\n",
    "# 合并layout\n",
    "def merge_layout(domain_list):\n",
    "    s3_doc_writer = get_s3_doctor(\"merge_layout\")\n",
    "    print(f\"start merge layout, current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    error_info = None\n",
    "    pre_domain = {}\n",
    "    domain_v = None\n",
    "    # 两两进行合并\n",
    "    index = 0\n",
    "    for domain_f in domain_list:\n",
    "        domain_paths = [f for f in list(list_s3_objects(domain_f, recursive=True)) if f.endswith(\".jsonl\")]\n",
    "        for fpath in domain_paths:\n",
    "            try:\n",
    "                for domain_v in read_s3_rows(fpath, use_stream=True):\n",
    "                    index += 1\n",
    "                    if index == 1:\n",
    "                        pre_domain = json_loads(domain_v.value)\n",
    "                    else:\n",
    "                        try:\n",
    "                            pre_domain = layout_similarity(pre_domain, json_loads(domain_v.value))\n",
    "                        except Exception as e:\n",
    "                            error_info = {\n",
    "                                \"error_type\": type(e).__name__,\n",
    "                                \"error_message\": str(e),\n",
    "                                \"traceback\": traceback.format_exc(),\n",
    "                                \"input_data\": domain_v.value,\n",
    "                                \"timestamp\": datetime.now().isoformat()\n",
    "                            }\n",
    "                            s3_doc_writer.write(error_info)\n",
    "                            continue\n",
    "            except Exception as e:\n",
    "                error_info = {\n",
    "                    \"error_type\": type(e).__name__,\n",
    "                    \"error_message\": str(e),\n",
    "                    \"traceback\": traceback.format_exc(),\n",
    "                    \"input_data\": domain_v.value,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "                s3_doc_writer.write(error_info)\n",
    "\n",
    "    if pre_domain:\n",
    "        output_data = json_dumps(pre_domain)\n",
    "        if len(output_data) < MAX_OUTPUT_ROW_SIZE:\n",
    "            yield {\"layout_dict\": output_data, \"domain\": pre_domain[\"domain\"]}\n",
    "        else:\n",
    "            print(f\"merge layout more 2g: {pre_domain}\")\n",
    "            error_info = {\n",
    "                \"error_type\": \"EOFError\",\n",
    "                \"error_message\": \"Memory more than required for vector is (2147483648)\",\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "                \"input_data\": domain_v.value,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            s3_doc_writer.write(error_info)\n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n",
    "\n",
    "\n",
    "# 解析输出数据\n",
    "def parse_output_data(row_data):\n",
    "    row_data.update({\"layout_id\": '_'.join([row_data[\"url_host_name\"], str(row_data[\"layout_id\"])])})\n",
    "    new_row_data_json = json_dumps(row_data)\n",
    "    if len(new_row_data_json) < MAX_OUTPUT_ROW_SIZE:\n",
    "        return {\"value\": new_row_data_json, \"layout_id\": row_data[\"layout_id\"]}\n",
    "    return None\n",
    "\n",
    "\n",
    "# 计算相似度\n",
    "def calculating_similarity(feature_dict, feature, max_layer_n):\n",
    "    for k, v in feature_dict.items():\n",
    "        if any(similarity(feature, h[\"feature\"], max_layer_n) >= SIMILARITY_THRESHOLD for h in v):\n",
    "            return int(k)\n",
    "    return -2\n",
    "\n",
    "\n",
    "# 相似度解析\n",
    "def parse_similarity(_iter):\n",
    "    s3_doc_writer = get_s3_doctor(\"parse_similarity\")\n",
    "    error_info = None\n",
    "    is_no_layout_id = False\n",
    "    for row in _iter:\n",
    "        layout_dict = json_loads(row.layout_dict)\n",
    "        layout_list = layout_dict.get(\"layout_list\", [])\n",
    "        if (len(layout_list) == 1 and layout_list[0] == -1) or len(layout_list) > MAX_LAYOUTLIST_SIZE:\n",
    "            is_no_layout_id = True\n",
    "        feature_dict = layout_dict.get(\"feature_dict\", {})\n",
    "        max_layer_n = layout_dict.get(\"max_layer_n\", 5)\n",
    "        domain = row.domain\n",
    "        count = row.count\n",
    "        file_d = row.file\n",
    "        offset = file_d.offset\n",
    "        length = file_d.length\n",
    "        record_count = file_d.record_count\n",
    "        idx = 0\n",
    "        try:\n",
    "            for detail_data in read_s3_lines_with_range(file_d[\"filepath\"], use_stream=True,\n",
    "                                                        bytes_range=(offset, offset + length)):\n",
    "                idx += 1\n",
    "                if idx > record_count:\n",
    "                    break\n",
    "                detail_data = json_loads(detail_data)\n",
    "                if is_no_layout_id is True:\n",
    "                    layout_id = -1\n",
    "                else:\n",
    "                    try:\n",
    "                        feature = get_feature(detail_data[\"html\"])\n",
    "                        if feature is None or not feature.get(\"tags\"):\n",
    "                            layout_id = -3\n",
    "                        else:\n",
    "                            layout_id = func_timeout(SIM_TIMEOUT_SECONDS, calculating_similarity,\n",
    "                                                     (feature_dict, feature, max_layer_n,))\n",
    "                    except FunctionTimedOut as e:\n",
    "                        error_info = {\n",
    "                            \"error_type\": type(e).__name__,\n",
    "                            \"error_message\": str(e),\n",
    "                            \"traceback\": traceback.format_exc(),\n",
    "                            \"input_data\": str(detail_data),\n",
    "                            \"timestamp\": datetime.now().isoformat()\n",
    "                        }\n",
    "                        s3_doc_writer.write(error_info)\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        error_info = {\n",
    "                            \"error_type\": type(e).__name__,\n",
    "                            \"error_message\": str(e),\n",
    "                            \"traceback\": traceback.format_exc(),\n",
    "                            \"input_data\": str(detail_data),\n",
    "                            \"timestamp\": datetime.now().isoformat()\n",
    "                        }\n",
    "                        s3_doc_writer.write(error_info)\n",
    "                        continue\n",
    "                line = {\n",
    "                    \"track_id\": detail_data[\"track_id\"],\n",
    "                    \"html\": detail_data[\"html\"],\n",
    "                    \"url\": detail_data[\"url\"],\n",
    "                    \"layout_id\": layout_id,\n",
    "                    \"max_layer_n\": max_layer_n,\n",
    "                    \"url_host_name\": domain,\n",
    "                    \"raw_warc_path\": detail_data[\"raw_warc_path\"]\n",
    "                }\n",
    "                json_line = parse_output_data(line)\n",
    "                if json_line is not None:\n",
    "                    yield json_line\n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"error_message\": str(e),\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "                \"input_data\": str(row),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            s3_doc_writer.write(error_info)\n",
    "            continue\n",
    "\n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n",
    "\n",
    "\n",
    "# 依据layout 数据入s3\n",
    "def save_s3_by_layout(outdata_list):\n",
    "    s3_doc_writer = get_s3_doctor(\"similarity_write\")\n",
    "    error_info = None\n",
    "    json_line = None\n",
    "    s3_writer = None\n",
    "    index = 0\n",
    "    output_file_size = 0\n",
    "    for index, row in enumerate(outdata_list):\n",
    "        try:\n",
    "            if output_file_size > MAX_OUTPUT_FILE_SIZE:\n",
    "                if json_line:\n",
    "                    s3_writer.flush()\n",
    "                    s3_writer = None\n",
    "                    output_file_size = 0\n",
    "            json_line = json_loads(row.value)\n",
    "            if s3_writer:\n",
    "                output_file_size += s3_writer.write(json_line)\n",
    "            else:\n",
    "                partition_id = str(uuid.uuid4())\n",
    "                output_file = f\"{OUTPUT_PATH}{partition_id}.jsonl.gz\"\n",
    "                s3_writer = S3DocWriter(output_file)\n",
    "                output_file_size += s3_writer.write(json_line)\n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"error_message\": str(e),\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "                \"input_data\": row.value,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            s3_doc_writer.write(error_info)\n",
    "            continue\n",
    "\n",
    "    if json_line:\n",
    "        s3_writer.flush()\n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n",
    "    yield {\"write_size\": index}\n",
    "\n",
    "\n",
    "# 读取数据，生成index相关字段\n",
    "def read_to_index(_iter):\n",
    "    s3_doc_writer = get_s3_doctor(\"layout_index\")\n",
    "    error_info = None\n",
    "    for path in _iter:\n",
    "        try:\n",
    "            for row in read_s3_rows(path, use_stream=True):\n",
    "                try:\n",
    "                    detail_data = json_loads(row.value)\n",
    "                    layout_id = detail_data[\"layout_id\"]\n",
    "                    url_host_name = detail_data[\"url_host_name\"]\n",
    "                    offset, length = map(int, row.loc.split(\"bytes=\")[-1].split(\",\"))\n",
    "                    yield {\"url_host_name\": url_host_name, \"filepath\": path, \"layout_id\": layout_id, \"offset\": offset,\n",
    "                           \"length\": length}\n",
    "                except Exception as e:\n",
    "                    error_info = {\n",
    "                        \"error_type\": type(e).__name__,\n",
    "                        \"error_message\": str(e),\n",
    "                        \"traceback\": traceback.format_exc(),\n",
    "                        \"input_data\": row.value,\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    s3_doc_writer.write(error_info)\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"error_message\": str(e),\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "                \"input_data\": path,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            s3_doc_writer.write(error_info)\n",
    "    if error_info is not None:\n",
    "        s3_doc_writer.flush()\n",
    "\n",
    "\n",
    "# 创建spark\n",
    "def create_spark(spark_name: str):\n",
    "    global spark\n",
    "    spark = new_spark_session(f\"layout.{spark_name}\", config)\n",
    "    global sc\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "\n",
    "# 关闭spark\n",
    "def close_spark():\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# main func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"第一步 choose domain\"\"\"\n",
    "\n",
    "\n",
    "def get_input_path(INPUT_PATH):\n",
    "    try:\n",
    "        with open(\"./already_exist.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            already_exist = eval(content) if content else []\n",
    "    except:\n",
    "        already_exist = []\n",
    "    input_path_lst = [f.replace(\"s3\", \"s3a\") for f in list(list_s3_objects(INPUT_PATH, recursive=True)) if\n",
    "                      f.endswith(\".jsonl\") and f not in already_exist]\n",
    "    if input_path_lst:\n",
    "        with open(\"./already_exist.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            already_exist.extend(input_path_lst)\n",
    "            f.write(str(already_exist))\n",
    "    get_choose_input_df(input_path_lst)\n",
    "\n",
    "\n",
    "def get_choose_input_df(input_path_lst):\n",
    "    df = spark.read.format(\"json\").load(input_path_lst)\n",
    "    df.cache()\n",
    "    filter_batch_df(df)\n",
    "\n",
    "\n",
    "def filter_batch_df(df: DataFrame):\n",
    "    for i in [3, 2, 1, 0]:\n",
    "        global count_data\n",
    "        count_data = COUNT_MAP[i]\n",
    "        if count_data[2] is None:\n",
    "            filter_df = df.filter(col(\"count\") > count_data[1])\n",
    "        else:\n",
    "            filter_df = df.filter(col(\"count\") > count_data[1]).filter(col(\"count\") <= count_data[2])\n",
    "        total_batch_count(filter_df)\n",
    "        parse_partition_id(filter_df, i)\n",
    "\n",
    "\n",
    "def total_batch_count(filter_df: DataFrame):\n",
    "    global total_count\n",
    "    total_count = filter_df.select(_sum(\"count\")).collect()[0][0]\n",
    "\n",
    "\n",
    "def parse_partition_id(filter_df: DataFrame, i: int):\n",
    "    NUM_PARTITIONS = round(total_count / DATA_SIZE_PER_BATCH)\n",
    "    if i == 0:\n",
    "        repart_df = filter_df.select([\"domain\"]).repartition(NUM_PARTITIONS, col(\"domain\"))\n",
    "        partition_df = repart_df.withColumn(\"partition_id\", spark_partition_id())\n",
    "    else:\n",
    "        weight_datas = filter_df.select([\"domain\", \"count\"]).collect()\n",
    "        partition_list = list(divide_list_to_chunks(weight_datas, NUM_PARTITIONS))\n",
    "        partition_df = spark.createDataFrame(partition_list)\n",
    "    join_to_write(filter_df, partition_df)\n",
    "\n",
    "\n",
    "def join_to_write(filter_df: DataFrame, partition_df: DataFrame, ):\n",
    "    df_with_weight = filter_df.join(partition_df, on=\"domain\")\n",
    "    output_schema = StructType([\n",
    "        StructField('write_size', LongType(), True),\n",
    "    ])\n",
    "\n",
    "    @pandas_udf(output_schema, PandasUDFType.GROUPED_MAP)\n",
    "    def pandas_udf_repartition(data_collected_series):\n",
    "        result = write_by_partitionid(data_collected_series)\n",
    "        if result:\n",
    "            return pd.DataFrame(result)\n",
    "\n",
    "    output_df = df_with_weight.groupby('partition_id').apply(pandas_udf_repartition)\n",
    "    output_df.count()\n",
    "\n",
    "\n",
    "def choose_main(INPUT_PATH):\n",
    "    spark_name = \"choose_domain\"\n",
    "    create_spark(spark_name)\n",
    "    get_input_path(INPUT_PATH)\n",
    "    close_spark()\n",
    "\n",
    "\n",
    "\"\"\" 第二步 layout\"\"\"\n",
    "\n",
    "\n",
    "def get_layout_input_df(batch: str):\n",
    "    input_df = spark.read.format(\"json\").load(batch)\n",
    "    domain_df = parse_valid_data(input_df, batch)\n",
    "    domain_count = domain_df.count()\n",
    "    parse_explode_df(domain_df, domain_count)\n",
    "\n",
    "\n",
    "def parse_valid_data(input_df: DataFrame, batch: str):\n",
    "    data_range = batch.split(\"/\")[-2].split(\"_\")[0]\n",
    "    data_count_rate = RATE_MAP.get(data_range, 1)\n",
    "    domain_df = input_df.withColumn(\"valid_count\", _round(col(\"count\") * data_count_rate).cast(\"integer\"))\n",
    "    return domain_df\n",
    "\n",
    "\n",
    "def parse_explode_df(domain_df: DataFrame, domain_count):\n",
    "    explode_df = domain_df.withColumn(\"file\", explode(col(\"files\"))).drop(\"files\")\n",
    "    parse_get_feature_df(explode_df, domain_df, domain_count)\n",
    "\n",
    "\n",
    "def parse_get_feature_df(explode_df: DataFrame, domain_df: DataFrame, domain_count):\n",
    "    schema = StructType([\n",
    "        StructField('value', StringType(), True),\n",
    "        StructField('domain', StringType(), True),\n",
    "        StructField('valid_count', IntegerType(), True),\n",
    "    ])\n",
    "    feature_df = explode_df.repartition(NUM_PARTITIONS).rdd.mapPartitions(get_all_domain_data).toDF(schema)\n",
    "    sample_by_valid_count(feature_df, domain_df, domain_count)\n",
    "\n",
    "\n",
    "def sample_by_valid_count(feature_df: DataFrame, domain_df: DataFrame, domain_count):\n",
    "    df_with_rand = feature_df.withColumn(\"rand\", expr(\"rand()\"))\n",
    "    row_num_window_spec = Window.partitionBy(\"domain\").orderBy(col(\"rand\"))\n",
    "    df_with_row_num = df_with_rand.withColumn(\"row_num\", row_number().over(row_num_window_spec))\n",
    "    domain_sample_df = df_with_row_num.filter(col(\"row_num\") <= col(\"valid_count\")).drop(\"rand\", \"row_num\",\n",
    "                                                                                         \"valid_count\", \"domain\")\n",
    "    write_domain_data(domain_sample_df)\n",
    "    calculating_layout_every_batch(domain_df, domain_count)\n",
    "\n",
    "\n",
    "def write_domain_data(domain_sample_df: DataFrame):\n",
    "    output_file_size_gb = 0.3\n",
    "    resize_func = spark_resize_file(output_file_size_gb)\n",
    "    new_output_df = resize_func(domain_sample_df)\n",
    "\n",
    "    config[\"skip_output_version\"] = True\n",
    "    config['skip_output_check'] = True\n",
    "    write_any_path(new_output_df, DOMAIN_PATH, config)\n",
    "\n",
    "\n",
    "def calculating_layout_every_batch(domain_df: DataFrame, domain_count):\n",
    "    output_schema = StructType([\n",
    "        StructField('value', StringType(), True),\n",
    "    ])\n",
    "    domain_lst = [f for f in list(list_s3_objects(DOMAIN_PATH, recursive=True)) if f.endswith(\".jsonl\")]\n",
    "    page_content = sc.parallelize(domain_lst, len(domain_lst))\n",
    "    layout_df = page_content.mapPartitions(parse_layout).toDF(output_schema)\n",
    "    write_merge_layout_data(layout_df)\n",
    "    merge_layout_by_layout_id(domain_df, domain_count)\n",
    "\n",
    "\n",
    "def write_merge_layout_data(layout_df: DataFrame):\n",
    "    config[\"skip_output_version\"] = True\n",
    "    config['skip_output_check'] = True\n",
    "    write_any_path(layout_df, BATCH_PATH, config)\n",
    "\n",
    "\n",
    "def merge_layout_by_layout_id(domain_df: DataFrame, domain_count):\n",
    "    mer_output_schema = StructType([\n",
    "        StructField('layout_dict', StringType(), True),\n",
    "        StructField('domain', StringType(), True),\n",
    "    ])\n",
    "\n",
    "    batch_lst = list(list_s3_objects(BATCH_PATH, recursive=False))\n",
    "    batch_page_content = sc.parallelize(batch_lst, len(batch_lst))\n",
    "    merge_layout_df = batch_page_content.mapPartitions(merge_layout).toDF(mer_output_schema)\n",
    "    join_to_write(merge_layout_df, domain_df)\n",
    "\n",
    "\n",
    "def join_to_write(merge_layout_df: DataFrame, domain_df: DataFrame):\n",
    "    join_df = domain_df.join(merge_layout_df, on=\"domain\", how=\"left\")\n",
    "\n",
    "    struct_col = struct(join_df[\"domain\"], join_df[\"count\"], join_df[\"files\"], join_df[\"layout_dict\"])\n",
    "    output_df = join_df.withColumn(\"value\", to_json(struct_col)).select(\"value\")\n",
    "\n",
    "    config[\"skip_output_version\"] = True\n",
    "    config['skip_output_check'] = True\n",
    "    write_any_path(output_df, OUTPUT_PATH, config)\n",
    "\n",
    "\n",
    "def parse_path(batch):\n",
    "    path_list = batch.split('/')\n",
    "    global OUTPUT_PATH\n",
    "    OUTPUT_PATH = f\"{BASE_LAYOUT_OUTPUT_PATH}{path_list[-2]}/{path_list[-1].replace('.jsonl', '')}/\"\n",
    "    global DOMAIN_PATH\n",
    "    DOMAIN_PATH = f\"{BASE_DOMAIN_PATH}{path_list[-2]}/{path_list[-1].replace('.jsonl', '')}/\"\n",
    "    global BATCH_PATH\n",
    "    BATCH_PATH = F\"{BASE_BATCH_PATH}{path_list[-2]}/{path_list[-1].replace('.jsonl', '')}/\"\n",
    "    return path_list\n",
    "\n",
    "\n",
    "def parse_input_path(input_path):\n",
    "    try:\n",
    "        with open(\"./is_layout_complated.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            already_exist = [i for i in content.split(\",\") if i] if content else []\n",
    "    except:\n",
    "        already_exist = []\n",
    "    input_path_lst = [i for i in [f.replace(\"s3\", \"s3a\") for f in list(list_s3_objects(input_path, recursive=True)) if\n",
    "                                  f.endswith(\".jsonl\")] if i not in already_exist]\n",
    "    return input_path_lst\n",
    "\n",
    "\n",
    "def layout_main(INPUT_PATH):\n",
    "    big_batch_lst = list(list_s3_objects(INPUT_PATH, recursive=False))\n",
    "    for big_batch in big_batch_lst:\n",
    "        batch_lst = parse_input_path(big_batch)\n",
    "        for batch in batch_lst:\n",
    "            path_list = parse_path(batch)\n",
    "            spark_name = \"_\".join([path_list[-2], path_list[-1].replace(\".jsonl\", \"\")])\n",
    "            create_spark(spark_name)\n",
    "            get_layout_input_df(batch)\n",
    "            close_spark()\n",
    "            with open(\"./is_layout_complated.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(batch + \",\")\n",
    "\n",
    "\n",
    "\"\"\"第三步 similarity\"\"\"\n",
    "\n",
    "\n",
    "def parse_sim_input_path(input_path):\n",
    "    try:\n",
    "        with open(\"./is_similarity_complated.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            already_exist = [i for i in content.split(\",\") if i] if content else []\n",
    "    except:\n",
    "        already_exist = []\n",
    "    input_path_lst = [i for i in [f.replace(\"s3\", \"s3a\") for f in list(list_s3_objects(input_path, recursive=False))] if\n",
    "                      i not in already_exist]\n",
    "    return input_path_lst\n",
    "\n",
    "\n",
    "def parse_sim_path(batch):\n",
    "    path_list = batch.split('/')\n",
    "    global OUTPUT_PATH\n",
    "    OUTPUT_PATH = f\"{BASE_OUTPUT_PATH}{path_list[-3]}/{path_list[-2]}/\"\n",
    "    return path_list\n",
    "\n",
    "\n",
    "def get_domain_df(batch):\n",
    "    input_f_df = spark.read.format(\"json\").load(batch).filter(col(\"layout_dict\").isNotNull())\n",
    "    input_df = input_f_df.withColumn(\"file\", explode(col(\"files\"))).drop(\"files\")\n",
    "    similarity_every_domain(input_df)\n",
    "\n",
    "\n",
    "def similarity_every_domain(input_df: DataFrame):\n",
    "    schema = StructType([\n",
    "        StructField('value', StringType(), True),\n",
    "        StructField('layout_id', StringType(), True),\n",
    "    ])\n",
    "\n",
    "    all_domain_df = input_df.repartition(NUM_PARTITIONS).rdd.mapPartitions(parse_similarity).toDF(schema)\n",
    "    write_by_layoutid(all_domain_df)\n",
    "\n",
    "\n",
    "def write_by_layoutid(all_domain_df: DataFrame):\n",
    "    output_schema = StructType([\n",
    "        StructField('write_size', IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    final_df = all_domain_df.repartition(WRITE_NUM_PARTITIONS, col(\"layout_id\")).sortWithinPartitions(col(\"layout_id\"))\n",
    "    out_df = final_df.rdd.mapPartitions(save_s3_by_layout)\n",
    "    out_df.count()\n",
    "\n",
    "\n",
    "def sim_main(INPUT_PATH):\n",
    "    big_batch_lst = list(list_s3_objects(INPUT_PATH, recursive=False))\n",
    "    for big_batch in big_batch_lst:\n",
    "        batch_lst = parse_sim_input_path(big_batch)\n",
    "        for batch in batch_lst:\n",
    "            path_list = parse_sim_path(batch)\n",
    "            spark_name = \"similarity.\" + \"_\".join([path_list[-3], path_list[-2]])\n",
    "            create_spark(spark_name)\n",
    "            get_domain_df(batch)\n",
    "            close_spark()\n",
    "            with open(\"./is_similarity_complated.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(batch + \",\")\n",
    "\n",
    "\n",
    "\"\"\"第四步 layout index\"\"\"\n",
    "\n",
    "\n",
    "def parse_index_input_path(input_path: str):\n",
    "    try:\n",
    "        with open(\"./is_index_complated.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            already_exist = [i for i in content.split(\",\") if i] if content else []\n",
    "    except:\n",
    "        already_exist = []\n",
    "    input_path_lst = [i for i in list(list_s3_objects(input_path, recursive=False)) if i not in already_exist]\n",
    "    return input_path_lst\n",
    "\n",
    "\n",
    "def create_index_df(batch: str):\n",
    "    input_path_lst = [i for i in list(list_s3_objects(batch, recursive=True)) if i.endswith(\".jsonl.gz\")]\n",
    "    schema = StructType([\n",
    "        StructField(\"layout_id\", StringType(), True),\n",
    "        StructField(\"url_host_name\", StringType(), True),\n",
    "        StructField(\"filepath\", StringType(), True),\n",
    "        StructField(\"offset\", LongType(), True),\n",
    "        StructField(\"length\", LongType(), True),\n",
    "    ])\n",
    "    page_content = sc.parallelize(input_path_lst, len(input_path_lst))\n",
    "    with_length_df = page_content.mapPartitions(read_to_index).toDF(schema)\n",
    "    data_to_index(with_length_df)\n",
    "\n",
    "\n",
    "def data_to_index(with_length_df: DataFrame):\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    group_df = with_length_df.groupBy([\"url_host_name\", \"layout_id\", \"filepath\"]).agg(\n",
    "        _sum(col(\"length\")).alias(\"length\"),\n",
    "        _min(col(\"offset\")).alias(\"offset\"),\n",
    "        _count(\"*\").alias(\"record_count\")\n",
    "    ).sort(\"url_host_name\", \"layout_id\", \"filepath\").withColumn(\"timestamp\", lit(current_time))\n",
    "\n",
    "    file_df = group_df.withColumn(\"file\",\n",
    "                                  struct(\n",
    "                                      col(\"filepath\").alias(\"filepath\"),\n",
    "                                      col(\"length\").cast(\"long\").alias(\"length\"),\n",
    "                                      col(\"offset\").cast(\"long\").alias(\"offset\"),\n",
    "                                      col(\"record_count\").cast(\"long\").alias(\"record_count\"),\n",
    "                                      col(\"timestamp\").alias(\"timestamp\")\n",
    "                                  )\n",
    "                                  ).select([\"url_host_name\", \"layout_id\", \"file\", \"record_count\"])\n",
    "\n",
    "    result_df = file_df.groupBy(\"layout_id\") \\\n",
    "        .agg(\n",
    "        _sum(\"record_count\").alias(\"count\"),\n",
    "        collect_list(\"file\").alias(\"files\"),\n",
    "        first(\"url_host_name\").alias(\"url_host_name\")\n",
    "    ).orderBy(\"count\", ascending=False)\n",
    "    write_by_two(result_df)\n",
    "\n",
    "\n",
    "def write_by_two(result_df: DataFrame):\n",
    "    struct_col = struct(result_df[\"layout_id\"], result_df[\"count\"], result_df[\"files\"], result_df[\"url_host_name\"])\n",
    "    output_df = result_df.withColumn(\"value\", to_json(struct_col)).select(\"value\")\n",
    "\n",
    "    output_file_size_gb = 2\n",
    "    resize_func = spark_resize_file(output_file_size_gb)\n",
    "    new_output_df = resize_func(output_df)\n",
    "\n",
    "    config[\"skip_output_version\"] = True\n",
    "    config['skip_output_check'] = True\n",
    "\n",
    "    write_any_path(new_output_df, OUTPUT_PATH, config)\n",
    "\n",
    "\n",
    "def parse_index_path(batch):\n",
    "    path_list = batch.split('/')\n",
    "    global OUTPUT_PATH\n",
    "    OUTPUT_PATH = f\"{BASE_OUTPUT_PATH}{path_list[-3]}/{path_list[-2]}/\"\n",
    "    return path_list\n",
    "\n",
    "\n",
    "def index_main(INPUT_PATH):\n",
    "    big_batch_lst = list(list_s3_objects(INPUT_PATH, recursive=False))\n",
    "    for big_batch in big_batch_lst:\n",
    "        batch_lst = parse_index_input_path(big_batch)\n",
    "        for batch in batch_lst:\n",
    "            path_list = parse_path(batch)\n",
    "            spark_name = \".\".join([path_list[-3], path_list[-2]])\n",
    "            create_spark(spark_name)\n",
    "            create_index_df(batch)\n",
    "            with open(\"./is_index_complated.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(batch + \",\")\n",
    "\n",
    "            close_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_main(INPUT_PATH)\n",
    "layout_main(CHOOSE_OUTPUT_PATH)\n",
    "sim_main(BASE_LAYOUT_OUTPUT_PATH)\n",
    "index_main(BASE_SIM_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python webkit_venv (ipykernel)",
   "language": "python",
   "name": "webkit_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
