{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a86ea9b-beca-4324-b101-5684b3ed0955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T07:59:54.207430Z",
     "iopub.status.busy": "2025-09-18T07:59:54.207141Z",
     "iopub.status.idle": "2025-09-18T07:59:58.189455Z",
     "shell.execute_reply": "2025-09-18T07:59:58.188614Z",
     "shell.execute_reply.started": "2025-09-18T07:59:54.207413Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载spark配置\n",
    "from xinghe.spark import *\n",
    "from app.common.json_util import *\n",
    "from xinghe.s3 import *\n",
    "from xinghe.s3.read import *\n",
    "from xinghe.ops.spark import spark_resize_file\n",
    "\n",
    "import os\n",
    "os.environ[\"LLM_WEB_KIT_CFG_PATH\"] = \"/share/xxx/.llm-web-kit.jsonc\"\n",
    "\n",
    "import heapq\n",
    "import math\n",
    "import uuid\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from copy import copy\n",
    "from collections import defaultdict\n",
    "from func_timeout import FunctionTimedOut, func_timeout\n",
    "from datetime import datetime\n",
    "\n",
    "from llm_web_kit.html_layout.html_layout_cosin import cluster_html_struct, get_feature, similarity, sum_tags\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.functions import when, spark_partition_id, row_number, col, collect_list, struct, from_json, expr, count as _count, pandas_udf, PandasUDFType, round as _round, lit, to_json, explode, min as _min, sum as _sum, first, max as _max\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"spark_conf_name\": \"spark_4\",\n",
    "    \"skip_success_check\": True,\n",
    "    \"spark.storage.memoryFraction\": 0.8,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a927594f-071f-4ba2-b884-73d5389fcd12",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc262e56-2da9-4462-94d4-b2917054eed2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T07:59:58.190425Z",
     "iopub.status.busy": "2025-09-18T07:59:58.190101Z",
     "iopub.status.idle": "2025-09-18T07:59:58.197584Z",
     "shell.execute_reply": "2025-09-18T07:59:58.197218Z",
     "shell.execute_reply.started": "2025-09-18T07:59:58.190407Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_spark(spark_name: str):\n",
    "    global spark\n",
    "    spark = new_spark_session(f\"cluster.layout.{spark_name}\", config)\n",
    "    global sc\n",
    "    sc = spark.sparkContext\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "def close_spark():\n",
    "    spark.stop()\n",
    "\n",
    "def get_s3_doctor(target_theme):\n",
    "    partition_id = str(uuid.uuid4())\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d\")\n",
    "    # TODO 错误日志存放地址\n",
    "    error_log_path = f\"{ERROR_PATH}{target_theme}/{current_time}/{partition_id}.jsonl\"\n",
    "    s3_doc_writer = S3DocWriter(path=error_log_path)\n",
    "    return s3_doc_writer\n",
    "\n",
    "def crush_read_path(path):\n",
    "    if isinstance(path, list):\n",
    "        return [f.replace('s3:', 's3a:') for f in path]\n",
    "    else:\n",
    "        return path.replace('s3:', 's3a:')\n",
    "\n",
    "BASE_PARTITION_ID = 0\n",
    "\n",
    "COUNT_MAP = [\n",
    "    (\"1-1600\", 1, 1600),\n",
    "    (\"1600-1.5w\", 1600, 15000),\n",
    "    (\"1.5w-10w\", 15000, 100000),\n",
    "    (\"10w\", 100000, None)\n",
    "]\n",
    "DATA_SIZE_PER_BATCH = 1000000000\n",
    "\n",
    "MAX_LAYOUTLIST_SIZE = 200\n",
    "SIMILARITY_THRESHOLD = 0.95\n",
    "TIMEOUT_SECONDS = 3600 * 5  # 超时时间\n",
    "SIM_TIMEOUT_SECONDS = 60 * 2  # 超时时间\n",
    "MAX_OUTPUT_ROW_SIZE = 1024 * 1024 * 1024 * 1.7\n",
    "MAX_OUTPUT_FILE_SIZE = 1024 * 1024 * 1024 * 1  # 输出输出文件大小限制\n",
    "NUM_PARTITIONS = 4000\n",
    "WRITE_NUM_PARTITIONS = 2000\n",
    "RATE_MAP = {\"10w\": 0.1, \"1.5w-10w\": 0.2, \"1600-1.5w\": 0.5, \"1-1600\": 0.7}\n",
    "\n",
    "ERROR_PATH = \"s3://xxx/\"\n",
    "\n",
    "INPUT_PATH = \"s3a://xxxx/\"\n",
    "CHOOSE_DOMAIN_OUTPUT_PATH = \"s3://xxx/\"\n",
    "\n",
    "CLUSTER_LAYOUT_BASE_OUTPUT_PATH = \"s3://xxx/\"\n",
    "BASE_DOMAIN_PATH = \"s3://xxx/\"\n",
    "BASE_BATCH_PATH = \"s3://xxx/\"\n",
    "\n",
    "LAYOUT_SIM_BASE_OUTPUT_PATH = \"s3://xxx/\"\n",
    "\n",
    "LAYOUT_INDEX_BASE_OUTPUT_PATH = \"s3://xxx/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120df5bb-c23b-4629-8722-dc6ad661aa31",
   "metadata": {},
   "source": [
    "# choose_domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59be7dbc-12b7-4fce-8e6e-37da745e0689",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42775d11-03a0-4a7d-808d-f6c1297f46a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T07:59:58.198289Z",
     "iopub.status.busy": "2025-09-18T07:59:58.198011Z",
     "iopub.status.idle": "2025-09-18T07:59:58.207870Z",
     "shell.execute_reply": "2025-09-18T07:59:58.207498Z",
     "shell.execute_reply.started": "2025-09-18T07:59:58.198275Z"
    }
   },
   "outputs": [],
   "source": [
    "def divide_list_to_chunks(values, n_chunks):\n",
    "    assignments = dict()\n",
    "    chunks = [(0, i) for i in range(n_chunks)]\n",
    "    heapq.heapify(chunks)\n",
    "\n",
    "    indexed_values = sorted([(val[\"count\"], idx, val[\"domain\"]) for idx, val in enumerate(values)], key=lambda x: -x[0])\n",
    "    for weight, idx, name in indexed_values:\n",
    "        current_sum, chunk_id = heapq.heappop(chunks)\n",
    "        assignments[name] = chunk_id\n",
    "        new_sum = current_sum + weight\n",
    "        heapq.heappush(chunks, (new_sum, chunk_id))\n",
    "        yield Row(domain=name, partition_id=chunk_id)\n",
    "\n",
    "def write_by_partitionid(_iter):\n",
    "    detail_data = None\n",
    "    s3_writer = None\n",
    "    for index, detail_data in _iter.iterrows():\n",
    "        line = {\n",
    "            \"domain\": detail_data[\"domain\"],\n",
    "            \"count\": detail_data[\"count\"],\n",
    "            \"partition_id\": detail_data[\"partition_id\"],\n",
    "            \"files\": detail_data[\"files\"].tolist(),\n",
    "        }\n",
    "        if s3_writer:\n",
    "            s3_writer.write(line)\n",
    "        else:\n",
    "            partition_id = detail_data[\"partition_id\"] + BASE_PARTITION_ID\n",
    "            output_file = f\"{CHOOSE_DOMAIN_OUTPUT_PATH}{count_data[0]}_{total_count}/{partition_id}.jsonl\"\n",
    "            s3_writer = S3DocWriter(output_file)\n",
    "            s3_writer.write(line)\n",
    "\n",
    "    if detail_data is not None:\n",
    "        s3_writer.flush()\n",
    "    yield {\"write_size\": index}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97e89b-7ef3-46f9-b3e9-8041ba9f8a77",
   "metadata": {},
   "source": [
    "## choose domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb041bd-5fff-4bc5-901b-94b539d71e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T07:59:58.208498Z",
     "iopub.status.busy": "2025-09-18T07:59:58.208321Z",
     "iopub.status.idle": "2025-09-18T07:59:58.221785Z",
     "shell.execute_reply": "2025-09-18T07:59:58.221424Z",
     "shell.execute_reply.started": "2025-09-18T07:59:58.208484Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_choose_input_path(INPUT_PATH):\n",
    "    try:\n",
    "        with open(\"./already_exist.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            already_exist = eval(content) if content else []\n",
    "    except:\n",
    "        already_exist = []\n",
    "    input_path_lst = [f.replace(\"s3\", \"s3a\") for f in list(list_s3_objects(INPUT_PATH, recursive=True)) if f.endswith(\".jsonl\") and f not in already_exist]\n",
    "    if input_path_lst:\n",
    "        with open(\"./already_exist.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            already_exist.extend(input_path_lst)\n",
    "            f.write(str(already_exist))\n",
    "    get_choose_input_df(input_path_lst)\n",
    "\n",
    "def get_choose_input_df(input_path_lst):\n",
    "    input_path_lst = crush_read_path(input_path_lst)\n",
    "    df = spark.read.format(\"json\").load(input_path_lst)\n",
    "    df.cache()\n",
    "    filter_batch_df(df)\n",
    "\n",
    "def filter_batch_df(df: DataFrame):\n",
    "    for i in [3,2,1,0]:\n",
    "        global count_data\n",
    "        count_data = COUNT_MAP[i]\n",
    "        if count_data[2] is None:\n",
    "            filter_df = df.filter(col(\"count\") > count_data[1])\n",
    "        else:\n",
    "            filter_df = df.filter(col(\"count\") > count_data[1]).filter(col(\"count\") <= count_data[2])\n",
    "        total_batch_count(filter_df)\n",
    "        if total_count < 1:\n",
    "            continue\n",
    "        parse_partition_id(filter_df, i)\n",
    "\n",
    "def total_batch_count(filter_df: DataFrame):\n",
    "    global total_count\n",
    "    total_count = filter_df.select(_sum(\"count\")).collect()[0][0]\n",
    "\n",
    "def parse_partition_id(filter_df: DataFrame, i: int):\n",
    "    NUM_PARTITIONS = math.ceil(total_count/DATA_SIZE_PER_BATCH)\n",
    "    if i == 0:\n",
    "        repart_df = filter_df.select([\"domain\"]).repartition(NUM_PARTITIONS, col(\"domain\"))\n",
    "        partition_df = repart_df.withColumn(\"partition_id\", spark_partition_id())\n",
    "    else:\n",
    "        # 分区数取决于total_count 和每批次的数据量级\n",
    "        weight_datas = filter_df.select([\"domain\", \"count\"]).collect()\n",
    "        partition_list = list(divide_list_to_chunks(weight_datas, NUM_PARTITIONS))\n",
    "        partition_df = spark.createDataFrame(partition_list)\n",
    "    join_to_write_choose(filter_df, partition_df)\n",
    "\n",
    "def join_to_write_choose(filter_df: DataFrame, partition_df: DataFrame, ):\n",
    "    df_with_weight = filter_df.join(partition_df, on=\"domain\")\n",
    "    \n",
    "    output_schema = StructType([\n",
    "        StructField('write_size', LongType(), True),\n",
    "    ])\n",
    "    \n",
    "    @pandas_udf(output_schema, PandasUDFType.GROUPED_MAP)\n",
    "    def pandas_udf_repartition(data_collected_series):\n",
    "        result = write_by_partitionid(data_collected_series)\n",
    "        if result:\n",
    "            return pd.DataFrame(result)\n",
    "        \n",
    "    output_df = df_with_weight.groupby('partition_id').apply(pandas_udf_repartition)\n",
    "\n",
    "def choose_main():\n",
    "    spark_name = \"choose_domain\"\n",
    "    create_spark(spark_name)\n",
    "    get_choose_input_path(INPUT_PATH)\n",
    "    close_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79554e66-6acf-4135-a85d-a48ec6da2cd4",
   "metadata": {},
   "source": [
    "# cluster_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27be3e17-f695-4bef-a401-3ae14a7164bf",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13b494b-eb53-46f7-a1eb-4a264032365a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T07:59:58.222485Z",
     "iopub.status.busy": "2025-09-18T07:59:58.222287Z",
     "iopub.status.idle": "2025-09-18T07:59:58.245952Z",
     "shell.execute_reply": "2025-09-18T07:59:58.245591Z",
     "shell.execute_reply.started": "2025-09-18T07:59:58.222471Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_domain_data_cluster(_iter):\n",
    "    s3_doc_writer = get_s3_doctor(\"get_feature\")\n",
    "    error_info = None\n",
    "    for row in _iter:\n",
    "        valid_count = row.valid_count\n",
    "        file_d = row.file\n",
    "        offset = file_d.offset\n",
    "        record_count = file_d.record_count\n",
    "        try:\n",
    "            for detail_data in read_s3_by_offset_limit(file_d.filepath, offset, limit=record_count):\n",
    "                try:\n",
    "                    detail_data = json_loads(detail_data.value)\n",
    "                    feature = get_feature(detail_data[\"html\"])\n",
    "                    if feature is None or not feature.get(\"tags\"):\n",
    "                        continue\n",
    "                    layer_n, total_n = sum_tags(feature[\"tags\"])\n",
    "                    line = {\n",
    "                        \"date\": detail_data[\"date\"],\n",
    "                        \"track_id\": detail_data[\"track_id\"],\n",
    "                        \"url\": detail_data[\"url\"],\n",
    "                        \"raw_warc_path\": detail_data[\"raw_warc_path\"],\n",
    "                        \"domain\": row.domain,\n",
    "                        \"sub_path\": row.domain,\n",
    "                        \"valid_count\": valid_count,\n",
    "                        \"feature\": feature,\n",
    "                        \"layer_n\": layer_n,\n",
    "                        \"total_n\": total_n\n",
    "                    }\n",
    "                    line = json_dumps(line)\n",
    "                    if len(line) < MAX_OUTPUT_ROW_SIZE:\n",
    "                        yield Row(**{\"value\": line, \"domain\": row.domain, \"valid_count\": valid_count})\n",
    "                    else:\n",
    "                        error_info = {\n",
    "                            \"error_type\": \"EOFError\",\n",
    "                            \"error_message\": \"Memory more than required for vector is (2147483648)\",\n",
    "                            \"traceback\": traceback.format_exc(),\n",
    "                            \"input_data\": detail_data,\n",
    "                            \"timestamp\": datetime.now().isoformat()\n",
    "                        }\n",
    "                        s3_doc_writer.write(error_info)\n",
    "                        continue\n",
    "                except Exception as e:\n",
    "                    error_info = {\n",
    "                        \"error_type\": type(e).__name__,\n",
    "                        \"error_message\": str(e),\n",
    "                        \"traceback\": traceback.format_exc(),\n",
    "                        \"input_data\": detail_data,\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    s3_doc_writer.write(error_info)\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"error_message\": str(e),\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "                \"input_data\": str(row),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            s3_doc_writer.write(error_info)\n",
    "            continue\n",
    "                        \n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n",
    "\n",
    "def crush_output_data_cluster(output_data):\n",
    "    output_data_json = json_dumps(output_data)\n",
    "    if len(output_data_json) < MAX_OUTPUT_ROW_SIZE:\n",
    "        return output_data_json\n",
    "    else:\n",
    "        return output_data\n",
    "\n",
    "def parse_batch_data_cluster(fpath):\n",
    "    sample_list = []\n",
    "    index = 0\n",
    "    for domain_v in read_s3_rows(fpath, use_stream=True):\n",
    "        index += 1\n",
    "        if index != 0 and not index % 800:\n",
    "            yield sample_list\n",
    "            sample_list = []\n",
    "        domain_data = json_loads(domain_v.value)\n",
    "        try:\n",
    "            lines = {\n",
    "                \"feature\": domain_data[\"feature\"],\n",
    "                \"layer_n\": domain_data[\"layer_n\"],\n",
    "                \"total_n\": domain_data[\"total_n\"],\n",
    "                \"track_id\": domain_data[\"track_id\"],\n",
    "                \"url\": domain_data[\"url\"],\n",
    "                \"domain\": domain_data[\"domain\"],\n",
    "                \"raw_warc_path\": domain_data[\"raw_warc_path\"],\n",
    "                \"date\": domain_data[\"date\"]\n",
    "            }\n",
    "            sample_list.append(lines)\n",
    "        except:\n",
    "            pass\n",
    "    if sample_list:\n",
    "        yield sample_list\n",
    "\n",
    "def calculating_layout_cluster(current_host_name, sample_list):\n",
    "    cluster_datas, layout_list = cluster_html_struct(sample_list)\n",
    "    feature_dict = defaultdict(list)\n",
    "    max_layer_n = cluster_datas[0][\"max_layer_n\"]\n",
    "    # 每个layout类别抽取3个网页\n",
    "    for r in cluster_datas:\n",
    "        layout_id = r[\"layout_id\"]\n",
    "        if layout_id == -1:\n",
    "            continue\n",
    "        if len(feature_dict[layout_id]) < 3:\n",
    "            cr = copy(r)\n",
    "            feature_dict[layout_id].append(cr)\n",
    "    if layout_list:\n",
    "        layout_tmp_dict = crush_output_data_cluster({\"domain\": current_host_name, \"sub_path\": current_host_name, \"feature_dict\": dict(feature_dict), \"layout_list\": layout_list, \"max_layer_n\": max_layer_n})\n",
    "        yield layout_tmp_dict\n",
    "\n",
    "def parse_layout_cluster(domain_list):\n",
    "    s3_doc_writer = get_s3_doctor(\"parse_layout\")\n",
    "    error_info = None\n",
    "    for fpath in domain_list:\n",
    "        try:\n",
    "            for sample_list in parse_batch_data_cluster(fpath):\n",
    "                try:\n",
    "                    if len(sample_list) > 1:\n",
    "                        current_host_name = sample_list[0][\"domain\"]\n",
    "                        for line in func_timeout(TIMEOUT_SECONDS, calculating_layout_cluster, (current_host_name, sample_list,)):\n",
    "                            if isinstance(line, str):\n",
    "                                yield {\"value\": line}\n",
    "                            elif isinstance(line, Dict):\n",
    "                                error_info = {\n",
    "                                    \"error_type\": \"MemoryError by row\",\n",
    "                                    \"error_message\": \"MemoryError by row\",\n",
    "                                    \"traceback\": traceback.format_exc(),\n",
    "                                    \"input_data\": line,\n",
    "                                    \"timestamp\": datetime.now().isoformat()\n",
    "                                }\n",
    "                                s3_doc_writer.write(error_info)\n",
    "                                continue\n",
    "                except FunctionTimedOut as e:\n",
    "                    error_info = {\n",
    "                        \"error_type\": type(e).__name__,\n",
    "                        \"error_message\": str(e),\n",
    "                        \"traceback\": traceback.format_exc(),\n",
    "                        \"input_data\": str(sample_list),\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    s3_doc_writer.write(error_info)\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    error_info = {\n",
    "                        \"error_type\": type(e).__name__,\n",
    "                        \"error_message\": str(e),\n",
    "                        \"traceback\": traceback.format_exc(),\n",
    "                        \"input_data\": str(sample_list),\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    s3_doc_writer.write(error_info)\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"error_message\": str(e),\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "                \"input_data\": fpath,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            s3_doc_writer.write(error_info)\n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n",
    "\n",
    "def layout_similarity_cluster(layout_d1, layout_d2):\n",
    "    max_layer_n = max(layout_d1[\"max_layer_n\"], layout_d2[\"max_layer_n\"])\n",
    "    layout_last = layout_d1\n",
    "    layout_last[\"max_layer_n\"] = max_layer_n\n",
    "    layout_list1 = layout_d1[\"layout_list\"]\n",
    "    layout_list2 = layout_d2[\"layout_list\"]\n",
    "    if len(layout_list1) > MAX_LAYOUTLIST_SIZE or len(layout_list2) > MAX_LAYOUTLIST_SIZE:\n",
    "        return layout_d1 if len(layout_list1) > len(layout_list2) else layout_d2\n",
    "    max_layout_id = max(layout_list1)\n",
    "    feature_dict1 = layout_d1[\"feature_dict\"]\n",
    "    feature_dict2 = layout_d2[\"feature_dict\"]\n",
    "    ls_v = []\n",
    "    [ls_v.extend(v) for k, v in feature_dict1.items()]\n",
    "    exist_layout_num = 0\n",
    "    for new_k, new_v in feature_dict2.items():\n",
    "        add_tmp_dict_v = True\n",
    "        for new_d in new_v:\n",
    "            if any(similarity(new_d[\"feature\"], h[\"feature\"], max_layer_n) >= SIMILARITY_THRESHOLD for h in ls_v):\n",
    "                add_tmp_dict_v = False\n",
    "                exist_layout_num += 1\n",
    "                break\n",
    "        if add_tmp_dict_v is True:\n",
    "            max_layout_id += 1\n",
    "            layout_last[\"feature_dict\"][str(max_layout_id)] = new_v\n",
    "            layout_last[\"layout_list\"].append(max_layout_id)\n",
    "    return layout_last\n",
    "                    \n",
    "def merge_layout_cluster(domain_list):\n",
    "    s3_doc_writer = get_s3_doctor(\"merge_layout\")\n",
    "    error_info = None\n",
    "    pre_domain = {}\n",
    "    domain_v = None\n",
    "    # 两两进行合并\n",
    "    index = 0\n",
    "    for domain_f in domain_list:\n",
    "        domain_paths = [f for f in list(list_s3_objects(domain_f, recursive=True)) if f.endswith(\".jsonl\")]\n",
    "        for fpath in domain_paths:\n",
    "            try:\n",
    "                for domain_v in read_s3_rows(fpath, use_stream=True):\n",
    "                    index += 1\n",
    "                    if index == 1:\n",
    "                        pre_domain = json_loads(domain_v.value)\n",
    "                    else:\n",
    "                        try:\n",
    "                            pre_domain = layout_similarity_cluster(pre_domain, json_loads(domain_v.value))\n",
    "                        except Exception as e:\n",
    "                            error_info = {\n",
    "                                \"error_type\": type(e).__name__,\n",
    "                                \"error_message\": str(e),\n",
    "                                \"traceback\": traceback.format_exc(),\n",
    "                                \"input_data\": domain_v.value,\n",
    "                                \"timestamp\": datetime.now().isoformat()\n",
    "                            }\n",
    "                            s3_doc_writer.write(error_info)\n",
    "                            continue\n",
    "            except Exception as e:\n",
    "                error_info = {\n",
    "                    \"error_type\": type(e).__name__,\n",
    "                    \"error_message\": str(e),\n",
    "                    \"traceback\": traceback.format_exc(),\n",
    "                    \"input_data\": domain_v.value,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "                s3_doc_writer.write(error_info)\n",
    "                \n",
    "    if pre_domain:            \n",
    "        output_data = json_dumps(pre_domain)\n",
    "        if len(output_data) < MAX_OUTPUT_ROW_SIZE:\n",
    "            yield {\"layout_dict\": output_data, \"domain\": pre_domain[\"domain\"]}\n",
    "        else:\n",
    "            error_info = {\n",
    "                \"error_type\": \"EOFError\",\n",
    "                \"error_message\": \"Memory more than required for vector is (2147483648)\",\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "                \"input_data\": domain_v.value,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            s3_doc_writer.write(error_info)\n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6425f161-df4f-4f1c-a64a-bf2f4bd9be6a",
   "metadata": {},
   "source": [
    "## main func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3755fc47-8a51-4986-a2bb-bf7b9f0ace1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T07:59:58.247462Z",
     "iopub.status.busy": "2025-09-18T07:59:58.247318Z",
     "iopub.status.idle": "2025-09-18T07:59:58.261509Z",
     "shell.execute_reply": "2025-09-18T07:59:58.261152Z",
     "shell.execute_reply.started": "2025-09-18T07:59:58.247448Z"
    }
   },
   "outputs": [],
   "source": [
    "# get data by domain\n",
    "def get_cluster_input_df(batch: str, path_list: List):\n",
    "    batch = crush_read_path(batch)\n",
    "    input_df = spark.read.format(\"json\").load(batch)\n",
    "    domain_df = parse_valid_data_cluster(input_df, path_list)\n",
    "    domain_count = domain_df.count()\n",
    "    parse_explode_df_cluster(domain_df, domain_count)\n",
    "    \n",
    "def parse_valid_data_cluster(input_df: DataFrame, path_list: List):\n",
    "    data_range = path_list[-2].split(\"_\")[0]\n",
    "    data_count_rate = RATE_MAP.get(data_range, 1)\n",
    "    domain_df = input_df.withColumn(\"valid_count\", _round(col(\"count\") * data_count_rate).cast(\"integer\"))\n",
    "    return domain_df\n",
    "\n",
    "def parse_explode_df_cluster(domain_df: DataFrame, domain_count):\n",
    "    explode_df = domain_df.withColumn(\"file\", explode(col(\"files\"))).drop(\"files\")\n",
    "    parse_get_feature_df_cluster(explode_df, domain_df, domain_count)\n",
    "\n",
    "def parse_get_feature_df_cluster(explode_df: DataFrame, domain_df: DataFrame, domain_count):\n",
    "    schema = StructType([\n",
    "        StructField('value', StringType(), True),\n",
    "        StructField('domain', StringType(), True),\n",
    "        StructField('valid_count', IntegerType(), True),\n",
    "    ])    \n",
    "    feature_df = explode_df.repartition(NUM_PARTITIONS).rdd.mapPartitions(get_all_domain_data_cluster).toDF(schema)\n",
    "    sample_by_valid_count_cluster(feature_df, domain_df, domain_count)\n",
    "\n",
    "def sample_by_valid_count_cluster(feature_df: DataFrame, domain_df: DataFrame, domain_count):\n",
    "    df_with_rand = feature_df.withColumn(\"rand\", expr(\"rand()\"))\n",
    "    row_num_window_spec = Window.partitionBy(\"domain\").orderBy(col(\"rand\"))\n",
    "    df_with_row_num = df_with_rand.withColumn(\"row_num\", row_number().over(row_num_window_spec))\n",
    "    domain_sample_df = df_with_row_num.filter(col(\"row_num\") <= col(\"valid_count\")).drop(\"rand\", \"row_num\", \"valid_count\", \"domain\")\n",
    "    write_domain_data(domain_sample_df)\n",
    "    calculating_layout_every_batch_cluster(domain_df, domain_count)\n",
    "\n",
    "def write_domain_data(domain_sample_df: DataFrame):\n",
    "    output_file_size_gb = 0.3\n",
    "    resize_func = spark_resize_file(output_file_size_gb)\n",
    "    new_output_df = resize_func(domain_sample_df)\n",
    "    \n",
    "    config[\"skip_output_version\"] = True\n",
    "    config['skip_output_check'] = True\n",
    "    write_any_path(new_output_df, DOMAIN_PATH, config)\n",
    "    \n",
    "def calculating_layout_every_batch_cluster(domain_df: DataFrame, domain_count):\n",
    "    output_schema = StructType([\n",
    "        StructField('value', StringType(), True),\n",
    "    ])\n",
    "    domain_lst = [f for f in list(list_s3_objects(DOMAIN_PATH, recursive=True)) if f.endswith(\".jsonl\")]\n",
    "    if len(domain_lst) > NUM_PARTITIONS:\n",
    "        page_content = sc.parallelize(domain_lst, NUM_PARTITIONS)\n",
    "    else:\n",
    "        page_content = sc.parallelize(domain_lst, len(domain_lst))\n",
    "    layout_df = page_content.mapPartitions(parse_layout_cluster).toDF(output_schema)\n",
    "    write_merge_layout_data(layout_df)\n",
    "    merge_layout_by_layout_id_cluster(domain_df, domain_count)\n",
    "\n",
    "def write_merge_layout_data(layout_df: DataFrame):\n",
    "    config[\"skip_output_version\"] = True\n",
    "    config['skip_output_check'] = True\n",
    "    write_any_path(layout_df, BATCH_PATH, config)\n",
    "\n",
    "def merge_layout_by_layout_id_cluster(domain_df: DataFrame, domain_count):\n",
    "    mer_output_schema = StructType([\n",
    "        StructField('layout_dict', StringType(), True),\n",
    "        StructField('domain', StringType(), True),\n",
    "    ])\n",
    "\n",
    "    batch_lst = list(list_s3_objects(BATCH_PATH, recursive=False))\n",
    "    batch_page_content = sc.parallelize(batch_lst, len(batch_lst))\n",
    "    merge_layout_df = batch_page_content.mapPartitions(merge_layout_cluster).toDF(mer_output_schema)\n",
    "    join_to_write_cluster(merge_layout_df, domain_df)\n",
    "\n",
    "def join_to_write_cluster(merge_layout_df: DataFrame, domain_df: DataFrame):\n",
    "    join_df = domain_df.join(merge_layout_df, on=\"domain\", how=\"left\")\n",
    "\n",
    "    struct_col = struct(join_df[\"domain\"], join_df[\"count\"], join_df[\"files\"], join_df[\"layout_dict\"])\n",
    "    output_df = join_df.withColumn(\"value\", to_json(struct_col)).select(\"value\")\n",
    "\n",
    "    config[\"skip_output_version\"] = True\n",
    "    config['skip_output_check'] = True\n",
    "    write_any_path(output_df, OUTPUT_PATH, config)\n",
    "\n",
    "def parse_cluster_path(batch):\n",
    "    path_list = batch.split('/')\n",
    "    global OUTPUT_PATH\n",
    "    OUTPUT_PATH = f\"{CLUSTER_LAYOUT_BASE_OUTPUT_PATH}{path_list[-2]}/{path_list[-1].replace('.jsonl', '')}/\"\n",
    "    global DOMAIN_PATH\n",
    "    DOMAIN_PATH = f\"{BASE_DOMAIN_PATH}{path_list[-2]}/{path_list[-1].replace('.jsonl', '')}/\"\n",
    "    global BATCH_PATH\n",
    "    BATCH_PATH = F\"{BASE_BATCH_PATH}{path_list[-2]}/{path_list[-1].replace('.jsonl', '')}/\"\n",
    "    return path_list\n",
    "\n",
    "def parse_cluster_input_path(input_path):\n",
    "    try:\n",
    "        with open(\"./is_layout_complated.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            already_exist = [i for i in content.split(\",\") if i] if content else []\n",
    "    except:\n",
    "        already_exist = []\n",
    "    input_path_lst = [i for i in [f.replace(\"s3\", \"s3a\") for f in list(list_s3_objects(input_path, recursive=True)) if f.endswith(\".jsonl\")] if i not in already_exist]\n",
    "    return input_path_lst\n",
    "\n",
    "def layout_main(big_batch):\n",
    "    input_path_lst = parse_cluster_input_path(big_batch)\n",
    "    for batch in input_path_lst:\n",
    "        path_list = parse_cluster_path(batch)\n",
    "        spark_name = '_'.join([path_list[-2], path_list[-1].replace('.jsonl', '')])\n",
    "        create_spark(spark_name)\n",
    "        get_cluster_input_df(batch, path_list)\n",
    "        close_spark()\n",
    "        with open(\"./is_layout_complated.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(\",\".join(batch) + \",\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d822db16-fdae-4273-8589-aa8d761b5065",
   "metadata": {},
   "source": [
    "# cluster_layout_little"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c74424-9d34-4335-9264-a651f60eaf70",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d67976-6609-415d-91b0-a815d756f02d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T07:59:58.262281Z",
     "iopub.status.busy": "2025-09-18T07:59:58.262011Z",
     "iopub.status.idle": "2025-09-18T07:59:58.281355Z",
     "shell.execute_reply": "2025-09-18T07:59:58.281002Z",
     "shell.execute_reply.started": "2025-09-18T07:59:58.262267Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_domain_data_cluster_little(_iter):\n",
    "    s3_doc_writer = get_s3_doctor(\"get_feature\")\n",
    "    error_info = None\n",
    "    for row in _iter:\n",
    "        valid_count = row.valid_count\n",
    "        file_d = row.file\n",
    "        offset = file_d.offset\n",
    "        record_count = file_d.record_count\n",
    "        try:\n",
    "            for detail_data in read_s3_by_offset_limit(file_d.filepath, offset, limit=record_count):\n",
    "                try:\n",
    "                    detail_data = json_loads(detail_data.value)\n",
    "                    feature = get_feature(detail_data[\"html\"])\n",
    "                    if feature is None or not feature.get(\"tags\"):\n",
    "                        continue\n",
    "                    layer_n, total_n = sum_tags(feature[\"tags\"])\n",
    "                    line = {\n",
    "                        \"date\": detail_data[\"date\"],\n",
    "                        \"track_id\": detail_data[\"track_id\"],\n",
    "                        \"url\": detail_data[\"url\"],\n",
    "                        \"raw_warc_path\": detail_data[\"raw_warc_path\"],\n",
    "                        \"domain\": row.domain,\n",
    "                        \"sub_path\": row.domain,\n",
    "                        \"valid_count\": valid_count,\n",
    "                        \"feature\": feature,\n",
    "                        \"layer_n\": layer_n,\n",
    "                        \"total_n\": total_n\n",
    "                    }\n",
    "                    line = json_dumps(line)\n",
    "                    if len(line) < MAX_OUTPUT_ROW_SIZE:\n",
    "                        yield Row(**{\"value\": line, \"domain\": row.domain, \"valid_count\": valid_count})\n",
    "                    else:\n",
    "                        error_info = {\n",
    "                            \"error_type\": \"EOFError\",\n",
    "                            \"error_message\": \"Memory more than required for vector is (2147483648)\",\n",
    "                            \"traceback\": traceback.format_exc(),\n",
    "                            \"input_data\": detail_data,\n",
    "                            \"timestamp\": datetime.now().isoformat()\n",
    "                        }\n",
    "                        s3_doc_writer.write(error_info)\n",
    "                        continue\n",
    "                except Exception as e:\n",
    "                    error_info = {\n",
    "                        \"error_type\": type(e).__name__,\n",
    "                        \"error_message\": str(e),\n",
    "                        \"traceback\": traceback.format_exc(),\n",
    "                        \"input_data\": detail_data,\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    s3_doc_writer.write(error_info)\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"error_message\": str(e),\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "                \"input_data\": str(row),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            s3_doc_writer.write(error_info)\n",
    "            continue\n",
    "                        \n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n",
    "\n",
    "def crush_output_data_cluster_little(output_data):\n",
    "    output_data_json = json_dumps(output_data)\n",
    "    if len(output_data_json) < MAX_OUTPUT_ROW_SIZE:\n",
    "        return output_data_json\n",
    "    else:\n",
    "        return output_data\n",
    "\n",
    "def parse_batch_data_cluster_little(domain_list):\n",
    "    sample_list = []\n",
    "    for index, domain_v in domain_list.iterrows():\n",
    "        if index != 0 and not index % 800:\n",
    "            yield sample_list\n",
    "            sample_list = []\n",
    "        domain_data = json_loads(domain_v.value)\n",
    "        try:\n",
    "            lines = {\n",
    "                \"feature\": domain_data[\"feature\"],\n",
    "                \"layer_n\": domain_data[\"layer_n\"],\n",
    "                \"total_n\": domain_data[\"total_n\"],\n",
    "                \"track_id\": domain_data[\"track_id\"],\n",
    "                \"url\": domain_data[\"url\"],\n",
    "                \"domain\": domain_data[\"domain\"],\n",
    "                \"raw_warc_path\": domain_data[\"raw_warc_path\"],\n",
    "                \"date\": domain_data[\"date\"]\n",
    "            }\n",
    "            sample_list.append(lines)\n",
    "        except:\n",
    "            pass\n",
    "    if sample_list:\n",
    "        yield sample_list\n",
    "\n",
    "def calculating_layout_cluster_little(current_host_name, sample_list):\n",
    "    cluster_datas, layout_list = cluster_html_struct(sample_list)\n",
    "    feature_dict = defaultdict(list)\n",
    "    max_layer_n = cluster_datas[0][\"max_layer_n\"]\n",
    "    # 每个layout类别抽取3个网页\n",
    "    for r in cluster_datas:\n",
    "        layout_id = r[\"layout_id\"]\n",
    "        if layout_id == -1:\n",
    "            continue\n",
    "        if len(feature_dict[layout_id]) < 3:\n",
    "            cr = copy(r)\n",
    "            feature_dict[layout_id].append(cr)\n",
    "    if layout_list:\n",
    "        layout_tmp_dict = crush_output_data_cluster_little({\"domain\": current_host_name, \"sub_path\": current_host_name, \"feature_dict\": dict(feature_dict), \"layout_list\": layout_list, \"max_layer_n\": max_layer_n})\n",
    "        yield layout_tmp_dict\n",
    "\n",
    "def parse_layout_cluster_little(domain_list):\n",
    "    s3_doc_writer = get_s3_doctor(\"parse_layout\")\n",
    "    error_info = None\n",
    "    current_host_name = domain_list[\"domain\"].unique()[0]\n",
    "    try:\n",
    "        for sample_list in parse_batch_data_cluster_little(domain_list):\n",
    "            try:\n",
    "                if len(sample_list) > 1:\n",
    "                    current_host_name = sample_list[0][\"domain\"]\n",
    "                    for line in func_timeout(TIMEOUT_SECONDS, calculating_layout_cluster_little, (current_host_name, sample_list,)):\n",
    "                        if isinstance(line, str):\n",
    "                            yield {\"value\": line, \"domain\": current_host_name}\n",
    "                        elif isinstance(line, Dict):\n",
    "                            error_info = {\n",
    "                                \"error_type\": \"MemoryError by row\",\n",
    "                                \"error_message\": \"MemoryError by row\",\n",
    "                                \"traceback\": traceback.format_exc(),\n",
    "                                \"input_data\": line,\n",
    "                                \"timestamp\": datetime.now().isoformat()\n",
    "                            }\n",
    "                            s3_doc_writer.write(error_info)\n",
    "                            continue\n",
    "            except FunctionTimedOut as e:\n",
    "                error_info = {\n",
    "                    \"error_type\": type(e).__name__,\n",
    "                    \"error_message\": str(e),\n",
    "                    \"traceback\": traceback.format_exc(),\n",
    "                    \"input_data\": str(sample_list),\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "                s3_doc_writer.write(error_info)\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                error_info = {\n",
    "                    \"error_type\": type(e).__name__,\n",
    "                    \"error_message\": str(e),\n",
    "                    \"traceback\": traceback.format_exc(),\n",
    "                    \"input_data\": str(sample_list),\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "                s3_doc_writer.write(error_info)\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        error_info = {\n",
    "            \"error_type\": type(e).__name__,\n",
    "            \"error_message\": str(e),\n",
    "            \"traceback\": traceback.format_exc(),\n",
    "            \"input_data\": current_host_name,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        s3_doc_writer.write(error_info)\n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n",
    "\n",
    "def layout_similarity_cluster_little(layout_d1, layout_d2):\n",
    "    max_layer_n = max(layout_d1[\"max_layer_n\"], layout_d2[\"max_layer_n\"])\n",
    "    layout_last = layout_d1\n",
    "    layout_last[\"max_layer_n\"] = max_layer_n\n",
    "    layout_list1 = layout_d1[\"layout_list\"]\n",
    "    layout_list2 = layout_d2[\"layout_list\"]\n",
    "    if len(layout_list1) > MAX_LAYOUTLIST_SIZE or len(layout_list2) > MAX_LAYOUTLIST_SIZE:\n",
    "        return layout_d1 if len(layout_list1) > len(layout_list2) else layout_d2\n",
    "    max_layout_id = max(layout_list1)\n",
    "    feature_dict1 = layout_d1[\"feature_dict\"]\n",
    "    feature_dict2 = layout_d2[\"feature_dict\"]\n",
    "    ls_v = []\n",
    "    [ls_v.extend(v) for k, v in feature_dict1.items()]\n",
    "    exist_layout_num = 0\n",
    "    for new_k, new_v in feature_dict2.items():\n",
    "        add_tmp_dict_v = True\n",
    "        for new_d in new_v:\n",
    "            if any(similarity(new_d[\"feature\"], h[\"feature\"], max_layer_n) >= SIMILARITY_THRESHOLD for h in ls_v):\n",
    "                add_tmp_dict_v = False\n",
    "                exist_layout_num += 1\n",
    "                break\n",
    "        if add_tmp_dict_v is True:\n",
    "            max_layout_id += 1\n",
    "            layout_last[\"feature_dict\"][str(max_layout_id)] = new_v\n",
    "            layout_last[\"layout_list\"].append(max_layout_id)\n",
    "    return layout_last\n",
    "                    \n",
    "def merge_layout_cluster_little(domain_list):\n",
    "    s3_doc_writer = get_s3_doctor(\"merge_layout\")\n",
    "    error_info = None\n",
    "    pre_domain = {}\n",
    "    domain_v = None\n",
    "    # 两两进行合并\n",
    "    for index, domain_v in domain_list.iterrows():\n",
    "        if index == 0:\n",
    "            pre_domain = json_loads(domain_v.value)\n",
    "        else:\n",
    "            try:\n",
    "                pre_domain = layout_similarity_cluster_little(pre_domain, json_loads(domain_v.value))\n",
    "            except Exception as e:\n",
    "                error_info = {\n",
    "                    \"error_type\": type(e).__name__,\n",
    "                    \"error_message\": str(e),\n",
    "                    \"traceback\": traceback.format_exc(),\n",
    "                    \"input_data\": domain_v.value,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "                s3_doc_writer.write(error_info)\n",
    "                \n",
    "    if pre_domain:\n",
    "        output_data = json_dumps(pre_domain)\n",
    "        if len(output_data) < MAX_OUTPUT_ROW_SIZE:\n",
    "            return pd.DataFrame({\"layout_dict\": output_data, \"domain\": pre_domain[\"domain\"]}, index=[0])\n",
    "        else:\n",
    "            error_info = {\n",
    "                \"error_type\": \"EOFError\",\n",
    "                \"error_message\": \"Memory more than required for vector is (2147483648)\",\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "                \"input_data\": domain_v.value,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            s3_doc_writer.write(error_info)\n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c144e52c-e5e8-4e77-80e0-8273686d4e41",
   "metadata": {},
   "source": [
    "## main func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21af9fd9-5e3b-43af-b8ac-e72cb8490fe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T07:59:58.282086Z",
     "iopub.status.busy": "2025-09-18T07:59:58.281819Z",
     "iopub.status.idle": "2025-09-18T07:59:58.299944Z",
     "shell.execute_reply": "2025-09-18T07:59:58.299582Z",
     "shell.execute_reply.started": "2025-09-18T07:59:58.282072Z"
    }
   },
   "outputs": [],
   "source": [
    "# get data by domain\n",
    "def get_input_df_cluster_little(batch: str):\n",
    "    batch = crush_read_path(batch)\n",
    "    input_df = spark.read.format(\"json\").load(batch)\n",
    "    domain_count = input_df.count()\n",
    "    input_valid_df = parse_valid_data_cluster_little(input_df, batch)\n",
    "    parse_explode_df_cluster_little(input_valid_df, domain_count)\n",
    "    \n",
    "def parse_valid_data_cluster_little(input_valid_df: DataFrame, batch: str):\n",
    "    data_range = batch.split(\"/\")[-2].split(\"_\")[0]\n",
    "    data_count_rate = RATE_MAP.get(data_range, 1)\n",
    "\n",
    "    df_with_valid_c = input_valid_df.withColumn(\"valid_count\", \n",
    "                                          when(col(\"count\") <= 800, col(\"count\"))\n",
    "                                          .otherwise(_round(col(\"count\") * data_count_rate).cast(\"integer\"))\n",
    "                                         )\n",
    "    \n",
    "    return df_with_valid_c\n",
    "\n",
    "def parse_explode_df_cluster_little(domain_df: DataFrame, domain_count):\n",
    "    explode_df = domain_df.withColumn(\"file\", explode(col(\"files\"))).drop(\"files\")\n",
    "    parse_get_feature_df_cluster_little(explode_df, domain_df, domain_count)\n",
    "\n",
    "def parse_get_feature_df_cluster_little(explode_df: DataFrame, domain_df: DataFrame, domain_count):\n",
    "    schema = StructType([\n",
    "        StructField('value', StringType(), True),\n",
    "        StructField('domain', StringType(), True),\n",
    "        StructField('valid_count', IntegerType(), True),\n",
    "    ])    \n",
    "    feature_df = explode_df.repartition(NUM_PARTITIONS).rdd.mapPartitions(get_all_domain_data_cluster_little).toDF(schema)\n",
    "    sample_by_valid_count_cluster_little(feature_df, domain_df, domain_count)\n",
    "\n",
    "def sample_by_valid_count_cluster_little(feature_df: DataFrame, domain_df: DataFrame, domain_count):\n",
    "    df_with_rand = feature_df.withColumn(\"rand\", expr(\"rand()\"))\n",
    "    row_num_window_spec = Window.partitionBy(\"domain\").orderBy(col(\"rand\"))\n",
    "    df_with_row_num = df_with_rand.withColumn(\"row_num\", row_number().over(row_num_window_spec))\n",
    "    domain_sample_df = df_with_row_num.filter(col(\"row_num\") <= col(\"valid_count\")).drop(\"rand\", \"row_num\")\n",
    "    calculating_layout_every_batch_cluster_little(domain_sample_df, domain_df, domain_count)\n",
    "\n",
    "def calculating_layout_every_batch_cluster_little(domain_sample_df: DataFrame, domain_df: DataFrame, domain_count):\n",
    "    output_schema = StructType([\n",
    "        StructField('value', StringType(), True),\n",
    "        StructField('domain', StringType(), True),\n",
    "    ])\n",
    "    \n",
    "    @pandas_udf(output_schema, PandasUDFType.GROUPED_MAP)\n",
    "    def pandas_udf_custom_process(data_collected_series):\n",
    "        result = parse_layout_cluster_little(data_collected_series)\n",
    "        if result:\n",
    "            return pd.DataFrame(result)\n",
    "    \n",
    "    rep_df = domain_sample_df.repartition(NUM_PARTITIONS, col(\"domain\"))\n",
    "    layout_df = rep_df.groupby('domain').apply(pandas_udf_custom_process)\n",
    "    merge_layout_by_layout_id_cluster_little(layout_df, domain_df, domain_count)\n",
    "\n",
    "\n",
    "def merge_layout_by_layout_id_cluster_little(layout_df: DataFrame, domain_df: DataFrame, domain_count):\n",
    "    output_schema = StructType([\n",
    "        StructField('layout_dict', StringType(), True),\n",
    "        StructField('domain', StringType(), True),\n",
    "    ])\n",
    "\n",
    "    merge_rep_df = layout_df.repartition(NUM_PARTITIONS, col(\"domain\"))\n",
    "    merge_layout_df = merge_rep_df.groupby('domain').applyInPandas(merge_layout_cluster_little, output_schema)\n",
    "    join_to_write_cluster_little(merge_layout_df, domain_df)\n",
    "\n",
    "def join_to_write_cluster_little(merge_layout_df: DataFrame, domain_df: DataFrame):\n",
    "    join_df = domain_df.join(merge_layout_df, on=\"domain\", how=\"left\")\n",
    "\n",
    "    struct_col = struct(join_df[\"domain\"], join_df[\"count\"], join_df[\"files\"], join_df[\"layout_dict\"])\n",
    "    output_df = join_df.withColumn(\"value\", to_json(struct_col)).select(\"value\")\n",
    "\n",
    "    config[\"skip_output_version\"] = True\n",
    "    config['skip_output_check'] = True\n",
    "    write_any_path(output_df, OUTPUT_PATH, config)\n",
    "\n",
    "def parse_path_cluster_little(batch):\n",
    "    path_list = batch.split('/')\n",
    "    global OUTPUT_PATH\n",
    "    OUTPUT_PATH = f\"{CLUSTER_LAYOUT_BASE_OUTPUT_PATH}{path_list[-2]}/{path_list[-1].replace('.jsonl', '')}/\"\n",
    "    return path_list\n",
    "\n",
    "def parse_input_path_cluster_little(input_path):\n",
    "    try:\n",
    "        with open(\"./is_little_layout_complated.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            already_exist = [i for i in content.split(\",\") if i] if content else []\n",
    "    except:\n",
    "        already_exist = []\n",
    "    input_path_lst = [i for i in [f.replace(\"s3\", \"s3a\") for f in list(list_s3_objects(input_path, recursive=True)) if f.endswith(\".jsonl\")] if i not in already_exist]\n",
    "    return input_path_lst\n",
    "\n",
    "def layout_little_main(big_batch):\n",
    "    input_path_lst = parse_input_path_cluster_little(big_batch)\n",
    "    for batch in input_path_lst:\n",
    "        path_list = parse_path_cluster_little(batch)\n",
    "        spark_name = '_'.join([path_list[-2], path_list[-1].replace('.jsonl', '')])\n",
    "        create_spark(spark_name)\n",
    "        get_input_df_cluster_little(batch)\n",
    "        close_spark()\n",
    "        with open(\"./is_little_layout_complated.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(batch + \",\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259adf4f-536e-446c-8506-71d374484e35",
   "metadata": {},
   "source": [
    "# layout_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fbcebc-535e-4ce0-8b8c-530e305e5cc4",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b9273-a0f6-4b6b-95f8-0c4f2e2ac1b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T07:59:58.300664Z",
     "iopub.status.busy": "2025-09-18T07:59:58.300397Z",
     "iopub.status.idle": "2025-09-18T07:59:58.317025Z",
     "shell.execute_reply": "2025-09-18T07:59:58.316662Z",
     "shell.execute_reply.started": "2025-09-18T07:59:58.300651Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_output_data(row_data):\n",
    "    row_data.update({\"layout_id\": '_'.join([row_data[\"url_host_name\"], str(row_data[\"layout_id\"])])})\n",
    "    new_row_data_json = json_dumps(row_data)\n",
    "    if len(new_row_data_json) < MAX_OUTPUT_ROW_SIZE:\n",
    "        return {\"value\": new_row_data_json, \"layout_id\": row_data[\"layout_id\"]}\n",
    "    return None\n",
    "        \n",
    "def calculating_similarity(feature_dict, feature, max_layer_n):\n",
    "    for k, v in feature_dict.items():\n",
    "        if any(similarity(feature, h[\"feature\"], max_layer_n) >= SIMILARITY_THRESHOLD for h in v):\n",
    "            return int(k)\n",
    "    return -2\n",
    "\n",
    "def parse_similarity(_iter):\n",
    "    s3_doc_writer = get_s3_doctor(\"parse_similarity\")\n",
    "    error_info = None\n",
    "    for row in _iter:\n",
    "        is_no_layout_id = False\n",
    "        layout_dict = json_loads(row.layout_dict) if row.layout_dict else {}\n",
    "        layout_list = layout_dict.get(\"layout_list\", [])\n",
    "        if not layout_list or (len(layout_list)==1 and layout_list[0]==-1):\n",
    "            is_no_layout_id = True\n",
    "        feature_dict = layout_dict.get(\"feature_dict\", {})\n",
    "        max_layer_n = layout_dict.get(\"max_layer_n\", 5)\n",
    "        domain = row.domain\n",
    "        file_d = row.file\n",
    "        offset = file_d.offset\n",
    "        record_count = file_d.record_count\n",
    "        try:\n",
    "            for detail_data in read_s3_by_offset_limit(file_d.filepath, offset, limit=record_count):\n",
    "                detail_data = json_loads(detail_data.value)\n",
    "                if is_no_layout_id is True:\n",
    "                    layout_id = -1\n",
    "                else:\n",
    "                    try:\n",
    "                        feature = get_feature(detail_data[\"html\"])\n",
    "                        if feature is None or not feature.get(\"tags\"):\n",
    "                            layout_id = -2\n",
    "                        else:\n",
    "                            layout_id = func_timeout(SIM_TIMEOUT_SECONDS, calculating_similarity, (feature_dict, feature, max_layer_n,))\n",
    "                    except FunctionTimedOut as e:\n",
    "                        error_info = {\n",
    "                            \"error_type\": type(e).__name__,\n",
    "                            \"error_message\": str(e),\n",
    "                            \"traceback\": traceback.format_exc(),\n",
    "                            \"input_data\": str(detail_data),\n",
    "                            \"timestamp\": datetime.now().isoformat()\n",
    "                        }\n",
    "                        s3_doc_writer.write(error_info)\n",
    "                        layout_id = -2\n",
    "                    except Exception as e:\n",
    "                        error_info = {\n",
    "                            \"error_type\": type(e).__name__,\n",
    "                            \"error_message\": str(e),\n",
    "                            \"traceback\": traceback.format_exc(),\n",
    "                            \"input_data\": str(detail_data),\n",
    "                            \"timestamp\": datetime.now().isoformat()\n",
    "                        }\n",
    "                        s3_doc_writer.write(error_info)\n",
    "                        layout_id = -2\n",
    "                line = {\n",
    "                    \"track_id\": detail_data[\"track_id\"],\n",
    "                    \"html\": detail_data[\"html\"],\n",
    "                    \"url\": detail_data[\"url\"],\n",
    "                    \"layout_id\": layout_id,\n",
    "                    \"max_layer_n\": max_layer_n,\n",
    "                    \"url_host_name\": domain,\n",
    "                    \"raw_warc_path\": detail_data[\"raw_warc_path\"]\n",
    "                }\n",
    "                json_line = parse_output_data(line)\n",
    "                if json_line is not None:\n",
    "                    yield json_line\n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"error_message\": str(e),\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "                \"input_data\": str(row),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            s3_doc_writer.write(error_info)\n",
    "            continue\n",
    "                    \n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n",
    "\n",
    "def save_s3_by_layout(outdata_list):\n",
    "    s3_doc_writer = get_s3_doctor(\"similarity_write\")\n",
    "    error_info = None\n",
    "    json_line = None\n",
    "    s3_writer = None\n",
    "    index = 0\n",
    "    offset = 0\n",
    "    offset_index = 0\n",
    "    for index, row in enumerate(outdata_list):\n",
    "        try:\n",
    "            if offset > MAX_OUTPUT_FILE_SIZE:\n",
    "                if json_line:\n",
    "                    s3_writer.flush()\n",
    "                    s3_writer = None\n",
    "                    offset = 0\n",
    "            json_line = json_loads(row.value)\n",
    "            json_line[\"offset\"] = offset\n",
    "            if s3_writer:\n",
    "                offset += s3_writer.write(json_line)\n",
    "            else:\n",
    "                partition_id = row.partition_id\n",
    "                offset_index = offset_index + 1\n",
    "                output_file = f\"{OUTPUT_PATH}{partition_id}_{offset_index}.jsonl.gz\"\n",
    "                s3_writer = S3DocWriter(output_file)\n",
    "                offset += s3_writer.write(json_line)\n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"error_message\": str(e),\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "                \"input_data\": row.value,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            s3_doc_writer.write(error_info)\n",
    "            continue\n",
    "\n",
    "    if json_line:\n",
    "        s3_writer.flush()\n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n",
    "    yield {\"write_size\": index}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a671eac7-50d8-470e-94ca-b6b1a0c0dbab",
   "metadata": {},
   "source": [
    "## main func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc5958-8f1e-42b4-888d-a33c75ba400a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T07:59:58.317612Z",
     "iopub.status.busy": "2025-09-18T07:59:58.317438Z",
     "iopub.status.idle": "2025-09-18T07:59:58.329846Z",
     "shell.execute_reply": "2025-09-18T07:59:58.329496Z",
     "shell.execute_reply.started": "2025-09-18T07:59:58.317598Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_input_path_sim(input_path):\n",
    "    try:\n",
    "        with open(\"./is_similarity_complated.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            already_exist = [i for i in content.split(\",\") if i] if content else []\n",
    "    except:\n",
    "        already_exist = []\n",
    "    input_path_lst = [i for i in [f.replace(\"s3\", \"s3a\") for f in list(list_s3_objects(input_path, recursive=False))] if i not in already_exist]\n",
    "    return input_path_lst\n",
    "\n",
    "def parse_path_sim(batch):\n",
    "    path_list = batch.split('/')\n",
    "    path1 = path_list[-3].split(\"_\")[0]\n",
    "    global OUTPUT_PATH\n",
    "    OUTPUT_PATH = f\"{LAYOUT_SIM_BASE_OUTPUT_PATH}{path1}/{path_list[-2]}/\"\n",
    "    return path_list\n",
    "\n",
    "def get_domain_df(batch):\n",
    "    batch = crush_read_path(batch)\n",
    "    input_f_df = spark.read.format(\"json\").load(batch)\n",
    "    input_df = input_f_df.withColumn(\"file\", explode(col(\"files\"))).drop(\"files\")\n",
    "    similarity_every_domain(input_df)\n",
    "\n",
    "def similarity_every_domain(input_df: DataFrame):\n",
    "    schema = StructType([\n",
    "        StructField('value', StringType(), True),\n",
    "        StructField('layout_id', StringType(), True),\n",
    "    ])\n",
    "    \n",
    "    all_domain_df = input_df.repartition(NUM_PARTITIONS).rdd.mapPartitions(parse_similarity).toDF(schema)\n",
    "    write_by_layoutid(all_domain_df)\n",
    "\n",
    "def write_by_layoutid(all_domain_df: DataFrame):\n",
    "    final_df = all_domain_df.repartition(WRITE_NUM_PARTITIONS, col(\"layout_id\")).withColumn(\"partition_id\", spark_partition_id()).sortWithinPartitions(col(\"layout_id\"))\n",
    "    out_df = final_df.rdd.mapPartitions(save_s3_by_layout)\n",
    "    out_df.count()\n",
    "\n",
    "def sim_main():\n",
    "    big_batch_lst = list(list_s3_objects(CLUSTER_LAYOUT_BASE_OUTPUT_PATH, recursive=False))\n",
    "    for big_batch in big_batch_lst:\n",
    "        input_path_lst = parse_input_path_sim(big_batch)\n",
    "        for batch in input_path_lst:\n",
    "            path_list = parse_path_sim(batch)\n",
    "            spark_name = 'sim.' + '_'.join([path_list[-3], path_list[-2]])\n",
    "            create_spark(spark_name)\n",
    "            get_domain_df(batch)\n",
    "            close_spark()   \n",
    "            with open(\"./is_similarity_complated.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(batch + \",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade51c83-a9e4-42dd-886b-e6e50be49812",
   "metadata": {},
   "source": [
    "# layout_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32afc659-8ba0-4b27-9483-021c3621ca7e",
   "metadata": {},
   "source": [
    "## main func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c65c62-10cc-440d-984a-e469ceb47565",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T07:59:58.330481Z",
     "iopub.status.busy": "2025-09-18T07:59:58.330284Z",
     "iopub.status.idle": "2025-09-18T07:59:58.350105Z",
     "shell.execute_reply": "2025-09-18T07:59:58.349761Z",
     "shell.execute_reply.started": "2025-09-18T07:59:58.330467Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_input_path_index(input_path: str):\n",
    "    try:\n",
    "        with open(\"./is_index_complated.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            already_exist = [i for i in content.split(\",\") if i] if content else []\n",
    "    except:\n",
    "        already_exist = []\n",
    "    input_path_lst = [i for i in list(list_s3_objects(input_path, recursive=False)) if i not in already_exist]\n",
    "    return input_path_lst\n",
    "\n",
    "def create_index_df(batch: str, path_list: List):\n",
    "    input_path_lst = [i for i in list(list_s3_objects(batch, recursive=True)) if i.endswith(\".jsonl.gz\")]\n",
    "\n",
    "    with_length_v_df = read_any_path(spark, ','.join(input_path_lst), config)\n",
    "    schema = StructType([\n",
    "        StructField(\"url_host_name\", StringType(), True),\n",
    "        StructField(\"layout_id\", StringType(), True),\n",
    "        StructField(\"offset\", LongType(), True),\n",
    "    ])\n",
    "    \n",
    "    df_with_struct = with_length_v_df.withColumn(\"json_struct\", from_json(with_length_v_df.value, schema))\n",
    "    with_length_df = df_with_struct.select(\"json_struct.*\", col(\"filename\").alias(\"filepath\"))\n",
    "    data_to_index(with_length_df, path_list)\n",
    "\n",
    "def data_to_index(with_length_df: DataFrame, path_list: List):\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    group_df = with_length_df.groupBy([\"url_host_name\", \"layout_id\", \"filepath\"]).agg(\n",
    "        (_max(col(\"offset\")) - _min(col(\"offset\"))).alias(\"length\"),\n",
    "        _min(col(\"offset\")).alias(\"offset\"),\n",
    "        _count(\"*\").alias(\"record_count\")\n",
    "    ).sort(\"url_host_name\", \"layout_id\", \"filepath\").withColumn(\"timestamp\", lit(current_time))\n",
    "    \n",
    "\n",
    "    file_df = group_df.withColumn(\"file\", \n",
    "          struct(\n",
    "            col(\"filepath\").alias(\"filepath\"),\n",
    "            col(\"length\").cast(\"long\").alias(\"length\"),\n",
    "            col(\"offset\").cast(\"long\").alias(\"offset\"),\n",
    "            col(\"record_count\").cast(\"long\").alias(\"record_count\"),\n",
    "            col(\"timestamp\").alias(\"timestamp\")\n",
    "          )\n",
    "        ).select([\"url_host_name\", \"layout_id\", \"file\", \"record_count\"])\n",
    "\n",
    "\n",
    "    result_df = file_df.groupBy(\"layout_id\") \\\n",
    "        .agg(\n",
    "            _sum(\"record_count\").alias(\"count\"),\n",
    "            collect_list(\"file\").alias(\"files\"),\n",
    "            first(\"url_host_name\").alias(\"url_host_name\")\n",
    "        ).orderBy(\"count\", ascending=False)\n",
    "    write_by_two(result_df, path_list)\n",
    "\n",
    "def write_by_two(result_df: DataFrame, path_list: List):\n",
    "    struct_col = struct(result_df[\"layout_id\"], result_df[\"count\"], result_df[\"files\"], result_df[\"url_host_name\"])\n",
    "    output_df = result_df.withColumn(\"value\", to_json(struct_col)).select(\"value\")\n",
    "\n",
    "    output_acc = S3UploadAcc(spark.sparkContext)\n",
    "    output_df.repartition(1).foreachPartition(upload_to_s3(OUTPUT_PATH, \"jsonl\", output_acc, prefix = path_list[-2]))\n",
    "\n",
    "def parse_path_index(batch):\n",
    "    path_list = batch.split('/')\n",
    "    global OUTPUT_PATH\n",
    "    OUTPUT_PATH = f\"{LAYOUT_INDEX_BASE_OUTPUT_PATH}{path_list[-3]}/\"\n",
    "    return path_list\n",
    "    \n",
    "def index_main():\n",
    "    big_batch_lst = list(list_s3_objects(LAYOUT_SIM_BASE_OUTPUT_PATH, recursive=False))\n",
    "    for big_batch in big_batch_lst:\n",
    "        batch_lst = parse_input_path_index(big_batch)\n",
    "        for batch in batch_lst:\n",
    "            path_list = parse_path_index(batch)\n",
    "            spark_name = \"index.\" + \".\".join([path_list[-3], path_list[-2]])\n",
    "            create_spark(spark_name)\n",
    "            create_index_df(batch, path_list)\n",
    "            close_spark()\n",
    "            with open(\"./is_index_complated.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(batch + \",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b5390-1639-4ed7-862f-00774c53c3ff",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1317ee-93f6-4148-b836-4eb630d44f12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T07:59:58.350784Z",
     "iopub.status.busy": "2025-09-18T07:59:58.350505Z",
     "iopub.status.idle": "2025-09-19T01:34:02.031014Z",
     "shell.execute_reply": "2025-09-19T01:34:02.030302Z",
     "shell.execute_reply.started": "2025-09-18T07:59:58.350770Z"
    }
   },
   "outputs": [],
   "source": [
    "choose_main()\n",
    "\n",
    "# 条件判断\n",
    "big_batch_lst = list(list_s3_objects(CHOOSE_DOMAIN_OUTPUT_PATH, recursive=False))\n",
    "for big_batch in big_batch_lst:\n",
    "    end_path = big_batch.split(\"/\")[-2]\n",
    "    if end_path.startswith(\"1-1600\"):\n",
    "        layout_little_main(big_batch)\n",
    "    else:\n",
    "        layout_main(big_batch)\n",
    "\n",
    "sim_main()\n",
    "\n",
    "index_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460c9dd-b6a8-451c-be1e-1c03cd6705a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python webkit_venv (ipykernel)",
   "language": "python",
   "name": "webkit_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
