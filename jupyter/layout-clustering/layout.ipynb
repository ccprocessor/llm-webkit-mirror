{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a86ea9b-beca-4324-b101-5684b3ed0955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T03:25:30.484686Z",
     "iopub.status.busy": "2025-05-29T03:25:30.484374Z",
     "iopub.status.idle": "2025-05-29T03:25:34.376706Z",
     "shell.execute_reply": "2025-05-29T03:25:34.376125Z",
     "shell.execute_reply.started": "2025-05-29T03:25:30.484667Z"
    }
   },
   "outputs": [],
   "source": [
    "from xinghe.spark import *\n",
    "from app.common.json_util import *\n",
    "from xinghe.s3 import *\n",
    "from xinghe.s3.read import *\n",
    "from xinghe.ops.spark import spark_resize_file\n",
    "\n",
    "config = {\n",
    "    \"spark_conf_name\": \"spark_4\",\n",
    "    \"skip_success_check\": True,\n",
    "    \"spark.executorEnv.LLM_WEB_KIT_CFG_PATH\": \"/share/xxx.jsonc\",\n",
    "    \"spark.yarn.queue\": \"pipeline.clean\",\n",
    "    \"spark.executor.memory\": \"40g\",\n",
    "}\n",
    "from llm_web_kit.libs.standard_utils import compress_and_decompress_str\n",
    "from llm_web_kit.html_layout.html_layout_cosin import cluster_html_struct, get_feature, similarity, sum_tags\n",
    "\n",
    "import base64\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "import zlib\n",
    "import uuid\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Union\n",
    "from copy import deepcopy, copy\n",
    "from urllib.parse import quote, unquote, urlparse, parse_qs\n",
    "from datetime import datetime\n",
    "from lxml import html\n",
    "from collections import defaultdict\n",
    "from func_timeout import FunctionTimedOut, func_timeout\n",
    "\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.functions import row_number, col, collect_list, struct, expr, rand, count, pandas_udf, PandasUDFType, \\\n",
    "    round as _round, lit, to_json, explode\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, BinaryType, StringType\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"LLM_WEB_KIT_CFG_PATH\"] = \"/share/xxx.jsonc\"\n",
    "\n",
    "TIMEOUT_SECONDS = 3600 * 5\n",
    "MAX_OUTPUT_ROW_SIZE = 1024 * 1024 * 1024 * 1.7\n",
    "SIMILARITY_THRESHOLD = 0.95\n",
    "RATE_MAP = {\"10w\": 0.1, \"1.5w-10w\": 0.2, \"1600-1.5w\": 0.5, \"1-1600\": 1, \"500-1600\": 1}\n",
    "NUM_PARTITIONS = 100000\n",
    "ERROR_PATH = \"s3://xxx/\"\n",
    "INPUT_PATH = \"s3://xxx/\"\n",
    "BASE_OUTPUT_PATH = \"s3://xxx/\"\n",
    "BASE_DOMAIN_PATH = \"s3://xxx/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8471645e-61ff-4d8f-9ba9-eb837826892e",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf4c96-b0eb-4a0b-84f1-b6318332b5c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T03:25:34.394234Z",
     "iopub.status.busy": "2025-05-29T03:25:34.394091Z",
     "iopub.status.idle": "2025-05-29T03:25:34.396978Z",
     "shell.execute_reply": "2025-05-29T03:25:34.396624Z",
     "shell.execute_reply.started": "2025-05-29T03:25:34.394221Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_s3_doctor(target_theme):\n",
    "    partition_id = str(uuid.uuid4())\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d\")\n",
    "    error_log_path = f\"{ERROR_PATH}{target_theme}/{current_time}/{partition_id}.jsonl\"\n",
    "    s3_doc_writer = S3DocWriter(path=error_log_path)\n",
    "    return s3_doc_writer\n",
    "\n",
    "\n",
    "def get_all_domain_data(_iter):\n",
    "    s3_doc_writer = get_s3_doctor(\"get_feature\")\n",
    "    error_info = None\n",
    "    for row in _iter:\n",
    "        valid_count = row.valid_count\n",
    "        file_d = row.file\n",
    "        offset = file_d.offset\n",
    "        length = file_d.length\n",
    "        record_count = file_d.record_count\n",
    "        idx = 0\n",
    "        try:\n",
    "            for detail_data in read_s3_lines_with_range(file_d.filepath, use_stream=True,\n",
    "                                                        bytes_range=(offset, offset + length)):\n",
    "                idx += 1\n",
    "                if idx > record_count:\n",
    "                    break\n",
    "                try:\n",
    "                    detail_data = json_loads(detail_data)\n",
    "                    feature = get_feature(detail_data[\"html\"])\n",
    "                    if feature is None or not feature.get(\"tags\"):\n",
    "                        continue\n",
    "                    layer_n, total_n = sum_tags(feature[\"tags\"])\n",
    "                    line = {\n",
    "                        \"date\": detail_data[\"date\"],\n",
    "                        \"track_id\": detail_data[\"track_id\"],\n",
    "                        \"url\": detail_data[\"url\"],\n",
    "                        \"raw_warc_path\": detail_data[\"raw_warc_path\"],\n",
    "                        \"domain\": row.domain,\n",
    "                        \"sub_path\": row.domain,\n",
    "                        \"valid_count\": valid_count,\n",
    "                        \"feature\": feature,\n",
    "                        \"layer_n\": layer_n,\n",
    "                        \"total_n\": total_n\n",
    "                    }\n",
    "                    line = json_dumps(line)\n",
    "                    if len(line) < MAX_OUTPUT_ROW_SIZE:\n",
    "                        # compress_line = compress_and_decompress_str(line)\n",
    "                        yield Row(**{\"value\": line, \"domain\": row.domain, \"valid_count\": valid_count})\n",
    "                    else:\n",
    "                        error_info = {\n",
    "                            \"error_type\": \"EOFError\",\n",
    "                            \"error_message\": \"Memory more than required for vector is (2147483648)\",\n",
    "                            \"traceback\": traceback.format_exc(),\n",
    "                            \"input_data\": detail_data,\n",
    "                            \"timestamp\": datetime.now().isoformat()\n",
    "                        }\n",
    "                        s3_doc_writer.write(error_info)\n",
    "                        continue\n",
    "                except Exception as e:\n",
    "                    error_info = {\n",
    "                        \"error_type\": type(e).__name__,\n",
    "                        \"error_message\": str(e),\n",
    "                        \"traceback\": traceback.format_exc(),\n",
    "                        \"input_data\": detail_data,\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    s3_doc_writer.write(error_info)\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"error_message\": str(e),\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "                \"input_data\": str(row),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            s3_doc_writer.write(error_info)\n",
    "            continue\n",
    "\n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n",
    "\n",
    "\n",
    "def crush_output_data(output_data):\n",
    "    output_data_json = json_dumps(output_data)\n",
    "    if len(output_data_json) < MAX_OUTPUT_ROW_SIZE:\n",
    "        return {\"value\": output_data_json, \"domain\": output_data[\"domain\"]}\n",
    "\n",
    "\n",
    "def parse_batch_data(fpath):\n",
    "    sample_list = []\n",
    "    index = 0\n",
    "    for domain_v in read_s3_rows(fpath, use_stream=True):\n",
    "        index += 1\n",
    "        if index != 0 and not index % 800:\n",
    "            yield sample_list\n",
    "            sample_list = []\n",
    "        domain_data = json_loads(domain_v.value)\n",
    "        try:\n",
    "            lines = {\n",
    "                \"feature\": domain_data[\"feature\"],\n",
    "                \"layer_n\": domain_data[\"layer_n\"],\n",
    "                \"total_n\": domain_data[\"total_n\"],\n",
    "                \"track_id\": domain_data[\"track_id\"],\n",
    "                \"url\": domain_data[\"url\"],\n",
    "                \"domain\": domain_data[\"domain\"],\n",
    "                \"raw_warc_path\": domain_data[\"raw_warc_path\"],\n",
    "                \"date\": domain_data[\"date\"]\n",
    "            }\n",
    "            sample_list.append(lines)\n",
    "        except:\n",
    "            pass\n",
    "    if sample_list:\n",
    "        yield sample_list\n",
    "\n",
    "\n",
    "def calculating_layout(current_host_name, sample_list):\n",
    "    cluster_datas, layout_list = cluster_html_struct(sample_list)\n",
    "    feature_dict = defaultdict(list)\n",
    "    max_layer_n = cluster_datas[0][\"max_layer_n\"]\n",
    "    # 每个layout类别抽取3个网页\n",
    "    for r in cluster_datas:\n",
    "        layout_id = r[\"layout_id\"]\n",
    "        if layout_id == -1:\n",
    "            continue\n",
    "        if len(feature_dict[layout_id]) < 3:\n",
    "            cr = copy(r)\n",
    "            feature_dict[layout_id].append(cr)\n",
    "    if layout_list:\n",
    "        layout_tmp_dict = crush_output_data(\n",
    "            {\"domain\": current_host_name, \"feature_dict\": dict(feature_dict), \"layout_list\": layout_list,\n",
    "             \"max_layer_n\": max_layer_n})\n",
    "        if layout_tmp_dict:\n",
    "            yield layout_tmp_dict\n",
    "\n",
    "\n",
    "def parse_layout(domain_list):\n",
    "    s3_doc_writer = get_s3_doctor(\"parse_layout\")\n",
    "    error_info = None\n",
    "    for domain_f in domain_list:\n",
    "        domain_paths = [f for f in list(list_s3_objects(domain_f, recursive=True)) if f.endswith(\".jsonl\")]\n",
    "        for fpath in domain_paths:\n",
    "            try:\n",
    "                for sample_list in parse_batch_data(fpath):\n",
    "                    try:\n",
    "                        if len(sample_list) > 1:\n",
    "                            current_host_name = sample_list[0][\"domain\"]\n",
    "                            for line in func_timeout(TIMEOUT_SECONDS, calculating_layout,\n",
    "                                                     (current_host_name, sample_list,)):\n",
    "                                yield line\n",
    "                    except FunctionTimedOut as e:\n",
    "                        error_info = {\n",
    "                            \"error_type\": type(e).__name__,\n",
    "                            \"error_message\": str(e),\n",
    "                            \"traceback\": traceback.format_exc(),\n",
    "                            \"input_data\": str(sample_list),\n",
    "                            \"timestamp\": datetime.now().isoformat()\n",
    "                        }\n",
    "                        s3_doc_writer.write(error_info)\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        error_info = {\n",
    "                            \"error_type\": type(e).__name__,\n",
    "                            \"error_message\": str(e),\n",
    "                            \"traceback\": traceback.format_exc(),\n",
    "                            \"input_data\": str(sample_list),\n",
    "                            \"timestamp\": datetime.now().isoformat()\n",
    "                        }\n",
    "                        s3_doc_writer.write(error_info)\n",
    "                        continue\n",
    "            except Exception as e:\n",
    "                error_info = {\n",
    "                    \"error_type\": type(e).__name__,\n",
    "                    \"error_message\": str(e),\n",
    "                    \"traceback\": traceback.format_exc(),\n",
    "                    \"input_data\": fpath,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "                s3_doc_writer.write(error_info)\n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n",
    "\n",
    "\n",
    "def layout_similarity(layout_d1, layout_d2):\n",
    "    max_layer_n = max(layout_d1[\"max_layer_n\"], layout_d2[\"max_layer_n\"])\n",
    "    layout_last = layout_d1\n",
    "    layout_last[\"max_layer_n\"] = max_layer_n\n",
    "    layout_list1 = layout_d1[\"layout_list\"]\n",
    "    max_layout_id = max(layout_list1)\n",
    "    feature_dict1 = layout_d1[\"feature_dict\"]\n",
    "    feature_dict2 = layout_d2[\"feature_dict\"]\n",
    "    ls_v = []\n",
    "    [ls_v.extend(v) for k, v in feature_dict1.items()]\n",
    "    exist_layout_num = 0\n",
    "    for new_k, new_v in feature_dict2.items():\n",
    "        add_tmp_dict_v = True\n",
    "        for new_d in new_v:\n",
    "            if any(similarity(new_d[\"feature\"], h[\"feature\"], max_layer_n) >= SIMILARITY_THRESHOLD for h in ls_v):\n",
    "                add_tmp_dict_v = False\n",
    "                exist_layout_num += 1\n",
    "                break\n",
    "        if add_tmp_dict_v is True:\n",
    "            max_layout_id += 1\n",
    "            layout_last[\"feature_dict\"][str(max_layout_id)] = new_v\n",
    "            layout_last[\"layout_list\"].append(max_layout_id)\n",
    "    return layout_last\n",
    "\n",
    "\n",
    "def merge_layout(domain_list):\n",
    "    s3_doc_writer = get_s3_doctor(\"merge_layout\")\n",
    "    error_info = None\n",
    "    pre_domain = {}\n",
    "    domain_v = None\n",
    "    # 两两进行合并\n",
    "    for index, domain_v in domain_list.iterrows():\n",
    "        if index == 0:\n",
    "            pre_domain = json_loads(domain_v.value)\n",
    "        else:\n",
    "            try:\n",
    "                pre_domain = layout_similarity(pre_domain, json_loads(domain_v.value))\n",
    "            except Exception as e:\n",
    "                error_info = {\n",
    "                    \"error_type\": type(e).__name__,\n",
    "                    \"error_message\": str(e),\n",
    "                    \"traceback\": traceback.format_exc(),\n",
    "                    \"input_data\": domain_v.value,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "                s3_doc_writer.write(error_info)\n",
    "\n",
    "    output_data = json_dumps(pre_domain)\n",
    "    if len(output_data) < MAX_OUTPUT_ROW_SIZE:\n",
    "        return pd.DataFrame({\"layout_dict\": output_data, \"domain\": pre_domain[\"domain\"]}, index=[0])\n",
    "    else:\n",
    "        error_info = {\n",
    "            \"error_type\": \"EOFError\",\n",
    "            \"error_message\": \"Memory more than required for vector is (2147483648)\",\n",
    "            \"traceback\": traceback.format_exc(),\n",
    "            \"input_data\": domain_v.value,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        s3_doc_writer.write(error_info)\n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f44a2f-8c65-4146-8a7b-a415f0a2ab21",
   "metadata": {},
   "source": [
    "# main func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8315c329-619c-483d-806c-9be915e23ea4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T03:25:34.437045Z",
     "iopub.status.busy": "2025-05-29T03:25:34.436911Z",
     "iopub.status.idle": "2025-05-29T03:25:34.456433Z",
     "shell.execute_reply": "2025-05-29T03:25:34.456083Z",
     "shell.execute_reply.started": "2025-05-29T03:25:34.437032Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_spark(spark_name: str):\n",
    "    global spark\n",
    "    spark = new_spark_session(f\"layout.write.{spark_name}\", config)\n",
    "    global sc\n",
    "    sc = spark.sparkContext\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "def get_input_df(batch: str):\n",
    "    input_df = spark.read.format(\"json\").load(batch)\n",
    "    domain_df = parse_valid_data(input_df, batch)\n",
    "    domain_count = domain_df.count()\n",
    "    parse_explode_df(domain_df, domain_count)\n",
    "\n",
    "\n",
    "def parse_valid_data(input_df: DataFrame, batch: str):\n",
    "    data_range = batch.split(\"/\")[-2].split(\"_\")[0]\n",
    "    data_count_rate = RATE_MAP.get(data_range, 1)\n",
    "    domain_df = input_df.withColumn(\"valid_count\", _round(col(\"count\") * data_count_rate).cast(\"integer\"))\n",
    "    return domain_df\n",
    "\n",
    "\n",
    "def parse_explode_df(domain_df: DataFrame, domain_count):\n",
    "    explode_df = domain_df.withColumn(\"file\", explode(col(\"files\"))).drop(\"files\")\n",
    "    parse_get_feature_df(explode_df, domain_df, domain_count)\n",
    "\n",
    "\n",
    "def parse_get_feature_df(explode_df: DataFrame, domain_df: DataFrame, domain_count):\n",
    "    schema = StructType([\n",
    "        StructField('value', StringType(), True),\n",
    "        StructField('domain', StringType(), True),\n",
    "        StructField('valid_count', IntegerType(), True),\n",
    "    ])\n",
    "    feature_df = explode_df.repartition(NUM_PARTITIONS).rdd.mapPartitions(get_all_domain_data).toDF(schema)\n",
    "    sample_by_valid_count(feature_df, domain_df, domain_count)\n",
    "\n",
    "\n",
    "def sample_by_valid_count(feature_df: DataFrame, domain_df: DataFrame, domain_count):\n",
    "    df_with_rand = feature_df.withColumn(\"rand\", expr(\"rand()\"))\n",
    "    row_num_window_spec = Window.partitionBy(\"domain\").orderBy(col(\"rand\"))\n",
    "    df_with_row_num = df_with_rand.withColumn(\"row_num\", row_number().over(row_num_window_spec))\n",
    "    domain_sample_df = df_with_row_num.filter(col(\"row_num\") <= col(\"valid_count\")).drop(\"rand\", \"row_num\",\n",
    "                                                                                         \"valid_count\", \"domain\")\n",
    "    write_domain_data(domain_sample_df)\n",
    "    calculating_layout_every_batch(domain_df, domain_count)\n",
    "\n",
    "\n",
    "def write_domain_data(domain_sample_df: DataFrame):\n",
    "    config[\"skip_output_version\"] = True\n",
    "    config['skip_output_check'] = True\n",
    "    write_any_path(domain_sample_df, DOMAIN_PATH, config)\n",
    "\n",
    "\n",
    "def calculating_layout_every_batch(domain_df: DataFrame, domain_count):\n",
    "    output_schema = StructType([\n",
    "        StructField('value', StringType(), True),\n",
    "        StructField('domain', StringType(), True),\n",
    "    ])\n",
    "    domain_lst = list(list_s3_objects(DOMAIN_PATH, recursive=False))\n",
    "    page_content = sc.parallelize(domain_lst, len(domain_lst))\n",
    "    layout_df = page_content.mapPartitions(parse_layout).toDF(output_schema)\n",
    "    merge_layout_by_layout_id(layout_df, domain_df, domain_count)\n",
    "\n",
    "\n",
    "def merge_layout_by_layout_id(layout_df: DataFrame, domain_df: DataFrame, domain_count):\n",
    "    output_schema = StructType([\n",
    "        StructField('layout_dict', StringType(), True),\n",
    "        StructField('domain', StringType(), True),\n",
    "    ])\n",
    "\n",
    "    if domain_count < NUM_PARTITIONS:\n",
    "        merge_rep_df = layout_df.repartition(domain_count, col(\"domain\"))\n",
    "    else:\n",
    "        merge_rep_df = layout_df.repartition(NUM_PARTITIONS, col(\"domain\"))\n",
    "    merge_layout_df = merge_rep_df.groupby('domain').applyInPandas(merge_layout, output_schema)\n",
    "    join_to_write(merge_layout_df, domain_df)\n",
    "\n",
    "\n",
    "def join_to_write(merge_layout_df: DataFrame, domain_df: DataFrame):\n",
    "    join_df = domain_df.join(merge_layout_df, on=\"domain\", how=\"left\")\n",
    "\n",
    "    struct_col = struct(join_df[\"domain\"], join_df[\"count\"], join_df[\"files\"], join_df[\"layout_dict\"])\n",
    "    output_df = join_df.withColumn(\"value\", to_json(struct_col)).select(\"value\")\n",
    "\n",
    "    config[\"skip_output_version\"] = True\n",
    "    config['skip_output_check'] = True\n",
    "    write_any_path(output_df, OUTPUT_PATH, config)\n",
    "\n",
    "\n",
    "def close_spark():\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "def parse_path(batch):\n",
    "    path_list = batch.split('/')\n",
    "    global OUTPUT_PATH\n",
    "    OUTPUT_PATH = f\"{BASE_OUTPUT_PATH}{path_list[-2]}/{path_list[-1].replace('.jsonl', '')}/\"\n",
    "    global DOMAIN_PATH\n",
    "    DOMAIN_PATH = f\"{BASE_DOMAIN_PATH}{path_list[-2]}/{path_list[-1].replace('.jsonl', '')}/\"\n",
    "    return path_list\n",
    "\n",
    "\n",
    "def parse_input_path(input_path):\n",
    "    try:\n",
    "        with open(\"./is_layout_complated.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            already_exist = [i for i in content.split(\",\") if i] if content else []\n",
    "    except:\n",
    "        already_exist = []\n",
    "    input_path_lst = [i for i in [f.replace(\"s3\", \"s3a\") for f in list(list_s3_objects(input_path, recursive=True)) if\n",
    "                                  f.endswith(\".jsonl\")] if i not in already_exist]\n",
    "    return input_path_lst\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_path_lst = parse_input_path(INPUT_PATH)\n",
    "    for batch in input_path_lst:\n",
    "        path_list = parse_path(batch)\n",
    "        spark_name = '_'.join([path_list[-2], path_list[-1].replace('.jsonl', '')])\n",
    "        create_spark(spark_name)\n",
    "        get_input_df(batch)\n",
    "        close_spark()\n",
    "        with open(\"./is_layout_complated.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(batch + \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d732478-d17f-4eb0-b380-e0631cc879dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T03:25:34.456981Z",
     "iopub.status.busy": "2025-05-29T03:25:34.456843Z"
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python webkit_venv (ipykernel)",
   "language": "python",
   "name": "webkit_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
