{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xinghe.spark import *\n",
    "from app.common.json_util import *\n",
    "from xinghe.s3 import *\n",
    "from xinghe.s3.read import *\n",
    "from xinghe.ops.spark import spark_resize_file\n",
    "\n",
    "import os\n",
    "os.environ[\"LLM_WEB_KIT_CFG_PATH\"] = \"/xxx.jsonc\"\n",
    "\n",
    "from llm_web_kit.libs.standard_utils import compress_and_decompress_str\n",
    "from llm_web_kit.html_layout.html_layout_cosin import cluster_html_struct, get_feature, similarity, sum_tags\n",
    "\n",
    "config = {\n",
    "    \"spark_conf_name\": \"spark_4\", # another value is \"spark_2\"\n",
    "    \"skip_success_check\": True,\n",
    "    \"spark.executorEnv.LLM_WEB_KIT_CFG_PATH\": \"/xxx.jsonc\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "spark = new_spark_session(\"cc_dumps.layoutID.index\", config)\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import uuid\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import struct, to_json, sum as _sum, collect_list, first\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType\n",
    "\n",
    "\n",
    "TIMEOUT_SECONDS = 3600 * 5  # 超时时间5min\n",
    "MAX_OUTPUT_ROW_SIZE = 1024 * 1024 * 1024 * 1.5\n",
    "SIMILARITY_THRESHOLD = 0.95\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# get layout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"xxx\"\n",
    "input_path_lst = [f for f in list(list_s3_objects(input_path, recursive=True)) if f.endswith(\".jsonl\")]\n",
    "len(input_path_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# 边读数据边生成index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_to_index(_iter):\n",
    "    # TODO 错误日志存放地址\n",
    "    error_log_path = f\"s3://xxx.jsonl\"\n",
    "    print(f\"error_log_path: {error_log_path}\")\n",
    "    s3_doc_writer = S3DocWriter(path=error_log_path)\n",
    "    error_info = None\n",
    "    for fpath in _iter:\n",
    "        current_layout_id = None\n",
    "        start_offset = None\n",
    "        layout_length = 0\n",
    "        idx = 0\n",
    "        print(f\"fpath:{fpath}\")\n",
    "        for row in read_s3_rows(fpath):\n",
    "            idx += 1\n",
    "            try:\n",
    "                detail_data = json_loads(row.value)\n",
    "                layout_id = detail_data[\"layout_id\"]\n",
    "                offset, length = map(int, row.loc.split(\"bytes=\")[-1].split(\",\"))\n",
    "                if layout_id == current_layout_id:\n",
    "                    layout_length += length\n",
    "                    continue\n",
    "                else:\n",
    "                    if current_layout_id is not None:\n",
    "                        print(f\"{current_layout_id} 该批数据批次结束, 总数据量为： {idx-1}\")\n",
    "                        line = {\n",
    "                            \"layout_id\": current_layout_id,\n",
    "                            \"url_host_name\": detail_data[\"url_host_name\"],\n",
    "                            \"count\": idx-1,\n",
    "                            \"file\": {\n",
    "                                \"filepath\": fpath,\n",
    "                                \"offset\": start_offset,\n",
    "                                \"length\": layout_length,\n",
    "                                \"record_count\": idx-1,\n",
    "                                \"timestamp\": int(time.time())\n",
    "                            }\n",
    "                        }\n",
    "                        yield line\n",
    "                        idx = 1\n",
    "                    current_layout_id = layout_id\n",
    "                    start_offset = offset\n",
    "                    layout_length = 0\n",
    "                    layout_length += length\n",
    "                    print(f\"新批次数据： {current_layout_id}, start_offset: {start_offset}, layout_length: {layout_length}\")\n",
    "            except Exception as e:\n",
    "                error_info = {\n",
    "                    \"error_type\": type(e).__name__,\n",
    "                    \"error_message\": str(e),\n",
    "                    \"traceback\": traceback.format_exc(),\n",
    "                    \"input_data\": row.value if hasattr(row, 'value') else str(row),\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "                s3_doc_writer.write(error_info)\n",
    "                continue\n",
    "    if current_layout_id is not None:\n",
    "        print(f\"last: {current_layout_id} 该批数据批次结束, 总数据量为： {idx}\")\n",
    "        line = {\n",
    "            \"layout_id\": current_layout_id,\n",
    "            \"url_host_name\": detail_data[\"url_host_name\"],\n",
    "            \"count\": idx,\n",
    "            \"file\": {\n",
    "                \"filepath\": fpath,\n",
    "                \"offset\": start_offset,\n",
    "                \"length\": layout_length,\n",
    "                \"record_count\": idx,\n",
    "                \"timestamp\": int(time.time())\n",
    "            }\n",
    "        }\n",
    "        yield line\n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"layout_id\", StringType(), True),\n",
    "    StructField(\"url_host_name\", StringType(), True),\n",
    "    StructField(\"count\", LongType(), True),\n",
    "    StructField(\"file\", StructType([\n",
    "        StructField(\"filepath\", StringType(), True),\n",
    "        StructField(\"offset\", LongType(), True),\n",
    "        StructField(\"length\", LongType(), True),\n",
    "        StructField(\"record_count\", LongType(), True),\n",
    "        StructField(\"timestamp\", IntegerType(), True),\n",
    "    ]), True),\n",
    "])\n",
    "\n",
    "page_content = sc.parallelize(input_path_lst, len(input_path_lst))\n",
    "dump_html_df = page_content.mapPartitions(read_to_index).toDF(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# 基于layout_id合并index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = dump_html_df.groupBy(\"layout_id\") \\\n",
    "    .agg(\n",
    "        _sum(\"count\").alias(\"count\"),\n",
    "        collect_list(\"file\").alias(\"files\"),\n",
    "        first(\"url_host_name\").alias(\"url_host_name\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_col = struct(result_df[\"layout_id\"], result_df[\"count\"], result_df[\"files\"], result_df[\"url_host_name\"])\n",
    "output_df = result_df.withColumn(\"value\", to_json(struct_col)).select(\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_size_gb = 2\n",
    "resize_func = spark_resize_file(output_file_size_gb)\n",
    "new_output_df = resize_func(output_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"s3://xxx/\"\n",
    "config[\"skip_output_version\"] = True\n",
    "config['skip_output_check'] = True\n",
    "\n",
    "write_any_path(new_output_df, output_path, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python webkit_venv (ipykernel)",
   "language": "python",
   "name": "webkit_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
