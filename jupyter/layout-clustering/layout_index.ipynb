{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xinghe.spark import *\n",
    "from app.common.json_util import *\n",
    "from xinghe.s3 import *\n",
    "from xinghe.s3.read import *\n",
    "from xinghe.ops.spark import spark_resize_file\n",
    "\n",
    "config = {\n",
    "    \"spark_conf_name\": \"spark_4\",\n",
    "    \"skip_success_check\": True,\n",
    "}\n",
    "\n",
    "import base64\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "import zlib\n",
    "import uuid\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Union\n",
    "from copy import deepcopy, copy\n",
    "from urllib.parse import quote, unquote, urlparse, parse_qs\n",
    "from datetime import datetime\n",
    "from lxml import html\n",
    "from collections import defaultdict\n",
    "from func_timeout import FunctionTimedOut, func_timeout\n",
    "\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col, collect_list, struct, expr, rand, count, pandas_udf, PandasUDFType, \\\n",
    "    round as _round, lit, to_json, sum as _sum, collect_list, first\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, BinaryType, StringType, MapType, LongType\n",
    "\n",
    "TIMEOUT_SECONDS = 3600 * 5\n",
    "MAX_OUTPUT_ROW_SIZE = 1024 * 1024 * 1024 * 1.5\n",
    "SIMILARITY_THRESHOLD = 0.95\n",
    "ERROR_PATH = \"s3://xxx/\"\n",
    "INPUT_PATH = \"s3://xxx/\"\n",
    "OUTPUT_PATH = \"s3://xxx/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_doctor(target_theme):\n",
    "    partition_id = str(uuid.uuid4())\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d\")\n",
    "    error_log_path = f\"{ERROR_PATH}{target_theme}/{current_time}/{partition_id}.jsonl\"\n",
    "    s3_doc_writer = S3DocWriter(path=error_log_path)\n",
    "    return s3_doc_writer\n",
    "\n",
    "\n",
    "def read_to_index(_iter):\n",
    "    s3_doc_writer = get_s3_doctor(\"layout_index\")\n",
    "    error_info = None\n",
    "    for fpath in _iter:\n",
    "        current_layout_id = None\n",
    "        start_offset = None\n",
    "        layout_length = 0\n",
    "        idx = 0\n",
    "        for row in read_s3_rows(fpath):\n",
    "            idx += 1\n",
    "            try:\n",
    "                detail_data = json_loads(row.value)\n",
    "                layout_id = detail_data[\"layout_id\"]\n",
    "                offset, length = map(int, row.loc.split(\"bytes=\")[-1].split(\",\"))\n",
    "                if layout_id == current_layout_id:\n",
    "                    layout_length += length\n",
    "                    continue\n",
    "                else:\n",
    "                    if current_layout_id is not None:\n",
    "                        line = {\n",
    "                            \"layout_id\": current_layout_id,\n",
    "                            \"url_host_name\": detail_data[\"url_host_name\"],\n",
    "                            \"count\": idx - 1,\n",
    "                            \"file\": {\n",
    "                                \"filepath\": fpath,\n",
    "                                \"offset\": start_offset,\n",
    "                                \"length\": layout_length,\n",
    "                                \"record_count\": idx - 1,\n",
    "                                \"timestamp\": int(time.time())\n",
    "                            }\n",
    "                        }\n",
    "                        yield line\n",
    "                        idx = 1\n",
    "                    current_layout_id = layout_id\n",
    "                    start_offset = offset\n",
    "                    layout_length = 0\n",
    "                    layout_length += length\n",
    "            except Exception as e:\n",
    "                error_info = {\n",
    "                    \"error_type\": type(e).__name__,\n",
    "                    \"error_message\": str(e),\n",
    "                    \"traceback\": traceback.format_exc(),\n",
    "                    \"input_data\": row.value if hasattr(row, 'value') else str(row),\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "                s3_doc_writer.write(error_info)\n",
    "                continue\n",
    "    if current_layout_id is not None:\n",
    "        line = {\n",
    "            \"layout_id\": current_layout_id,\n",
    "            \"url_host_name\": detail_data[\"url_host_name\"],\n",
    "            \"count\": idx,\n",
    "            \"file\": {\n",
    "                \"filepath\": fpath,\n",
    "                \"offset\": start_offset,\n",
    "                \"length\": layout_length,\n",
    "                \"record_count\": idx,\n",
    "                \"timestamp\": int(time.time())\n",
    "            }\n",
    "        }\n",
    "        yield line\n",
    "    if error_info:\n",
    "        s3_doc_writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# main func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark(spark_name: str):\n",
    "    global spark\n",
    "    spark = new_spark_session(f\"layout.index.{spark_name}\", config)\n",
    "    sc = spark.sparkContext\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "def parse_input_path(input_path: str):\n",
    "    try:\n",
    "        with open(\"./is_index_complated.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            already_exist = [i for i in content.split(\",\") if i] if content else []\n",
    "    except:\n",
    "        already_exist = []\n",
    "    input_path_lst = [i for i in list(list_s3_objects(input_path, recursive=False)) if i not in already_exist]\n",
    "    return input_path_lst\n",
    "\n",
    "\n",
    "def create_index_df(input_path_lst: List):\n",
    "    schema = StructType([\n",
    "        StructField(\"layout_id\", StringType(), True),\n",
    "        StructField(\"url_host_name\", StringType(), True),\n",
    "        StructField(\"count\", LongType(), True),\n",
    "        StructField(\"file\", StructType([\n",
    "            StructField(\"filepath\", StringType(), True),\n",
    "            StructField(\"offset\", LongType(), True),\n",
    "            StructField(\"length\", LongType(), True),\n",
    "            StructField(\"record_count\", LongType(), True),\n",
    "            StructField(\"timestamp\", IntegerType(), True),\n",
    "        ]), True),\n",
    "    ])\n",
    "    page_content = sc.parallelize(input_path_lst, len(input_path_lst))\n",
    "    dump_html_df = page_content.mapPartitions(read_to_index).toDF(schema)\n",
    "    merge_index(dump_html_df)\n",
    "\n",
    "\n",
    "def merge_index(dump_html_df: DataFrame):\n",
    "    result_df = dump_html_df.groupBy(\"layout_id\") \\\n",
    "        .agg(\n",
    "        _sum(\"count\").alias(\"count\"),\n",
    "        collect_list(\"file\").alias(\"files\"),\n",
    "        first(\"url_host_name\").alias(\"url_host_name\")\n",
    "    )\n",
    "    write_by_two(result_df)\n",
    "\n",
    "\n",
    "def write_by_two(result_df: DataFrame):\n",
    "    struct_col = struct(result_df[\"layout_id\"], result_df[\"count\"], result_df[\"files\"], result_df[\"url_host_name\"])\n",
    "    output_df = result_df.withColumn(\"value\", to_json(struct_col)).select(\"value\")\n",
    "\n",
    "    output_file_size_gb = 2\n",
    "    resize_func = spark_resize_file(output_file_size_gb)\n",
    "    new_output_df = resize_func(output_df)\n",
    "\n",
    "    config[\"skip_output_version\"] = True\n",
    "    config['skip_output_check'] = True\n",
    "\n",
    "    write_any_path(new_output_df, OUTPUT_PATH, config)\n",
    "\n",
    "\n",
    "def close_spark():\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark_name = INPUT_PATH.split(\"/\")[-1]\n",
    "    create_spark(spark_name)\n",
    "    input_path_lst = parse_input_path(INPUT_PATH)\n",
    "    for batch in input_path_lst:\n",
    "        batch_lst = [i for i in list(list_s3_objects(batch, recursive=True)) if i.endswith(\".jsonl\")]\n",
    "        create_index_df(batch_lst)\n",
    "        with open(\"./is_index_complated.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(batch + \",\")\n",
    "    close_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python webkit_venv (ipykernel)",
   "language": "python",
   "name": "webkit_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
