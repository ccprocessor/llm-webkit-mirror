{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from xinghe.spark import *\n",
    "from app.common.json_util import *\n",
    "from xinghe.s3 import *\n",
    "\n",
    "config = {\n",
    "    \"spark_conf_name\": \"spark_4\",\n",
    "    \"skip_success_check\": True,\n",
    "}\n",
    "\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import zlib\n",
    "import base64\n",
    "from typing import Union\n",
    "\n",
    "MAX_OUTPUT_ROW_SIZE = 1024 * 1024 * 1024 * 1.5\n",
    "CC_WARC = ['xx']\n",
    "base_unique_path = [\"xx\"]\n",
    "output_path = \"xx\"\n",
    "\n",
    "spark = new_spark_session(\"cc_dumps.dedup.thr\", config)\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_and_decompress_str(input_data: Union[str, bytes], compress: bool = True, base: bool = False) -> Union[str, bytes]:\n",
    "    try:\n",
    "        if compress:\n",
    "            # 确保输入是字节串\n",
    "            if isinstance(input_data, str):\n",
    "                input_bytes = input_data.encode('utf-8')\n",
    "            elif isinstance(input_data, bytes):\n",
    "                input_bytes = input_data\n",
    "            else:\n",
    "                raise TypeError(\"Input must be a string or bytes object.\")\n",
    "\n",
    "            if base:\n",
    "                # 压缩并转换为Base64字符串\n",
    "                compressed_bytes = zlib.compress(input_bytes)\n",
    "                return base64.b64encode(compressed_bytes).decode('utf-8')\n",
    "            else:\n",
    "                return zlib.compress(input_bytes)\n",
    "                \n",
    "        else:\n",
    "            # 解码Base64字符串并解压缩\n",
    "            if isinstance(input_data, str):\n",
    "                compressed_bytes = base64.b64decode(input_data)\n",
    "            elif isinstance(input_data, bytearray):\n",
    "                compressed_bytes = bytes(input_data)\n",
    "            elif isinstance(input_data, bytes):\n",
    "                compressed_bytes = input_data\n",
    "            else:\n",
    "                raise TypeError(\"Input must be a Base64 encoded string or bytes object.\")\n",
    "\n",
    "            decompressed_bytes = zlib.decompress(compressed_bytes)\n",
    "            return decompressed_bytes.decode('utf-8')  # 假设原始数据是UTF-8编码的字符串\n",
    "\n",
    "    except (zlib.error, base64.binascii.Error, UnicodeDecodeError) as e:\n",
    "        raise ValueError(f\"Error during compression/decompression: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# html source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#warc_paths = []\n",
    "#for dump in DUMPS:\n",
    "#    dump_path = f'{CC_WARC}{dump}/'\n",
    "#    warc_paths.extend([x for x in list(list_s3_objects(dump_path, recursive=True)) if \"/warc/\" in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_path_to_html(row_iter):\n",
    "    for row in row_iter:\n",
    "        try:\n",
    "            detail_datas = json_loads(row.value)\n",
    "        except:\n",
    "            continue\n",
    "        track_id = detail_datas.get(\"track_id\", \"\")\n",
    "        #filename = zz.loc\n",
    "        if detail_datas.get(\"main_html\", \"\"):\n",
    "            #detail_datas[\"raw_warc_path\"] = filename\n",
    "            #fpath_path = fpath.split('/')\n",
    "            #detail_datas[\"sub_path\"] = f\"{fpath_path[4]}/{fpath_path[-1].replace('.warc.gz', '')}\"\n",
    "            yield Row(**{\"value\": json_dumps(detail_datas), \"track_id\": track_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_schema = StructType([\n",
    "    StructField(\"track_id\", StringType(), True),\n",
    "    StructField(\"value\", StringType(), True),\n",
    "])\n",
    "#page_content = sc.parallelize(warc_paths, len(warc_paths))\n",
    "CC_WARC_df = read_any_path(spark, \",\".join(CC_WARC), config)\n",
    "\n",
    "dump_html_df = CC_WARC_df.rdd.mapPartitions(parse_path_to_html).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id_df = read_any_path(spark, ','.join(base_unique_path), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_schema = StructType([\n",
    "    StructField(\"track_id\", StringType(), True),\n",
    "])\n",
    "\n",
    "dump_ods_df_with_struct = unique_id_df.withColumn(\"json_struct\", from_json(unique_id_df.value, unique_schema))\n",
    "unique_id_v_df = dump_ods_df_with_struct.select(\"json_struct.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_df = dump_html_df.join(unique_id_v_df, on='track_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# write gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = inner_df.select(\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"skip_output_version\"] = True\n",
    "config[\"output_compression\"] = \"gz\"\n",
    "write_any_path(output_df, output_path, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (ipykernel)",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
