{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载spark配置\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from xinghe.spark import *\n",
    "from app.common.json_util import *\n",
    "from xinghe.s3 import *\n",
    "from pyspark.sql.functions import col, count, when\n",
    "from collections import defaultdict\n",
    "# config = {\n",
    "#     \"spark_conf_name\": \"spark_2\",\n",
    "#     \"skip_success_check\": True,\n",
    "#     \"input_format\": \"parquet\",\n",
    "#     \"spark.dynamicAllocation.maxExecutors\": 800,\n",
    "# }\n",
    "schema = StructType([StructField('value', StringType(), True)])\n",
    "config = {\n",
    "    \"spark_conf_name\": \"spark_2\",\n",
    "    \"skip_success_check\": True,\n",
    "    \"spark.sql.shuffle.partitions\":10000,\n",
    "    \"spark.executor.memory\":\"20g\",  # 默认30g\n",
    "    \"spark.driver.memory\":\"10g\",  # 默认20g,\n",
    "}\n",
    "\n",
    "spark = new_spark_session(\"cc-extract-index-test\", config)\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = [\"s3://xx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = read_any_path(spark, \",\".join(input_path), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inline_tags = [\"li\", \"td\", \"tr\", \"br\"]\n",
    "def is_block_element(node) -> bool:\n",
    "    \"\"\"如果标签不在内联元素集合中，默认为块级元素。 但是，如果一个内联元素包含块级元素，则该内联元素被视为块级元素。\"\"\"\n",
    "    if node.tag in inline_tags:\n",
    "        return any(is_block_element(child) for child in node.iterchildren())\n",
    "    return isinstance(node, html.HtmlElement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_data(row_iter):\n",
    "    for row in row_iter:\n",
    "        data = json_loads(row.value)\n",
    "        yield Row(**{\"value\": json_dumps(data), \"layout_id\": data[\"layout_id\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "repartition_df = input_df.rdd.mapPartitions(json_data).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_p_tag(xpath: str) -> bool:\n",
    "    \"\"\"检查节点的XPath是否以p标签结束\"\"\"\n",
    "    if not xpath:\n",
    "        return False\n",
    "    # 分割XPath并获取最后一部分\n",
    "    parts = xpath.strip('/').split('/')\n",
    "    if not parts:\n",
    "        return False\n",
    "    last_segment = parts[-1]\n",
    "    # 检查最后一段是否是p标签（带或不带索引）\n",
    "    if last_segment.startswith('p'):\n",
    "        # 处理带索引的情况如 p[1]\n",
    "        rest = last_segment[1:]\n",
    "        if not rest:  # 只有'p'\n",
    "            return True\n",
    "        if rest.startswith('[') and rest.endswith(']') and rest[1:-1].isdigit():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from lxml.html import HtmlComment, fromstring, tostring\n",
    "from typing import Generator, List, Dict, Any, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "def json_dumps(data):\n",
    "    \"\"\"自定义JSON序列化函数\"\"\"\n",
    "    return json.dumps(data, ensure_ascii=False)\n",
    "\n",
    "class Row:\n",
    "    \"\"\"模拟行对象\"\"\"\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "def find_common_prefixes(sequences: List[List[str]], min_occurrence: int = 2) -> List[Tuple[List[str], int]]:\n",
    "    \"\"\"\n",
    "    找到所有序列中至少出现min_occurrence次的最长公共前缀\n",
    "    返回一个列表，包含(公共前缀, 出现次数)元组\n",
    "    \"\"\"\n",
    "    if not sequences:\n",
    "        return []\n",
    "    \n",
    "    # 统计所有前缀及其出现频率\n",
    "    prefix_counts = defaultdict(int)\n",
    "    \n",
    "    # 遍历每个序列的所有可能前缀\n",
    "    for seq in sequences:\n",
    "        for i in range(1, len(seq) + 1):\n",
    "            prefix = tuple(seq[:i])  # 使用元组作为可哈希键\n",
    "            prefix_counts[prefix] += 1\n",
    "    \n",
    "    # 过滤出出现次数足够的候选前缀\n",
    "    candidates = [prefix for prefix, count in prefix_counts.items() \n",
    "                 if count >= min_occurrence and len(prefix) > 0]\n",
    "    \n",
    "    # 按长度排序（最长的在前）\n",
    "    candidates.sort(key=len, reverse=True)\n",
    "    \n",
    "    # 选择最长的有效前缀\n",
    "    result = []\n",
    "    selected_prefixes = set()\n",
    "    \n",
    "    for prefix in candidates:\n",
    "        prefix_tuple = tuple(prefix)\n",
    "        \n",
    "        # 检查是否已被更长的前缀包含\n",
    "        is_subset = False\n",
    "        for selected in selected_prefixes:\n",
    "            if prefix_tuple == selected[:len(prefix_tuple)]:\n",
    "                is_subset = True\n",
    "                break\n",
    "        \n",
    "        if not is_subset:\n",
    "            result.append((list(prefix), prefix_counts[prefix_tuple]))\n",
    "            selected_prefixes.add(prefix_tuple)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def find_common_suffixes(sequences: List[List[str]], min_occurrence: int = 2) -> List[Tuple[List[str], int]]:\n",
    "    \"\"\"\n",
    "    找到所有序列中至少出现min_occurrence次的最长公共后缀\n",
    "    返回一个列表，包含(公共后缀, 出现次数)元组\n",
    "    \"\"\"\n",
    "    if not sequences:\n",
    "        return []\n",
    "    \n",
    "    # 反转所有序列\n",
    "    reversed_seqs = [list(reversed(seq)) for seq in sequences]\n",
    "    \n",
    "    # 使用前缀查找方法\n",
    "    reversed_prefixes = find_common_prefixes(reversed_seqs, min_occurrence)\n",
    "    \n",
    "    # 反转结果返回\n",
    "    return [(list(reversed(prefix)), count) for prefix, count in reversed_prefixes]\n",
    "\n",
    "def extract_text_nodes(row_iter) -> Generator[Any, None, None]:\n",
    "    # 收集所有文档的节点序列及元数据\n",
    "    doc_sequences = []\n",
    "    doc_data_map = {}\n",
    "    \n",
    "    for row in row_iter:\n",
    "        try:\n",
    "            data = json.loads(row.value)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "            \n",
    "        html_content = data.get(\"main_html\", \"\")\n",
    "        track_id = data.get(\"track_id\", \"\")\n",
    "        \n",
    "        if not html_content.strip():\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            tree = fromstring(html_content)\n",
    "            root_tree = tree.getroottree()\n",
    "            \n",
    "            sequence = []       # 文本节点序列\n",
    "            nodes_list = []     # 节点对象列表\n",
    "            xpaths = []         # 节点的XPath\n",
    "            \n",
    "            # 遍历所有节点，收集叶子文本节点\n",
    "            for node in tree.iter():\n",
    "                if isinstance(node, HtmlComment):\n",
    "                    continue\n",
    "                    \n",
    "                if len(node) > 0:  # 非叶子节点跳过\n",
    "                    continue\n",
    "                    \n",
    "                # 拼接文本内容\n",
    "                text_parts = []\n",
    "                if node.text and node.text.strip():\n",
    "                    text_parts.append(node.text.strip())\n",
    "                if node.tail and node.tail.strip():\n",
    "                    text_parts.append(node.tail.strip())\n",
    "                \n",
    "                text = \" \".join(text_parts).strip()\n",
    "                if not text:\n",
    "                    continue\n",
    "                    \n",
    "                sequence.append(text)\n",
    "                nodes_list.append(node)\n",
    "                xpaths.append(root_tree.getpath(node))\n",
    "            \n",
    "            doc_sequences.append(sequence)\n",
    "            doc_data_map[track_id] = {\n",
    "                \"original_data\": data,\n",
    "                \"sequence\": sequence,\n",
    "                \"nodes_list\": nodes_list,\n",
    "                \"xpaths\": xpaths,\n",
    "                \"tree\": tree,\n",
    "                \"root_tree\": root_tree\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {track_id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # 查找公共前缀和后缀（至少出现2次）\n",
    "    common_heads = find_common_prefixes(doc_sequences, min_occurrence=2)\n",
    "    common_tails = find_common_suffixes(doc_sequences, min_occurrence=2)\n",
    "    \n",
    "    print(f\"Found {len(common_heads)} common heads and {len(common_tails)} common tails\")\n",
    "    \n",
    "    for track_id, doc_data in doc_data_map.items():\n",
    "        sequence = doc_data[\"sequence\"]\n",
    "        tree = doc_data[\"tree\"]\n",
    "        nodes_list = doc_data[\"nodes_list\"]\n",
    "        xpaths = doc_data[\"xpaths\"]\n",
    "        \n",
    "        dedup_info = {\n",
    "            \"head_xpaths\": [],\n",
    "            \"tail_xpaths\": [],\n",
    "            \"removed_head\": 0,\n",
    "            \"removed_tail\": 0,\n",
    "            \"matched_head\": None,\n",
    "            \"matched_tail\": None,\n",
    "            \"head_count\": 0,\n",
    "            \"tail_count\": 0\n",
    "        }\n",
    "        \n",
    "        # 移除公共头部（跳过<p>标签内的节点）\n",
    "        for head, count in common_heads:\n",
    "            head_len = len(head)\n",
    "            if sequence[:head_len] == head:\n",
    "                # 检查是否有节点在<p>标签内\n",
    "                skip_removal = any(is_in_p_tag(xpath) for xpath in xpaths[:head_len])\n",
    "                if skip_removal:\n",
    "                    print(f\"Skipping head removal for {track_id} due to <p> tag\")\n",
    "                    continue\n",
    "                    \n",
    "                dedup_info[\"matched_head\"] = head\n",
    "                dedup_info[\"head_count\"] = count\n",
    "                dedup_info[\"removed_head\"] = head_len\n",
    "                dedup_info[\"head_xpaths\"] = xpaths[:head_len]\n",
    "                \n",
    "                for node in reversed(nodes_list[:head_len]):\n",
    "                    parent = node.getparent()\n",
    "                    if parent is not None:\n",
    "                        parent.remove(node)\n",
    "                break\n",
    "        \n",
    "        # 移除公共尾部（跳过<p>标签内的节点）\n",
    "        for tail, count in common_tails:\n",
    "            tail_len = len(tail)\n",
    "            if sequence[-tail_len:] == tail:\n",
    "                # 检查是否有节点在<p>标签内\n",
    "                skip_removal = any(is_in_p_tag(xpath) for xpath in xpaths[-tail_len:])\n",
    "                if skip_removal:\n",
    "                    print(f\"Skipping tail removal for {track_id} due to <p> tag\")\n",
    "                    continue\n",
    "                    \n",
    "                dedup_info[\"matched_tail\"] = tail\n",
    "                dedup_info[\"tail_count\"] = count\n",
    "                dedup_info[\"removed_tail\"] = tail_len\n",
    "                dedup_info[\"tail_xpaths\"] = xpaths[-tail_len:]\n",
    "                \n",
    "                for node in reversed(nodes_list[-tail_len:]):\n",
    "                    parent = node.getparent()\n",
    "                    if parent is not None:\n",
    "                        parent.remove(node)\n",
    "                break\n",
    "        \n",
    "        new_html = tostring(tree, encoding=\"unicode\", pretty_print=False)\n",
    "        doc_data[\"original_data\"][\"new_html\"] = new_html\n",
    "        doc_data[\"original_data\"][\"dedup_info\"] = dedup_info\n",
    "        \n",
    "        yield Row(value=json_dumps(doc_data[\"original_data\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "repartitioned_df = repartition_df.repartition(\"layout_id\").rdd.mapPartitions(extract_text_nodes).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_any_path(repartitioned_df, \"xx\", config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quyuan-kit-dev",
   "language": "python",
   "name": "quyuan-kit-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
