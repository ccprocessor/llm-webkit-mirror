{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.functions import explode, count,col, format_number\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType, ArrayType\n",
    "\n",
    "from app.common.json_util import *\n",
    "from xinghe.spark import *\n",
    "from xinghe.s3 import *\n",
    "\n",
    "config = {\n",
    "    \"spark_conf_name\": \"spark_2\",\n",
    "    \"skip_success_check\": True,\n",
    "    \"spark.yarn.queue\": \"qa\",\n",
    "    # \"spark.dynamicAllocation.maxExecutors\":120,\n",
    "    \"spark.executor.memory\": \"80g\",\n",
    "    \"spark.speculation\": \"true\",     # 启用推测执行\n",
    "    \"maxRecordsPerFile\": 200000,      # 增加每文件记录数以减少总文件数\n",
    "    \"output_compression\": \"gz\",\n",
    "}\n",
    "\n",
    "\n",
    "spark = new_spark_session(\"cc_domain_hash\", config)\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [\n",
    "    \"s3://web-parse-huawei/CC/pre-dedup/v008/unique_html/CC-MAIN-2013-20/\", # 第一批\n",
    "]\n",
    "\n",
    "\n",
    "input_df = read_any_path(spark, \",\".join(input_paths), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, DataFrame, Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, IntegerType, StructType, StructField\n",
    "import xxhash\n",
    "from urllib.parse import urlparse\n",
    "import json\n",
    "from pyspark.sql.functions import monotonically_increasing_id, expr\n",
    "\n",
    "\n",
    "# 配置参数\n",
    "hash_count = 10000  # 域名哈希桶数量10000\n",
    "target_records_per_file = 100000  # 每个文件的目标记录数100000\n",
    "# 分区设置10000\n",
    "numPartitions_id = 10000\n",
    "numPartitions_subpath = 15000\n",
    "\n",
    "# 定义提取domain的UDF\n",
    "def extract_domain(url):\n",
    "    if url is None:\n",
    "        return None\n",
    "    try:\n",
    "        hostname = urlparse(url).hostname\n",
    "        return hostname.lower() if hostname else None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# 定义计算domain_hash_id的UDF\n",
    "def compute_domain_hash(domain):\n",
    "    if domain is None:\n",
    "        return None\n",
    "    return xxhash.xxh64_intdigest(domain) % hash_count\n",
    "\n",
    "# 注册UDF以在DataFrame操作中使用\n",
    "extract_domain_udf = F.udf(extract_domain, StringType())\n",
    "compute_domain_hash_udf = F.udf(compute_domain_hash, IntegerType())\n",
    "\n",
    "\n",
    "def process_with_df_api_and_file_indices(df, target_records_per_file):\n",
    "    \"\"\"\n",
    "    一体化处理函数，完成以下步骤：\n",
    "    1. 从JSON提取字段\n",
    "    2. 添加domain和domain_hash_id\n",
    "    3. 计算file_idx（确保为非负值）\n",
    "    4. 更新sub_path，包含domain_hash_id和file_idx\n",
    "    5. 重新构建JSON（防止字段重复）\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 提取url用于计算域名和哈希\n",
    "    extracted_df = df.withColumn(\"url\", F.get_json_object(F.col(\"value\"), \"$.url\")) \\\n",
    "        .withColumn(\"original_sub_path\", F.get_json_object(F.col(\"value\"), \"$.sub_path\"))\n",
    "    \n",
    "    # 2. 提取域名和计算哈希ID\n",
    "    processed_df = extracted_df \\\n",
    "        .withColumn(\"domain\", extract_domain_udf(F.col(\"url\"))) \\\n",
    "        .withColumn(\"domain_hash_id\", compute_domain_hash_udf(F.col(\"domain\"))) \\\n",
    "        .filter(F.col(\"domain_hash_id\").isNotNull())\n",
    "    \n",
    "    # 3. 使用高效方法计算file_idx，确保为非负值\n",
    "    # 添加唯一ID\n",
    "    df_with_id = processed_df.withColumn(\"_uid\", F.abs(monotonically_increasing_id()))\n",
    "\n",
    "    # 使用更细粒度的预分区\n",
    "    df_with_id_repartitioned = df_with_id.repartition(numPartitions_id, \"domain_hash_id\")\n",
    "\n",
    "    # 然后再应用窗口函数\n",
    "    df_with_indices = df_with_id_repartitioned \\\n",
    "        .withColumn(\"rel_pos\", F.row_number().over(Window.partitionBy(\"domain_hash_id\").orderBy(\"_uid\"))) \\\n",
    "        .withColumn(\"file_idx\", ((F.col(\"rel_pos\") - 1) / target_records_per_file).cast(\"long\"))\n",
    "    \n",
    "    # 4. 创建新的sub_path字段，包含domain_hash_id和file_idx\n",
    "    def update_complete_json(json_str, domain, domain_hash_id, file_idx, original_sub_path):\n",
    "        \"\"\"更新JSON，保留所有原始字段\"\"\"\n",
    "        try:\n",
    "            data = json_loads(json_str)\n",
    "            # 添加新字段\n",
    "            data[\"domain\"] = domain\n",
    "            data[\"domain_hash_id\"] = domain_hash_id\n",
    "            data[\"file_idx\"] = file_idx\n",
    "            # 更新sub_path\n",
    "            data[\"sub_path\"] = f\"{domain_hash_id}/{original_sub_path if original_sub_path else ''}/{file_idx}\"\n",
    "            return json_dumps(data)\n",
    "        except Exception as e:\n",
    "            print(f\"JSON更新错误: {e}\")\n",
    "            return json_str\n",
    "    \n",
    "    # 注册UDF\n",
    "    update_json_udf = F.udf(update_complete_json, StringType())\n",
    "    \n",
    "    # 应用UDF\n",
    "    final_df = df_with_indices.withColumn(\n",
    "        \"value\",\n",
    "        update_json_udf(\n",
    "            F.col(\"value\"),  # 原始完整JSON\n",
    "            F.col(\"domain\"),\n",
    "            F.col(\"domain_hash_id\"),\n",
    "            F.col(\"file_idx\"),\n",
    "            F.col(\"original_sub_path\")\n",
    "        )\n",
    "    ).withColumn(\n",
    "        \"sub_path\",\n",
    "        F.concat(\n",
    "            F.col(\"domain_hash_id\").cast(\"string\"), \n",
    "            F.lit(\"/\"), \n",
    "            F.col(\"original_sub_path\"), \n",
    "            F.lit(\"/\"),\n",
    "            F.col(\"file_idx\").cast(\"string\")\n",
    "        )\n",
    "    ).select(\"value\", \"sub_path\")  # 只保留value和sub_path列\n",
    "    \n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 主处理流程 =====\n",
    "\n",
    "# 使用一体化处理函数处理数据\n",
    "prepared_df = process_with_df_api_and_file_indices(input_df, target_records_per_file)\n",
    "\n",
    "regrouped_df = prepared_df.repartition(numPartitions_subpath, \"sub_path\")\n",
    "\n",
    "# 验证分区结果是否符合预期\n",
    "# quick_validation(regrouped_df)\n",
    "\n",
    "# # 获取分区数量\n",
    "# num_partitions = regrouped_df.rdd.getNumPartitions()\n",
    "# print(f\"当前DataFrame有 {num_partitions} 个分区\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写入中间数据\n",
    "# output_path = \"s3://qa-huawei/chupei/cc-domain-centric-store/data-mid-1dump-0412-v2/\"\n",
    "output_path = \"s3://web-parse-hw60p/CC-domain/\"\n",
    "write_any_path(regrouped_df, output_path, config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (ipykernel)",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
